{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load in Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"deep\", desat=0.6)\n",
    "sns.set_context(rc={\"figure.figsize\": (8,4)})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load In NBA Score Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_dir = '/data2/nba-rt-prediction/sparkfiles/cleanedDF'\n",
    "df = spark.read.format('csv')\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .option(\"dateFormat\", \"yyyy-MM-dd\")\\\n",
    "                    .load(cleaned_dir).coalesce(2)\n",
    "\n",
    "                # For some reason my key is none upon load ! Rebuild\n",
    "df = df.withColumn(\"key\", concat(date_format(df.dateOrig, \"yyyy-MM-dd\"),lit(\".\"),col(\"away_team\"),lit(\".\"),col(\"home_team\")))\n",
    "            \n",
    "#df.printSchema()\n",
    "df_pd_X= df.select([\"cf1\",\"cf2\",\"home_score\", \"away_score\", \"score_diff_amh\", \"home_team_spread\",\"pct_complete\"]).toPandas()\n",
    "X = df_pd_X.values\n",
    "df_pd_Y= df.select([\"home_win\"]).toPandas()\n",
    "Y = df_pd_Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 29.35296743,   8.39902472,  86.80487805, ...,   8.96747967,\n",
       "         11.66666667,  97.91666667],\n",
       "       [ 23.74256995,   4.84313707,  85.34146341, ...,   8.72357724,\n",
       "         11.66666667,  96.875     ],\n",
       "       [ 20.17445863,   3.24555738,  83.87804878, ...,   8.4796748 ,\n",
       "         11.66666667,  95.83333333],\n",
       "       ..., \n",
       "       [  0.        ,   0.        ,   2.09059233, ...,   0.        ,\n",
       "          3.5       ,   2.08333333],\n",
       "       [  0.        ,   0.        ,   1.04529617, ...,   0.        ,\n",
       "          3.5       ,   1.04166667],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          3.5       ,   0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n",
    "# Create Train / cv or Dev / Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Use DNN Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test fraction correct (Accuracy) = 0.82\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, Y, train_size=0.7, random_state=0)\n",
    "lr = LogisticRegressionCV()\n",
    "lr.fit(train_X, train_y)\n",
    "pred_y = lr.predict(test_X)\n",
    "print(\"Test fraction correct (Accuracy) = {:.2f}\".format(lr.score(test_X, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(6, input_shape=(7,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cb = model.fit(train_X, train_y, verbose=0, batch_size=100, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2752/3412 [=======================>......] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "\n",
    "(loss, accuracy) = model.evaluate(test_X, test_y, verbose=1,)\n",
    "#print('Test score:', score[0])\n",
    "#print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.63935662219200218,\n",
       "  0.63822568544463076,\n",
       "  0.64249811546381286,\n",
       "  0.65946217742988467,\n",
       "  0.69062578396510765,\n",
       "  0.72241769486499452,\n",
       "  0.76514199562604834,\n",
       "  0.77883890481653095,\n",
       "  0.792535809887567,\n",
       "  0.78864036207973254,\n",
       "  0.79442070638393447,\n",
       "  0.79768785734696734,\n",
       "  0.79429504491780145,\n",
       "  0.79643126478084936,\n",
       "  0.79680824056586064,\n",
       "  0.79454636566301606,\n",
       "  0.79341542854114944,\n",
       "  0.79454636522860167,\n",
       "  0.80057803396305027,\n",
       "  0.79467202672469428,\n",
       "  0.79429504872267198,\n",
       "  0.79819049647058726,\n",
       "  0.79731087744250972,\n",
       "  0.7965569187570799,\n",
       "  0.80271424820867132,\n",
       "  0.80145764818261034,\n",
       "  0.80346820308923061,\n",
       "  0.804599146607474,\n",
       "  0.80522744045631467,\n",
       "  0.80598139798829949,\n",
       "  0.80598140098426052,\n",
       "  0.80924855119830308,\n",
       "  0.80937421225998141,\n",
       "  0.80359386761124396,\n",
       "  0.80560441920732728,\n",
       "  0.80623271602216928,\n",
       "  0.80774063379748384,\n",
       "  0.80510178011366695,\n",
       "  0.81063080629412032,\n",
       "  0.80748931651260425,\n",
       "  0.80309122474267269,\n",
       "  0.80673536272557067,\n",
       "  0.80660969863797172,\n",
       "  0.81226438328859707,\n",
       "  0.80887157616228222,\n",
       "  0.80585574142056271,\n",
       "  0.81012817391141267,\n",
       "  0.80987684763865009,\n",
       "  0.81264136319305469,\n",
       "  0.80447348217533954],\n",
       " 'loss': [0.66325383735799581,\n",
       "  0.63249929171766162,\n",
       "  0.61912284184412969,\n",
       "  0.59385298088765914,\n",
       "  0.57278506560791798,\n",
       "  0.5567619318817093,\n",
       "  0.51562684871767539,\n",
       "  0.49514030818514981,\n",
       "  0.47944412764876176,\n",
       "  0.47337669585032149,\n",
       "  0.46919540992391923,\n",
       "  0.46508619005010787,\n",
       "  0.45727010209466323,\n",
       "  0.45876716448452404,\n",
       "  0.45699269876734161,\n",
       "  0.45205605548361072,\n",
       "  0.45282811476945217,\n",
       "  0.44648381810776699,\n",
       "  0.44646781763859089,\n",
       "  0.44767046594895377,\n",
       "  0.4470740511328109,\n",
       "  0.44197054173245925,\n",
       "  0.44617119647204861,\n",
       "  0.44202001787184708,\n",
       "  0.43641759129018753,\n",
       "  0.43439120830198902,\n",
       "  0.4335525389231758,\n",
       "  0.43030577295088357,\n",
       "  0.43060847450273365,\n",
       "  0.42834329071821475,\n",
       "  0.43111418448641359,\n",
       "  0.42707496740674816,\n",
       "  0.43064555559484285,\n",
       "  0.42665783664364704,\n",
       "  0.42435952956636336,\n",
       "  0.42715360876242997,\n",
       "  0.42645098308307738,\n",
       "  0.4239163051435893,\n",
       "  0.42325464059552165,\n",
       "  0.42759940593678258,\n",
       "  0.42353893331499909,\n",
       "  0.42401241117699356,\n",
       "  0.4218850116928668,\n",
       "  0.42162772049164465,\n",
       "  0.42326393183465877,\n",
       "  0.42328994130033676,\n",
       "  0.41776075350276548,\n",
       "  0.41954774363282399,\n",
       "  0.42078550001519743,\n",
       "  0.42887724789580856]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RNN Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,  -8,   0,  77,  66, -10,  -2,  58], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "rnn_train_X = sequence.pad_sequences(train_X, maxlen=10)\n",
    "np.shape(rnn_train_X)\n",
    "np.shape(train_X)\n",
    "\n",
    "\n",
    "rnn_train_X[2]\n",
    "#test_X, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use DNN - Tflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vector Assembler\n",
    "#feature_cols  = [\"home_score\", \"away_score\", \"score_diff_amh\", \"home_team_spread\",\"pct_complete\", \"cf1\", \"cf2\"]\n",
    "feature_cols = [\"home_score\", \"away_score\", \"score_diff_amh\", \"home_team_spread\",\"pct_complete\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def load_csv(filename):\n",
    "    file = pd.read_csv(filename, header=0)\n",
    "\n",
    "    # get sample's metadata\n",
    "    n_samples = int(file.columns[0])\n",
    "    n_features = int(file.columns[1])\n",
    "\n",
    "    # divide samples into explanation variables and target variable\n",
    "    data = np.empty((n_samples, n_features))\n",
    "    target = np.empty((n_samples,), dtype=np.int)\n",
    "    for i, row in enumerate(file.itertuples()):\n",
    "        target[i] = np.asarray(row[-1], dtype=np.int)\n",
    "        data[i] = np.asarray(row[1:n_features+1], dtype=np.float64)\n",
    "    return (data, target)\n",
    "\n",
    "# output train data \n",
    "def get_batch_data(x_train, y_train, size=None):\n",
    "    if size is None:\n",
    "        size = len(x_train)\n",
    "    batch_xs = x_train\n",
    "    batch_ys = []\n",
    "\n",
    "    # convert to 1-of-N vector\n",
    "    for i in range(len(y_train)):\n",
    "        val = np.zeros((CLASS_SIZE), dtype=np.float64)\n",
    "        val[y_train[i]] = 1.0\n",
    "        batch_ys.append(val)\n",
    "    batch_ys = np.asarray(batch_ys)\n",
    "    return batch_xs[:size], batch_ys[:size]\n",
    "\n",
    "# output test data\n",
    "def get_test_data(x_test, y_test):\n",
    "    batch_ys = []\n",
    "\n",
    "    # convert to 1-of-N vector\n",
    "    for i in range(len(y_test)):\n",
    "        val = np.zeros((CLASS_SIZE), dtype=np.float64)\n",
    "        val[y_test[i]] = 1.0\n",
    "        batch_ys.append(val)\n",
    "    return x_test, np.asarray(batch_ys)\n",
    "\n",
    "# for parameter initialize\n",
    "def get_stddev(in_dim, out_dim):\n",
    "    return 1.3 / math.sqrt(float(in_dim) + float(out_dim))\n",
    "\n",
    "# DNN Model Class\n",
    "class Classifier:\n",
    "    def __init__(self, hidden_units=[10], n_classes=0, data_size = 0):\n",
    "        self._hidden_units = hidden_units\n",
    "        self._n_classes = n_classes\n",
    "        self._data_size = data_size\n",
    "        self._sess = tf.Session()\n",
    "\n",
    "    # build model\n",
    "    def inference(self, x):\n",
    "        hidden = []\n",
    "\n",
    "        # Input Layer\n",
    "        with tf.name_scope(\"input\"):\n",
    "            weights = tf.Variable(tf.truncated_normal([DATA_SIZE, self._hidden_units[0]], stddev=get_stddev(DATA_SIZE, self._hidden_units[0]), seed=42), name='weights')\n",
    "            biases = tf.Variable(tf.zeros([self._hidden_units[0]]), name='biases')\n",
    "            input = tf.matmul(x, weights) + biases\n",
    "\n",
    "        # Hidden Layers\n",
    "        for index, num_hidden in enumerate(self._hidden_units):\n",
    "            if index == len(self._hidden_units) - 1: break\n",
    "            with tf.name_scope(\"hidden{}\".format(index+1)):\n",
    "                weights = tf.Variable(tf.truncated_normal([num_hidden, self._hidden_units[index+1]], seed=42, stddev=get_stddev(num_hidden, self._hidden_units[index+1])), name='weights')\n",
    "                biases = tf.Variable(tf.zeros([self._hidden_units[index+1]]), name='biases')\n",
    "                inputs = input if index == 0 else hidden[index-1]\n",
    "                hidden.append(tf.nn.relu(tf.matmul(inputs, weights) + biases, name=\"hidden{}\".format(index+1)))\n",
    "        \n",
    "        # Output Layer\n",
    "        with tf.name_scope('output'):\n",
    "            weights = tf.Variable(tf.truncated_normal([self._hidden_units[-1], self._n_classes], seed=42, stddev=get_stddev(self._hidden_units[-1], self._n_classes)), name='weights')\n",
    "            biases = tf.Variable(tf.zeros([self._n_classes]), name='biases')\n",
    "            logits = tf.nn.softmax(tf.matmul(hidden[-1], weights) + biases)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    # loss function\n",
    "    def loss(self, logits, y):        \n",
    "        #return -tf.reduce_mean(y * tf.log(logits))\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "    # fitting function for train data\n",
    "    def fit(self, x_train=None, y_train=None, steps=200):\n",
    "        # build model\n",
    "        x = tf.placeholder(tf.float32, [None, DATA_SIZE])\n",
    "        y = tf.placeholder(tf.float32, [None, CLASS_SIZE])\n",
    "        logits = self.inference(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        train_op = tf.train.AdamOptimizer(0.003).minimize(loss)\n",
    "\n",
    "        # save variables\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        self._logits = logits\n",
    " \n",
    "        # init parameters\n",
    "        #init = tf.initialize_all_variables() \n",
    "        init = tf.global_variables_initializer()\n",
    "        self._sess.run(init)\n",
    "\n",
    "        # train\n",
    "        for i in range(steps):\n",
    "            batch_xs, batch_ys = get_batch_data(x_train, y_train)\n",
    "            self._sess.run(train_op, feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "    # evaluation function for test data\n",
    "    def evaluate(self, x_test=None, y_test=None):\n",
    "        x_test, y_test = get_test_data(x_test, y_test)\n",
    "        \n",
    "        # build accuracy calculate step\n",
    "        correct_prediction = tf.equal(tf.argmax(self._logits, 1), tf.argmax(self._y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # evaluate\n",
    "        return self._sess.run([accuracy], feed_dict={self._x: x_test, self._y: y_test})\n",
    "\n",
    "    # label pridiction\n",
    "    def predict(self, samples):\n",
    "        predictions = tf.argmax(self._logits, 1)\n",
    "        return self._sess.run(predictions, {self._x: samples})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
