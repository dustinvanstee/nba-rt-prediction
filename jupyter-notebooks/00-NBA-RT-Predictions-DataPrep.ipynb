{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Lookup Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monthMap = {\n",
    "    \"Jan\": \"01\",\n",
    "    \"Feb\": \"02\",\n",
    "    \"Mar\": \"03\",\n",
    "    \"Apr\": \"04\",\n",
    "    \"May\": \"05\",\n",
    "    \"Jun\": \"06\",\n",
    "    \"Jul\": \"07\",\n",
    "    \"Aug\": \"08\",\n",
    "    \"Sep\": \"09\",\n",
    "    \"Oct\": \"10\",\n",
    "    \"Nov\": \"11\",\n",
    "    \"Dec\": \"12\"\n",
    "}\n",
    "\n",
    "teamMap = {\n",
    "  \"Atlanta\" : \"atl\",\n",
    "  \"Boston\"  : \"bos\",\n",
    "  \"Brooklyn\"  : \"bkn\",\n",
    "  \"Charlotte\"  : \"cha\",\n",
    "  \"Chicago\"  : \"chi\",\n",
    "  \"Cleveland\"  : \"cle\",\n",
    "  \"Dallas\"  : \"dal\",\n",
    "  \"Denver\"  : \"den\",\n",
    "  \"Detroit\"  : \"det\",\n",
    "  \"Golden State\"  : \"gst\",\n",
    "  \"Houston\"  : \"hou\",\n",
    "  \"Indiana\"  : \"ind\",\n",
    "  \"LA Clippers\"  : \"lac\",\n",
    "  \"LA Lakers\"  : \"lal\",\n",
    "  \"Memphis\"  : \"mem\",\n",
    "  \"Miami\"  : \"mia\",\n",
    "  \"Milwaukee\"  : \"mil\",\n",
    "  \"Minnesota\"  : \"min\",\n",
    "  \"New Orleans\"  : \"nor\",\n",
    "  \"New York\"  : \"nyk\",\n",
    "  \"Oklahoma City\"  : \"okc\",\n",
    "  \"Orlando\"  : \"orl\",\n",
    "  \"Philadelphia\"  : \"phi\",\n",
    "  \"Phila.\"  : \"phi\",\n",
    "  \"Phoenix\"  : \"pho\",\n",
    "  \"Portland\"  : \"por\",\n",
    "  \"Sacramento\" : \"sac\",\n",
    "  \"San Antonio\"  : \"san\",\n",
    "  \"Toronto\"  : \"tor\",\n",
    "  \"Utah\"  : \"uta\",\n",
    "  \"Washington\"  : \"wsh\",\n",
    "   None : \"none\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load In NBA Score Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType,DateType\n",
    "\n",
    "customSchema = StructType([\n",
    "    StructField(\"dateOrig\", DateType()),\n",
    "    StructField(\"ts\", StringType()),\n",
    "    StructField(\"away_team\", StringType()),\n",
    "    StructField(\"away_score\", IntegerType()),\n",
    "    StructField(\"home_tema\", StringType()),\n",
    "    StructField(\"home_home_score\", IntegerType()),\n",
    "    StructField(\"timestring\", StringType()),\n",
    "    StructField(\"timeleft\", DoubleType()),\n",
    "    StructField(\"gameid\", IntegerType())\n",
    "])\n",
    "\n",
    "nbafile = '/data2/nba-rt-prediction/scoredata/scores_nba.2015.test.dat'\n",
    "rtscoresAndFinalDF = spark.read.format('csv')\\\n",
    "                    .option(\"header\", \"false\")\\\n",
    "                    .option(\"inferSchema\", \"false\")\\\n",
    "                    .option(\"nullValue\", \"empty\")\\\n",
    "                    .option(\"dateFormat\", \"yyyy-MM-dd\")\\\n",
    "                    .option(\"mode\",\"DROPMALFORMED\")\\\n",
    "                    .schema(customSchema)\\\n",
    "                    .load(nbafile).coalesce(2)\n",
    "    \n",
    "rtscoresAndFinalDF.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDFs For Creating Extra Columns In Real Time Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new team name column.. do simple lookup conversion with a UDF\n",
    "def mapper(teamin) :\n",
    "    return teamMap[teamin]\n",
    "\n",
    "mapperudf = udf(mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Date Logic to adjust for games that finish on the day after .... \n",
    "# This is so that I can join them against the spread which was dated the day prior...\n",
    "# This is due to not having a great key to join my tables ...\n",
    "\n",
    "datecrossregex = re.compile(\"^0[0-3]\") # midnight to 3am\n",
    "def dateadjust(datein, tsin ) : \n",
    "    #dateary = datein.split(\"-\")\n",
    "    tsary   = tsin.split(\":\")\n",
    "    sub_one_day = datetime.timedelta(days=1)\n",
    "    newdate = datein\n",
    "    if datecrossregex.match(tsary[0]) :\n",
    "        #day = \"%02d\".format(int(dateary[2]) -1)\n",
    "        #newdate = dateary(0) + \"-\" + dateary(1) + \"-\" + day   \n",
    "        newdate = datein - sub_one_day\n",
    "    return str(newdate)\n",
    "\n",
    "dateadjustudf = udf(dateadjust)\n",
    "\n",
    "\n",
    "# UDFs to create some extra features ... this one is for an experiemental combination of Time left and Score difference.  \n",
    "# Made this via intuition.  This can be extended to add other custom features\n",
    "import math\n",
    "def scoredivtimeXform(numerator, denominator):\n",
    "    rv = numerator/(math.pow(denominator+1,0.5))\n",
    "    return rv\n",
    "scoredivtimeUdf = udf(scoredivtimeXform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle The Real Time And Final Score Data.  Add Columns To The Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Overtime games from this analysis\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.filter(~col(\"timestring\").like(\"%OT%\"))\n",
    "\n",
    "# Create short 3 character team names \n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teamxyz\", mapperudf(col(\"away_team_full\")))\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"home_team_\", mapperudf(col(\"home_team_full\")))\n",
    "\n",
    "# Add a score differential Column \n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"away_score-home_score\", col(\"away_score\") - col(\"home_score\"))\n",
    "\n",
    "# Transform the Date.  This is for games that spanned multiple days and gave me a headache.  \n",
    "# Games adjusted to the day they started on.\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"date\",  dateadjustudf(col(\"dateOrig\"),col(\"ts\")))\n",
    "\n",
    "# Create a Key for me to use to join with my odds data later.  Key = date.away_team_.home_team_\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"key\", concat(col(\"date\"),lit(\".\"),col(\"away_team_\"),lit(\".\"),col(\"home_team_\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtscoresAndFinalDF.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate The Real Time And Final Data From One Common Dataframe To Two Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Currently based on the way the data was sampled, both real time scores and final scores are written as seperate records to the same file.  I need to pull these apart, and then join the dataframes so that I have a real time score and features, and know if the game was won or lost ....\n",
    "\n",
    "# Create Final Score DF\n",
    "# Note a shortcut for repeating the dataframe within the filter is to use a $   df.filter(df(\"foo\").contains ... is equiv to df.filter($\"foo\".contains)\n",
    "\n",
    "finalscoresDF = rtscoresAndFinalDF.filter(col(\"timestring\").like(\"%FINAL%\"))\n",
    "\n",
    "# Rename some columns so that join later doesnt have name overlaps\n",
    "finalscoresDF = finalscoresDF.withColumnRenamed(\"away_score\", \"final_away_score\")\n",
    "finalscoresDF = finalscoresDF.withColumnRenamed(\"home_score\", \"final_home_score\")\n",
    "\n",
    "# Create final score difference\n",
    "finalscoresDF = finalscoresDF.withColumn(\"final_away_score-final_home_score\", col(\"final_away_score\") - col(\"final_home_score\"))\n",
    "\n",
    "# Add a Win/loss column Win = 1, Loss = 0\n",
    "finalscoresDF = finalscoresDF.withColumn(\"win-loss-enc\", (when(col(\"final_away_score-final_home_score\") > 0.0, 1.0).otherwise(0)))\n",
    "\n",
    "#(when(df['age'] == 2, 3).otherwise(4)\n",
    "\n",
    "# Remove Halftime records as this particular case isn't handled well... (for now)\n",
    "rtscoresDF = rtscoresAndFinalDF.filter(~col(\"timestring\").like(\"%FINAL%\")).filter(~col(\"timestring\").like(\"HALFTIME\"))\n",
    "\n",
    "# Create final score difference\n",
    "rtscoresDF = rtscoresDF.withColumn(\"away_score-home_score\", col(\"away_score\") - col(\"home_score\"))\n",
    "\n",
    "# Create a unique feature based on my custom UDF.  Idea here is that I have intuition that timeleft and score difference are a strong predictor when combined\n",
    "rtscoresDF = rtscoresDF.withColumn(\"score-div-time\", scoredivtimeUdf(col(\"away_score\") - col(\"home_score\"), col(\"timeleft\")*2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Take A Look Of What We Have For The Two Dataframes We Just Wrangled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Printouts .....\n",
    "print(\"final scores data frame\")\n",
    "finalscoresDF.show(5)\n",
    "print(\"real time scores data frame\")\n",
    "rtscoresDF.show(5)\n",
    "finalscoresDF.printSchema\n",
    "\n",
    "\n",
    "print \"##########################################\"\n",
    "print \"Total Data Points in rtscoresDF = {0}\".format(rtscoresDF.count())\n",
    "print \"Total Data Points in rtscoresDF uniq = {0}\".format(rtscoresDF.sort(\"key\").distinct().count())\n",
    "print \"Total Data Points in finalscoresDF = {0}\".format(finalscoresDF.count())\n",
    "print \"##########################################\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Interpret the Odds data\n",
    "````How to interpret the odds data ...\n",
    "Example Golden State -12.5 O (207.0) -125.0 | Detroit 12.5 U (207.0) 145.0\n",
    "Here Golden State the away team is a 12.5 pt favorite to win.  The over under is in parentheses (207) and is the 50/50 line between teams sum of scores\n",
    "being above/below that line.  \n",
    "Finally the -125 / +145 numbers are whats known at the moneyline odds. \n",
    "    A negative number means you need to bet 125$ to get a 100$ payout\n",
    "    A positive number means you need to bet 100$ to get a 145$ payout\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load In Odds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, the data is very raw, and needs to be pre-processed .  I will start by loading it as an RDD and perform a lot of transformations.  Once I have it properly parsed, I will convert to a dataframe.\n",
    "# This is not beautiful, but gets the job done\n",
    "# Data format .....\n",
    "#       <title>New Orleans 2.5 O (207.0) 125.0 | Phila. -2.5 U (207.0) -145.0 (Apr 05, 2016 07:10 PM)</title>\n",
    "#       <title>Detroit 4.0 O (202.0) 160.0 | Miami -4.0 U (202.0) -190.0 (Apr 05, 2016 08:05 PM)</title>\n",
    "oddsfile = \"/data2/nba-rt-prediction/nbaodds_042516.xml\"\n",
    "\n",
    "# Reading the data in as an RDD first.  There isn't a dataframe parser for this XML I have, so I will write a custom parser ....\n",
    "oddsrdd = spark.read.text(oddsfile).rdd\n",
    "# just grabbing the text within the < ... > tags.  I can do this, because the format is super simple and not nested\n",
    "# the subscript [0] is due to the fact that spark.read.text read in the oddsfile of Type Row.  Need to index\n",
    "# into it to get the string\n",
    "gameStringRdd = oddsrdd.map(lambda x : x[0][x[0].find('>')+1:x[0].rfind('<')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is where I do the heavy lifting of parsing my XML .. and then finally convert my RDD to a dataframe .....\n",
    "# just lots of string parsing and data type conversions\n",
    "def parseOdds(line_in) : \n",
    "    away_str = line_in[0:line_in.find('|')]  \n",
    "    home_and_date_str = line_in[line_in.find('|')+2:-1]\n",
    "    home_str = home_and_date_str[0:home_and_date_str.rfind('(')]\n",
    "    date_str = home_and_date_str[home_and_date_str.rfind('(')+1:len(home_and_date_str)]\n",
    "    date_str = date_str.replace(',', '')\n",
    "    \n",
    "    # parse away string\n",
    "    overunder = away_str[away_str.find('(')+1:away_str.find(')')]\n",
    "    away_team_ml = away_str[away_str.find(')')+2:len(away_str)-1]\n",
    "    away_str_2 = away_str[0:away_str.find('(')-3]\n",
    "    away_team_spread = away_str_2[away_str_2.rfind(' ')+1:len(away_str_2)]\n",
    "    away_team_full = away_str_2[0:away_str_2.rfind(' ')]\n",
    "    away_team_ = teamMap[away_team_full]\n",
    "    \n",
    "     # parse home string\n",
    "    overunder = home_str[home_str.find('(')+1:home_str.find(')')]\n",
    "    teamhml = home_str[home_str.find(')')+2:len(home_str)-1]\n",
    "    home_str_2 = home_str[0:home_str.find('(')-3]\n",
    "    teamhspread = home_str_2[home_str_2.rfind(' ')+1:len(home_str_2)]\n",
    "    teamlongh = home_str_2[0:home_str_2.rfind(' ')]\n",
    "    teamh = teamMap[teamlongh]\n",
    "   \n",
    "    # parse date string\n",
    "    dateInfo = date_str.split(' ')\n",
    "    dateStr = dateInfo[2] + \"-\" + monthMap[dateInfo[0]] + \"-\" + dateInfo[1]\n",
    "    # This will become my join key for the other data sets\n",
    "    key = dateStr +\".\" + away_team_ + \".\" + teamh\n",
    "    return (key,away_team_full,away_team_,away_team_spread,overunder,away_team_ml,teamlongh,teamh,teamhml,dateStr)\n",
    "\n",
    "#def parseOdds(line_in) : \n",
    "#    away_str = line_in[0:line_in.find('|')]  \n",
    "#    return (away_str,away_str)\n",
    "\n",
    "\n",
    "oddsDF = gameStringRdd.map(lambda x : parseOdds(x))\\\n",
    "        .toDF([\"key\",\"away_team_full\",\"away_team_\",\"away_team_spread\",\"overunder\",\"away_team_ml\",\"teamlongh\",\"teamh\",\"teamhml\",\"dateStr\"]).distinct()\n",
    "\n",
    "# OddsDF has some dups due to the fact that I have multiple readings ...\n",
    "oddsDF.registerTempTable(\"odds_table\")\n",
    "\n",
    "oddsDF = spark.sql(\"SELECT key, FIRST(away_team_full) as away_team_full, FIRST(away_team_) as away_team_,\\\n",
    "  AVG(away_team_spread) as away_team_spread, AVG(overunder) as overunder, AVG(away_team_ml) as away_team_ml,\\\n",
    "  FIRST(teamlongh) as teamlongh,FIRST(teamh) as teamh,AVG(teamhml) as teamhml, FIRST(dateStr) as dateStr FROM odds_table GROUP BY key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Some Of The Odds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oddsDF.show(5)\n",
    "print \"Total Home Teams      = {0}\".format(oddsDF.select(\"teamh\").distinct().count())\n",
    "print \"Total Away Teams      = {0}\".format(oddsDF.select(\"away_team_\").distinct().count())\n",
    "print \"Total Games Collected = {0} \".format(oddsDF.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join The Odds And Final Score Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is where we join the Odds/Realtime scores/ Final Scores into one wholistic data set as input for Logistic Machine Learning\n",
    "\n",
    "# Create a smaller Final Score Dataframe.  Just keep the key, final score a and b, the win/loss indicator\n",
    "finalslicedscoresDF1 = finalscoresDF.select([\"key\",\"final_away_score\",\"final_home_score\",\"win-loss-enc\"]).distinct()\n",
    "# First Join the 2 smallest data frames ... odd and final.\n",
    "gameDF = oddsDF.join(finalslicedscoresDF1, oddsDF[\"key\"] == finalslicedscoresDF1[\"key\"], \"inner\").drop(oddsDF[\"key\"])\n",
    "# Drop these redundant columns prior to joining with Realtime score dataframe\n",
    "gameDF = gameDF.drop(\"away_team_full\")\n",
    "gameDF = gameDF.drop(\"home_team_full\")\n",
    "gameDF = gameDF.drop(\"away_team_\")\n",
    "gameDF = gameDF.drop(\"home_team_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Out the Game Dataframe ... notice we have the odds data merged with the win loss data ....\n",
    "#print(\"gameDF\")\n",
    "#gameDF.sort([\"key\"]).show(152)\n",
    "print \"Total finalscoresDF = {0}\".format(finalscoresDF.count())\n",
    "print \"Total oddsDF = {0}\".format(oddsDF.count())\n",
    "print \"Total Games after joining odds and score data = {0}\".format(gameDF.count())\n",
    "\n",
    "#gameDF.select(\"key\").sort([\"key\"]).show(144)\n",
    "\n",
    "finalscoresDF.filter(col(\"key\") == \"2016-04-24.gst.hou\").show()\n",
    "oddsDF.filter(col(\"key\") == \"2016-04-24.gst.hou\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join The Game Dataframe With The Real Time Score Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDF = rtscoresDF.join(gameDF, rtscoresDF[\"key\"] == gameDF[\"key\"], \"inner\").drop(gameDF[\"key\"])\n",
    "print(\"cleanedDF : Cleaned Data Frame for use with ML algos\")\n",
    "cleanedDF.show(3)\n",
    "print \"Total Data Points in rtscoresDF = {0}\".format(rtscoresDF.count())\n",
    "print \"Total Data Points in gameDF = {0}\".format(gameDF.count())\n",
    "print \"Total Data Points in joined cleanedDF = {0}\".format(cleanedDF.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out cleanedDF for followon activities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " cleanedDF.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since the data is small, you can coalesce to a single partition\n",
    "# cleanedDF.coalesce(1).write.partitionBy('away_team_').mode('overwrite').format(\"csv\").save(\"/data2/nba-rt-prediction/sparkfiles/cleanedDF\")\n",
    "cleanedDF.coalesce(1).write.mode('overwrite').format(\"csv\").option(\"header\",\"true\").save(\"/data2/nba-rt-prediction/sparkfiles/cleanedDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
