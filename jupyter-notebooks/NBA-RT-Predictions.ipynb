{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load in Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "#import org.apache.commons.io.IOUtils\n",
    "#import java.net.URL\n",
    "#import java.nio.charset.Charset\n",
    "#import com.databricks.spark.csv\n",
    "#import org.apache.spark.sql.SQLContext\n",
    "#import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,DoubleType,DateType,TimestampType};\n",
    "#import scala.util.matching.Regex\n",
    "#import org.apache.spark.mllib.regression.LabeledPoint\n",
    "#import org.apache.spark.mllib.linalg.Vectors\n",
    "#import org.apache.spark.ml.classification.LogisticRegression\n",
    "#import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "#import org.apache.spark.sql._\n",
    "#import org.apache.spark.sql.types.{StructType,StructField,StringType,DoubleType};\n",
    "#import org.apache.spark.sql.DataFrame\n",
    "#import org.apache.spark.sql.Column\n",
    "from pyspark.sql.functions import *\n",
    "#// Need this for my shorthand $ notation\n",
    "#import sqlContext.implicits._  \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Lookup Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monthMap = {\n",
    "    \"Jan\": \"01\",\n",
    "    \"Feb\": \"02\",\n",
    "    \"Mar\": \"03\",\n",
    "    \"Apr\": \"04\",\n",
    "    \"May\": \"05\",\n",
    "    \"Jun\": \"06\",\n",
    "    \"Jul\": \"07\",\n",
    "    \"Aug\": \"08\",\n",
    "    \"Sep\": \"09\",\n",
    "    \"Oct\": \"10\",\n",
    "    \"Nov\": \"11\",\n",
    "    \"Dec\": \"12\"\n",
    "}\n",
    "\n",
    "teamMap = {\n",
    "  \"Atlanta\" : \"atl\",\n",
    "  \"Boston\"  : \"bos\",\n",
    "  \"Brooklyn\"  : \"bkn\",\n",
    "  \"Charlotte\"  : \"cha\",\n",
    "  \"Chicago\"  : \"chi\",\n",
    "  \"Cleveland\"  : \"cle\",\n",
    "  \"Dallas\"  : \"dal\",\n",
    "  \"Denver\"  : \"den\",\n",
    "  \"Detroit\"  : \"det\",\n",
    "  \"Golden State\"  : \"gst\",\n",
    "  \"Houston\"  : \"hou\",\n",
    "  \"Indiana\"  : \"ind\",\n",
    "  \"LA Clippers\"  : \"lac\",\n",
    "  \"LA Lakers\"  : \"lal\",\n",
    "  \"Memphis\"  : \"mem\",\n",
    "  \"Miami\"  : \"mia\",\n",
    "  \"Milwaukee\"  : \"mil\",\n",
    "  \"Minnesota\"  : \"min\",\n",
    "  \"New Orleans\"  : \"nor\",\n",
    "  \"New York\"  : \"nyk\",\n",
    "  \"Oklahoma City\"  : \"okc\",\n",
    "  \"Orlando\"  : \"orl\",\n",
    "  \"Philadelphia\"  : \"phi\",\n",
    "  \"Phila.\"  : \"phi\",\n",
    "  \"Phoenix\"  : \"pho\",\n",
    "  \"Portland\"  : \"por\",\n",
    "  \"Sacramento\" : \"sac\",\n",
    "  \"San Antonio\"  : \"san\",\n",
    "  \"Toronto\"  : \"tor\",\n",
    "  \"Utah\"  : \"uta\",\n",
    "  \"Washington\"  : \"wsh\",\n",
    "   None : \"none\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load In NBA Score Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+------+------------+------+------------+--------+---------+\n",
      "|  dateOrig|      ts|    teamlonga|scorea|   teamlongb|scoreb|  timestring|timeleft|   gameid|\n",
      "+----------+--------+-------------+------+------------+------+------------+--------+---------+\n",
      "|2016-04-05|15:06:16|      Phoenix|     0|     Atlanta|     0|(8:00 PM ET)|    48.0|400829044|\n",
      "|2016-04-05|15:06:16|      Chicago|     0|     Memphis|     0|(8:00 PM ET)|    48.0|400829045|\n",
      "|2016-04-05|15:06:16|    Cleveland|     0|   Milwaukee|     0|(8:00 PM ET)|    48.0|400829046|\n",
      "|2016-04-05|15:06:16|Oklahoma City|     0|      Denver|     0|(9:00 PM ET)|    48.0|400829047|\n",
      "|2016-04-05|15:06:16|  New Orleans|     0|Philadelphia|     0|(7:00 PM ET)|    48.0|400829041|\n",
      "+----------+--------+-------------+------+------------+------+------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType,DateType\n",
    "\n",
    "customSchema = StructType([\n",
    "    StructField(\"dateOrig\", DateType()),\n",
    "    StructField(\"ts\", StringType()),\n",
    "    StructField(\"teamlonga\", StringType()),\n",
    "    StructField(\"scorea\", IntegerType()),\n",
    "    StructField(\"teamlongb\", StringType()),\n",
    "    StructField(\"scoreb\", IntegerType()),\n",
    "    StructField(\"timestring\", StringType()),\n",
    "    StructField(\"timeleft\", DoubleType()),\n",
    "    StructField(\"gameid\", IntegerType())\n",
    "])\n",
    "\n",
    "nbafile = '/data2/nba-rt-prediction/scoredata/scores_nba.2015.test.dat'\n",
    "rtscoresAndFinalDF = spark.read.format('csv')\\\n",
    "                    .option(\"header\", \"false\")\\\n",
    "                    .option(\"inferSchema\", \"false\")\\\n",
    "                    .option(\"nullValue\", \"empty\")\\\n",
    "                    .option(\"dateFormat\", \"yyyy-MM-dd\")\\\n",
    "                    .option(\"mode\",\"DROPMALFORMED\")\\\n",
    "                    .schema(customSchema)\\\n",
    "                    .load(nbafile).coalesce(2)\n",
    "    \n",
    "rtscoresAndFinalDF.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+------+------------+------+-------------+--------+---------+----+\n",
      "|  dateOrig|      ts|    teamlonga|scorea|   teamlongb|scoreb|   timestring|timeleft|   gameid|blah|\n",
      "+----------+--------+-------------+------+------------+------+-------------+--------+---------+----+\n",
      "|2016-04-05|15:06:16|      Phoenix|     0|     Atlanta|     0| (8:00 PM ET)|    48.0|400829044|   5|\n",
      "|2016-04-05|15:06:16|      Chicago|     0|     Memphis|     0| (8:00 PM ET)|    48.0|400829045|   5|\n",
      "|2016-04-05|15:06:16|    Cleveland|     0|   Milwaukee|     0| (8:00 PM ET)|    48.0|400829046|   5|\n",
      "|2016-04-05|15:06:16|Oklahoma City|     0|      Denver|     0| (9:00 PM ET)|    48.0|400829047|   5|\n",
      "|2016-04-05|15:06:16|  New Orleans|     0|Philadelphia|     0| (7:00 PM ET)|    48.0|400829041|   5|\n",
      "|2016-04-05|15:06:16|      Detroit|     0|       Miami|     0| (8:00 PM ET)|    48.0|400829042|   5|\n",
      "|2016-04-05|15:06:16|    Charlotte|     0|     Toronto|     0| (7:30 PM ET)|    48.0|400829043|   5|\n",
      "|2016-04-05|15:06:16|  San Antonio|     0|        Utah|     0| (9:00 PM ET)|    48.0|400829048|   5|\n",
      "|2016-04-05|15:06:16|     Portland|     0|  Sacramento|     0|(10:00 PM ET)|    48.0|400829049|   5|\n",
      "|2016-04-05|15:06:16|    LA Lakers|     0| LA Clippers|     0|(10:30 PM ET)|    48.0|400829051|   5|\n",
      "|2016-04-05|15:06:16|    Minnesota|     0|Golden State|     0|(10:30 PM ET)|    48.0|400829050|   5|\n",
      "|2016-04-05|15:07:51|      Phoenix|     0|     Atlanta|     0| (8:00 PM ET)|    48.0|400829044|   5|\n",
      "|2016-04-05|15:07:51|      Chicago|     0|     Memphis|     0| (8:00 PM ET)|    48.0|400829045|   5|\n",
      "|2016-04-05|15:07:51|    Cleveland|     0|   Milwaukee|     0| (8:00 PM ET)|    48.0|400829046|   5|\n",
      "|2016-04-05|15:07:51|Oklahoma City|     0|      Denver|     0| (9:00 PM ET)|    48.0|400829047|   5|\n",
      "|2016-04-05|15:07:51|  New Orleans|     0|Philadelphia|     0| (7:00 PM ET)|    48.0|400829041|   5|\n",
      "|2016-04-05|15:07:51|      Detroit|     0|       Miami|     0| (8:00 PM ET)|    48.0|400829042|   5|\n",
      "|2016-04-05|15:07:51|    Charlotte|     0|     Toronto|     0| (7:30 PM ET)|    48.0|400829043|   5|\n",
      "|2016-04-05|15:07:51|  San Antonio|     0|        Utah|     0| (9:00 PM ET)|    48.0|400829048|   5|\n",
      "|2016-04-05|15:07:51|     Portland|     0|  Sacramento|     0|(10:00 PM ET)|    48.0|400829049|   5|\n",
      "+----------+--------+-------------+------+------------+------+-------------+--------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rtscoresAndFinalDF.withColumn(\"blah\", dayofmonth(rtscoresAndFinalDF[\"dateOrig\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+------+------------+------+----------+--------+---------+\n",
      "|  dateOrig|      ts|  teamlonga|scorea|   teamlongb|scoreb|timestring|timeleft|   gameid|\n",
      "+----------+--------+-----------+------+------------+------+----------+--------+---------+\n",
      "|2016-04-05|21:22:09|New Orleans|    93|Philadelphia|   107|   (FINAL)|     0.0|400829041|\n",
      "|2016-04-05|22:08:42|  Charlotte|    90|     Toronto|    96|   (FINAL)|     0.0|400829043|\n",
      "|2016-04-05|22:25:25|    Chicago|    92|     Memphis|   108|   (FINAL)|     0.0|400829045|\n",
      "|2016-04-05|22:28:58|    Phoenix|    90|     Atlanta|   103|   (FINAL)|     0.0|400829044|\n",
      "|2016-04-05|22:30:29|  Cleveland|   109|   Milwaukee|    80|   (FINAL)|     0.0|400829046|\n",
      "+----------+--------+-----------+------+------------+------+----------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+--------+-----------+------+------------+------+-------------+-------------+---------+\n",
      "|  dateOrig|      ts|  teamlonga|scorea|   teamlongb|scoreb|   timestring|     timeleft|   gameid|\n",
      "+----------+--------+-----------+------+------------+------+-------------+-------------+---------+\n",
      "|2016-04-05|19:23:42|New Orleans|    23|Philadelphia|    12|(4:39 IN 1ST)|        40.65|400829041|\n",
      "|2016-04-05|19:23:57|New Orleans|    23|Philadelphia|    14|(4:05 IN 1ST)|40.0833333333|400829041|\n",
      "|2016-04-05|19:24:13|New Orleans|    23|Philadelphia|    14|(3:41 IN 1ST)|39.6833333333|400829041|\n",
      "|2016-04-05|19:24:28|New Orleans|    23|Philadelphia|    14|(3:32 IN 1ST)|39.5333333333|400829041|\n",
      "|2016-04-05|19:24:43|New Orleans|    23|Philadelphia|    16|(3:24 IN 1ST)|         39.4|400829041|\n",
      "+----------+--------+-----------+------+------------+------+-------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rtscoresAndFinalDF.filter(rtscoresAndFinalDF.timestring.contains(\"FINAL\")).show(5)\n",
    "rtscoresAndFinalDF.filter(rtscoresAndFinalDF.timestring.contains(\"1ST\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDFs For Creating Extra Columns In Real Time Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new team name column.. do simple lookup conversion with a UDF\n",
    "def mapper(teamin) :\n",
    "    return teamMap[teamin]\n",
    "\n",
    "mapperudf = udf(mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Date Logic to adjust for games that finish on the day after .... \n",
    "# This is so that I can join them against the spread which was dated the day prior...\n",
    "# This is due to not having a great key to join my tables ...\n",
    "\n",
    "datecrossregex = re.compile(\"^0[0-3]\") # midnight to 3am\n",
    "def dateadjust(datein, tsin ) : \n",
    "    #dateary = datein.split(\"-\")\n",
    "    tsary   = tsin.split(\":\")\n",
    "    sub_one_day = datetime.timedelta(days=1)\n",
    "    newdate = datein\n",
    "    if datecrossregex.match(tsary[0]) :\n",
    "        #day = \"%02d\".format(int(dateary[2]) -1)\n",
    "        #newdate = dateary(0) + \"-\" + dateary(1) + \"-\" + day   \n",
    "        newdate = datein - sub_one_day\n",
    "    return str(newdate)\n",
    "\n",
    "dateadjustudf = udf(dateadjust)\n",
    "\n",
    "\n",
    "# UDFs to create some extra features ... this one is for an experiemental combination of Time left and Score difference.  \n",
    "# Made this via intuition.  This can be extended to add other custom features\n",
    "import math\n",
    "def scoredivtimeXform(numerator, denominator):\n",
    "    rv = numerator/(math.pow(denominator+1,0.5))\n",
    "    return rv\n",
    "scoredivtimeUdf = udf(scoredivtimeXform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle The Real Time And Final Score Data.  Add Columns To The Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Overtime games from this analysis\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.filter(~col(\"timestring\").like(\"%OT%\"))\n",
    "# Create short 3 character team names \n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teama\", mapperudf(col(\"teamlonga\")))\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teamb\", mapperudf(col(\"teamlongb\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a score differential Column \n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"scorea-scoreb\", col(\"scorea\") - col(\"scoreb\"))\n",
    "\n",
    "# Transform the Date.  This is for games that spanned multiple days and gave me a headache.  \n",
    "# Games adjusted to the day they started on.\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"date\",  dateadjustudf(col(\"dateOrig\"),col(\"ts\")))\n",
    "\n",
    "# Create a Key for me to use to join with my odds data later.  Key = date.teama.teamb\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"key\", concat(col(\"date\"),lit(\".\"),col(\"teama\"),lit(\".\"),col(\"teamb\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+------+------------+------+-------------+--------+---------+-----+-----+-------------+----------+------------------+\n",
      "|  dateOrig|      ts|    teamlonga|scorea|   teamlongb|scoreb|   timestring|timeleft|   gameid|teama|teamb|scorea-scoreb|      date|               key|\n",
      "+----------+--------+-------------+------+------------+------+-------------+--------+---------+-----+-----+-------------+----------+------------------+\n",
      "|2016-04-05|15:06:16|      Phoenix|     0|     Atlanta|     0| (8:00 PM ET)|    48.0|400829044|  pho|  atl|            0|2016-04-05|2016-04-05.pho.atl|\n",
      "|2016-04-05|15:06:16|      Chicago|     0|     Memphis|     0| (8:00 PM ET)|    48.0|400829045|  chi|  mem|            0|2016-04-05|2016-04-05.chi.mem|\n",
      "|2016-04-05|15:06:16|    Cleveland|     0|   Milwaukee|     0| (8:00 PM ET)|    48.0|400829046|  cle|  mil|            0|2016-04-05|2016-04-05.cle.mil|\n",
      "|2016-04-05|15:06:16|Oklahoma City|     0|      Denver|     0| (9:00 PM ET)|    48.0|400829047|  okc|  den|            0|2016-04-05|2016-04-05.okc.den|\n",
      "|2016-04-05|15:06:16|  New Orleans|     0|Philadelphia|     0| (7:00 PM ET)|    48.0|400829041|  nor|  phi|            0|2016-04-05|2016-04-05.nor.phi|\n",
      "|2016-04-05|15:06:16|      Detroit|     0|       Miami|     0| (8:00 PM ET)|    48.0|400829042|  det|  mia|            0|2016-04-05|2016-04-05.det.mia|\n",
      "|2016-04-05|15:06:16|    Charlotte|     0|     Toronto|     0| (7:30 PM ET)|    48.0|400829043|  cha|  tor|            0|2016-04-05|2016-04-05.cha.tor|\n",
      "|2016-04-05|15:06:16|  San Antonio|     0|        Utah|     0| (9:00 PM ET)|    48.0|400829048|  san|  uta|            0|2016-04-05|2016-04-05.san.uta|\n",
      "|2016-04-05|15:06:16|     Portland|     0|  Sacramento|     0|(10:00 PM ET)|    48.0|400829049|  por|  sac|            0|2016-04-05|2016-04-05.por.sac|\n",
      "|2016-04-05|15:06:16|    LA Lakers|     0| LA Clippers|     0|(10:30 PM ET)|    48.0|400829051|  lal|  lac|            0|2016-04-05|2016-04-05.lal.lac|\n",
      "|2016-04-05|15:06:16|    Minnesota|     0|Golden State|     0|(10:30 PM ET)|    48.0|400829050|  min|  gst|            0|2016-04-05|2016-04-05.min.gst|\n",
      "|2016-04-05|15:07:51|      Phoenix|     0|     Atlanta|     0| (8:00 PM ET)|    48.0|400829044|  pho|  atl|            0|2016-04-05|2016-04-05.pho.atl|\n",
      "|2016-04-05|15:07:51|      Chicago|     0|     Memphis|     0| (8:00 PM ET)|    48.0|400829045|  chi|  mem|            0|2016-04-05|2016-04-05.chi.mem|\n",
      "|2016-04-05|15:07:51|    Cleveland|     0|   Milwaukee|     0| (8:00 PM ET)|    48.0|400829046|  cle|  mil|            0|2016-04-05|2016-04-05.cle.mil|\n",
      "|2016-04-05|15:07:51|Oklahoma City|     0|      Denver|     0| (9:00 PM ET)|    48.0|400829047|  okc|  den|            0|2016-04-05|2016-04-05.okc.den|\n",
      "|2016-04-05|15:07:51|  New Orleans|     0|Philadelphia|     0| (7:00 PM ET)|    48.0|400829041|  nor|  phi|            0|2016-04-05|2016-04-05.nor.phi|\n",
      "|2016-04-05|15:07:51|      Detroit|     0|       Miami|     0| (8:00 PM ET)|    48.0|400829042|  det|  mia|            0|2016-04-05|2016-04-05.det.mia|\n",
      "|2016-04-05|15:07:51|    Charlotte|     0|     Toronto|     0| (7:30 PM ET)|    48.0|400829043|  cha|  tor|            0|2016-04-05|2016-04-05.cha.tor|\n",
      "|2016-04-05|15:07:51|  San Antonio|     0|        Utah|     0| (9:00 PM ET)|    48.0|400829048|  san|  uta|            0|2016-04-05|2016-04-05.san.uta|\n",
      "|2016-04-05|15:07:51|     Portland|     0|  Sacramento|     0|(10:00 PM ET)|    48.0|400829049|  por|  sac|            0|2016-04-05|2016-04-05.por.sac|\n",
      "+----------+--------+-------------+------+------------+------+-------------+--------+---------+-----+-----+-------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rtscoresAndFinalDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate The Real Time And Final Data From One Common Dataframe To Two Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Currently based on the way the data was sampled, both real time scores and final scores are written as seperate records to the same file.  I need to pull these apart, and then join the dataframes so that I have a real time score and features, and know if the game was won or lost ....\n",
    "\n",
    "# Create Final Score DF\n",
    "# Note a shortcut for repeating the dataframe within the filter is to use a $   df.filter(df(\"foo\").contains ... is equiv to df.filter($\"foo\".contains)\n",
    "\n",
    "var finalscoresDF = rtscoresAndFinalDF.filter($\"time-string\".like(\"%FINAL%\"))\n",
    "\n",
    "# Rename some columns so that join later doesnt have name overlaps\n",
    "finalscoresDF = finalscoresDF.withColumnRenamed(\"scorea\", \"fscorea\")\n",
    "finalscoresDF = finalscoresDF.withColumnRenamed(\"scoreb\", \"fscoreb\")\n",
    "\n",
    "// Create final score difference\n",
    "finalscoresDF = finalscoresDF.withColumn(\"fscorea-fscoreb\", $\"fscorea\" - $\"fscoreb\")\n",
    "\n",
    "// Add a Win/loss column Win = 1, Loss = 0\n",
    "finalscoresDF = finalscoresDF.withColumn(\"win-loss-enc\", ($\"fscorea-fscoreb\" > 0).cast(\"double\"))\n",
    "\n",
    "// Remove Halftime records as this particular case isn't handled well... (for now)\n",
    "var rtscoresDF = rtscoresAndFinalDF.filter(!$\"time-string\".like(\"%FINAL%\")).filter(!$\"time-string\".like(\"HALFTIME\"))\n",
    "\n",
    "// Create final score difference\n",
    "rtscoresDF = rtscoresDF.withColumn(\"scorea-scoreb\", $\"scorea\" - $\"scoreb\")\n",
    "\n",
    "// Create a unique feature based on my custom UDF.  Idea here is that I have intuition that timeleft and score difference are a strong predictor when combined\n",
    "rtscoresDF = rtscoresDF.withColumn(\"score-div-time\", scoredivtimeUdf($\"scorea\" - $\"scoreb\", $\"timeleft\"*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Take A Look Of What We Have For The Two Dataframes We Just Wrangled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Some Printouts .....\n",
    "println(\"final scores data frame\")\n",
    "finalscoresDF.show(5)\n",
    "println(\"real time scores data frame\")\n",
    "rtscoresDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Interpret the Odds data\n",
    "````How to interpret the odds data ...\n",
    "Example Golden State -12.5 O (207.0) -125.0 | Detroit 12.5 U (207.0) 145.0\n",
    "Here Golden State is a 12.5 pt favorite to win.  The over under is in parentheses (207) and is the 50/50 line between teams sum of scores\n",
    "being above/below that line.  \n",
    "Finally the -125 / +145 numbers are whats known at the moneyline odds. \n",
    "    A negative number means you need to bet 125$ to get a 100$ payout\n",
    "    A positive number means you need to bet 100$ to get a 145$ payout\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load In Odds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, the data is very raw, and needs to be pre-processed .  I will start by loading it as an RDD and perform a lot of transformations.  Once I have it properly parsed, I will convert to a dataframe.\n",
    "# This is not beautiful, but gets the job done\n",
    "# Data format .....\n",
    "#       <title>New Orleans 2.5 O (207.0) 125.0 | Phila. -2.5 U (207.0) -145.0 (Apr 05, 2016 07:10 PM)</title>\n",
    "#       <title>Detroit 4.0 O (202.0) 160.0 | Miami -4.0 U (202.0) -190.0 (Apr 05, 2016 08:05 PM)</title>\n",
    "\n",
    "// Reading the data in as an RDD first.  There isn't a dataframe parser for this XML I have, so I will write a custom parser ....\n",
    "val oddsrdd = sc.textFile(oddsfile)\n",
    "\n",
    "// just grabbing the text within the < ... > tags.  I can do this, because the format is super simple and not nested\n",
    "val gameStringRdd = oddsrdd.map(x => x.substring(x.indexOf('>')+1,x.lastIndexOf('<')))\n",
    "\n",
    "// String to Double converter helper\n",
    "def parseDouble(s: String) = try { Some(s.toDouble) } catch { case _ : Throwable => None }\n",
    "\n",
    "// This is where I do the heavy lifting of parsing my XML .. and then finally convert my RDD to a dataframe .....\n",
    "// just lots of string parsing and data type conversions\n",
    "val oddsDF = gameStringRdd.map(x => {\n",
    "  // find the period, and then find the space prior ot the period\n",
    "  // Philadelphia special case.  since later on I index on a decimal point, I am assuming its part of a number and not a team abbrev\n",
    "  // also removing commas with the cryptic filterNot section ... essentially \n",
    "   val x1 = x.replace(\"Phila.\", \"Philadelphia\").filterNot(\",\" contains _)\n",
    "   val ss1 = x1.substring(0,x1.indexOf('.'))\n",
    "   val teamlonga = ss1.substring(0,ss1.lastIndexOf(' '))\n",
    "   val teama = teamMap(teamlonga)\n",
    "   val ss2 = x1.substring(ss1.lastIndexOf(' ')+1,x1.length)\n",
    "   val teamaspread = parseDouble(ss2.substring(0,ss2.indexOf(' ')) )\n",
    "   val ss3 = ss2.substring(ss2.indexOf(' ')+1,ss2.length)\n",
    "   val overunder = parseDouble(ss3.substring(ss3.indexOf('(')+1,ss3.indexOf(')')))\n",
    "   val ss4 = ss3.substring(ss3.indexOf(')')+2,ss3.length)\n",
    "   val teamaml = parseDouble(ss4.substring(0,ss4.indexOf(' ')))\n",
    "   val ss5x = ss4.substring(ss4.indexOf('|')+2,ss4.length)\n",
    "   val ss5 = ss5x.substring(0,ss5x.indexOf('.'))\n",
    "   \n",
    "   val teamlongb = ss5.substring(0,ss5.lastIndexOf(' '))\n",
    "   val teamb = teamMap(teamlongb)\n",
    "   val ss6 = ss5x.substring(ss5.lastIndexOf(' ')+1,ss5x.length)\n",
    "\n",
    "   val teambml = parseDouble(ss6.substring(ss6.indexOf(')')+2,ss6.lastIndexOf('(')-1))\n",
    "   val ss7 = ss6.substring(ss6.lastIndexOf('(')+1,ss6.length)\n",
    "   val dateInfo = ss7.split(' ')\n",
    "   val dateStr = dateInfo(2) + \"-\" + monthMap(dateInfo(0)) + \"-\" + dateInfo(1)\n",
    "   // This will become my join key for the other data sets\n",
    "   val key = dateStr +\".\" + teama + \".\" + teamb\n",
    "  (key,teamlonga,teama,teamaspread,overunder,teamaml,teamlongb,teamb,teambml,dateStr)\n",
    "   \n",
    "  }\n",
    ").toDF(\"key\",\"teamlonga\",\"teama\",\"teamaspread\",\"overunder\",\"teamaml\",\"teamlongb\",\"teamb\",\"teambml\",\"dateStr\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Inspect Some Of The Odds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oddsDF.show(5)\n",
    "printf(\"Total Home Teams      = %2d\\n\",oddsDF.select(\"teama\").distinct.sort(\"teama\").count())\n",
    "printf(\"Total Away Teams      = %2d\\n\",oddsDF.select(\"teamb\").distinct.sort(\"teamb\").count())\n",
    "printf(\"Total Games Collected = %d\\n \",oddsDF.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Join The Odds And Final Score Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Here is where we join the Odds/Realtime scores/ Final Scores into one wholistic data set as input for Logistic Machine Learning\n",
    "\n",
    "// Create a smaller Final Score Dataframe.  Just keep the key, final score a and b, the win/loss indicator\n",
    "var finalslicedscoresDF = finalscoresDF.select($\"key\",$\"fscorea\",$\"fscoreb\",$\"win-loss-enc\")\n",
    "\n",
    "// First Join the 2 smallest data frames ... odd and final.\n",
    "var gameDF = oddsDF.join(finalslicedscoresDF, oddsDF(\"key\") === finalscoresDF(\"key\")).drop(oddsDF(\"key\"))\n",
    "// Drop these redundant columns prior to joining with Realtime score dataframe\n",
    "gameDF = gameDF.drop(\"teamlonga\")\n",
    "gameDF = gameDF.drop(\"teamlongb\")\n",
    "gameDF = gameDF.drop(\"teama\")\n",
    "gameDF = gameDF.drop(\"teamb\")\n",
    "\n",
    "// Print Out the Game Dataframe ... notice we have the odds data merged with the win loss data ....\n",
    "println(\"gameDF\")\n",
    "gameDF.show(3)\n",
    "printf(\"Total Joined Games Collected = %d\\n \",gameDF.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Join The Game Dataframe With The Real Time Score Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val lrDF = rtscoresDF.join(gameDF, rtscoresDF(\"key\") === gameDF(\"key\")).drop(gameDF(\"key\"))\n",
    "println(\"lrDF : Logistic Regression Data Frame\")\n",
    "lrDF.show(3)\n",
    "printf(\"Total Data Points in DataSet = %d\\n \",lrDF.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lets Look At Some Stats From 'Unpacked' Logistic Regression Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " lrDF.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###'Pack' Logistic Dataframe Into Required Format For Logistic Regression [R] - Creating A Simple/Complex Dataframe For Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Logistic regression requires that\n",
    "// The best way I found to modify data in a custom way is with map, however it returns an RDD, so you will need to run toDF at the end\n",
    "// Logistic DF requires a DF of type  => [label: double, features: vector]\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "// These is a helper function that converts an 'Any(Int)' type to a Double or Any(Double) to a Double\n",
    "val ai2d : (Any => Double) = (in:Any) => in.asInstanceOf[java.lang.Integer].doubleValue\n",
    "val ad2d : (Any => Double) = (in:Any) => in.asInstanceOf[java.lang.Double]\n",
    "\n",
    "var nbaSimpleLrDF = lrDF.map {\n",
    "    dfrow => {\n",
    "    // get the indexes ... prob should move this out of the looop!\n",
    "      val tgt = ad2d( dfrow(dfrow.fieldIndex(\"win-loss-enc\")))\n",
    "      val f1  = ai2d( dfrow(dfrow.fieldIndex(\"scorea-scoreb\")))\n",
    "      val f2 =  ad2d( dfrow(dfrow.fieldIndex(\"timeleft\")))\n",
    "      LabeledPoint(tgt,Vectors.dense(f1,f2))\n",
    "    }\n",
    "}.toDF(\"label\",\"features\")\n",
    "\n",
    "var nbaComplexLrDF = lrDF.map {\n",
    "    dfrow => {\n",
    "    // get the indexes ... prob should move this out of the looop!\n",
    "      val tgt = ad2d( dfrow(dfrow.fieldIndex(\"win-loss-enc\")))\n",
    "      val f1  = ai2d( dfrow(dfrow.fieldIndex(\"scorea-scoreb\")))\n",
    "      val f2  = ad2d( dfrow(dfrow.fieldIndex(\"teamaspread\")))\n",
    "      val f3  = ad2d( dfrow(dfrow.fieldIndex(\"overunder\")))\n",
    "      val f4  = ad2d( dfrow(dfrow.fieldIndex(\"teamaml\")))\n",
    "      val f5 =  ad2d( dfrow(dfrow.fieldIndex(\"timeleft\")))\n",
    "      // Add a new feature that weight point differential more as time left gets smaller...\n",
    "      // I played around with excel and this looked ok ....\n",
    "      val f6  = ad2d( dfrow(dfrow.fieldIndex(\"score-div-time\")))\n",
    "      LabeledPoint(tgt,Vectors.dense(f1,f2,f3,f4,f5,f6))\n",
    "    }\n",
    "}.toDF(\"label\",\"features\")\n",
    "\n",
    "\n",
    "nbaSimpleLrDF.show(3)\n",
    "nbaComplexLrDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Create The Model And Train It And Test It "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainAndTest( indf : org.apache.spark.sql.DataFrame, modelPath : String ) : (org.apache.spark.sql.DataFrame, org.apache.spark.ml.classification.LogisticRegressionModel)  = {\n",
    "    val splits = indf.randomSplit(Array(0.60,0.39,0.01), seed = 11L)\n",
    "    val trainingdf = splits(0).cache()\n",
    "    val testdf = splits(1).cache()\n",
    "    val crossvaldf = splits(2).cache()\n",
    "    \n",
    "    println(\"Tranining Samples = \" + trainingdf.count())\n",
    "    println(\"Test      Samples = \" + testdf.count())\n",
    "    println(\"Cross Val Samples = \" + crossvaldf.count())\n",
    "    \n",
    "    // Setup some of the configurations for the Logistic regression model ..\n",
    "    // Here we could try a pipeline with params to select the 'best setting' but in the interest of time will go with this\n",
    "    val lr = new LogisticRegression()\n",
    "      .setMaxIter(50)\n",
    "    .setRegParam(0.10)\n",
    "    .setElasticNetParam(0.0)\n",
    "\n",
    "    // Fit the model\n",
    "    val lrModel   = lr.fit(trainingdf)\n",
    "\n",
    "    println(\"Reg Parameter:    =\" + lrModel.getRegParam)\n",
    "    println(\"lrModel.intercept = \" + lrModel.intercept)\n",
    "    println(\"lrModel.weights   = \" + lrModel.weights)\n",
    "\n",
    "    // Save the model for later use ....\n",
    "    // Argh ! -> in 1.6.1 api, but not 1.5.2 :(  \n",
    "    // lrModel.save(\"modelPath\" )\n",
    "    \n",
    "    ////  Create a logistic regression summary object ////\n",
    "    // val lrSummary = lrModel.summary\n",
    "    // println(\"lrSummary.objectiveHistory = \" + lrSummary.objectiveHistory.length)\n",
    "    // println(lrSummary.objectiveHistory.deep.mkString(\"\\n\"))\n",
    "    ////\n",
    "    \n",
    "    // transform is now used in lieu of predict from mllib.  Found this after studying the API for a while\n",
    "    val predictions = lrModel.transform(testdf)\n",
    "\n",
    "    // Select (prediction, true label) and compute test error\n",
    "    val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setPredictionCol(\"prediction\")\n",
    "      .setMetricName(\"precision\")\n",
    "    \n",
    "    val accuracy = evaluator.evaluate(predictions)\n",
    "    println(\"Test Error = \" + (1.0 - accuracy))\n",
    "\n",
    "    // return the \n",
    "    (predictions,lrModel)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test And Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time[R](block: => R): R = {\n",
    "  val t0 = System.nanoTime()\n",
    "  val result = block\n",
    "  println(\"Elapsed time: \" + (System.nanoTime - t0) + \"ns\")\n",
    "  result\n",
    " }\n",
    " \n",
    "val (simplePredictionDF, simpleModel)   = trainAndTest(nbaSimpleLrDF, \"/data/resources/nbaSimpleModel\")\n",
    "val (complexPredictionDF, complexModel) = trainAndTest(nbaComplexLrDF, \"/data/resources/nbaComplexModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complexPredictionDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper To Unpack MLLIB Label and Features Vector Into A Standard Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// The whole point to doing this is so that I can visualize the features and the outcomes.  Zeppelin cannot render the Vector type well at all, so converting the data back to a flattened style of layout\n",
    "// Expect a column name to be passed that is the features vector, all other columns should come back as-is\n",
    "// I could extend the MLLIB DF to do this !! and  do it\n",
    "\n",
    "\n",
    "\n",
    " // Create a Row from values.\n",
    "// Row(value1, value2, value3, ...)\n",
    " // Create a Row from a Seq of values.\n",
    "// Row.fromSeq(Seq(value1, value2, ...))\n",
    "\n",
    "// Goal of this function to return a new flattened data frame with the Vector datatype replaced with multiple columns.  This\n",
    "// is done so that I can use zeppelin to graph my data since it doesnt really handle vectors well for graphing.\n",
    "// not super flexible as of now, but someday might make it better ...\n",
    "\n",
    "def convertVectorDF (indf : org.apache.spark.sql.DataFrame, labelIndex : Int, featureIndex : Int, probIndex : Int, predictionIndex : Int) = {\n",
    "\n",
    "    //indf.select(colname).show(5)\n",
    "    val myCols = indf.columns\n",
    "    \n",
    "    // debug stuff\n",
    "    println(\"featureIndex value = \" + myCols(featureIndex))\n",
    "    println(\"probIndex value = \" + myCols(probIndex))\n",
    "    println(\"predictionIndex value = \" + myCols(predictionIndex))\n",
    "    \n",
    "    if(myCols(labelIndex) != \"label\") {\n",
    "        println(\"labelIndex value = \" + myCols(labelIndex))\n",
    "        println(\"Error\")\n",
    "    }\n",
    "    \n",
    "    \n",
    "    println(\" myCols.length = \" + myCols.length)\n",
    "    //println(\"Vector Size = \" + indf[0].features)\n",
    "    indf.show(3)\n",
    "    \n",
    "    // build an RDD of sql Rows here, then use toDF to convert back to a Dataframe..\n",
    "    val tmprdd = indf.map { line => \n",
    "        var reind = 0\n",
    "        var newRow : Seq[Double]= Seq(line(labelIndex).asInstanceOf[Double])\n",
    "        var labeledPointVector = line(featureIndex).asInstanceOf[org.apache.spark.mllib.linalg.Vector]\n",
    "        for(i <- 0 to labeledPointVector.size -1 ) {\n",
    "            newRow = newRow :+ labeledPointVector(i).asInstanceOf[Double]\n",
    "        }\n",
    "        var probArray = line(probIndex).asInstanceOf[org.apache.spark.mllib.linalg.Vector]\n",
    "        newRow = newRow :+ probArray(0).asInstanceOf[Double]\n",
    "        newRow = newRow :+ line(predictionIndex).asInstanceOf[Double]\n",
    "        //  https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/sql/Row.html see example where \n",
    "        //  Row.fromSeq is used to make a Row\n",
    "        Row.fromSeq(newRow)\n",
    "    }\n",
    "    //http://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema\n",
    "    //val schema =\n",
    "      // StructType(\n",
    "      // schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n",
    "    //tmprdd\n",
    "    //https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/types/StructType.html\n",
    "\n",
    "    // Take the first row. Each row just has a single 'Row' object at position 0\n",
    "    val numCols = tmprdd.take(1)(0).length\n",
    "   //val struct =\n",
    "    //StructType(\n",
    "     //StructField(\"label\", IntegerType, true) ::\n",
    "     //StructField(\"f1\", LongType, false) ::\n",
    "     //StructField(\"c\", BooleanType, false) :: Nil)\n",
    "     //sqlContext.createDataFrame(tmprdd,schema)\n",
    "    var schemaString = myCols(labelIndex);\n",
    "    for(i <-0 to numCols - 4){\n",
    "        schemaString = schemaString + \",f\" +i \n",
    "    }\n",
    "    schemaString = schemaString + \",\" + myCols(probIndex);\n",
    "    schemaString = schemaString + \",\" + myCols(predictionIndex);\n",
    "    \n",
    "    //schemaString = schemaString + tmprdd.take(1)(0).toString\n",
    "    println(schemaString)\n",
    "    val schema  = StructType(schemaString.split(\",\").map(fieldName => StructField(fieldName,DoubleType,true)))\n",
    "    val finalDF = sqlContext.createDataFrame(tmprdd, schema)\n",
    "    finalDF\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lets Publish The Model and Analysis And Vizualize In Jupyter ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var simpleUnpackedPredictionDF = convertVectorDF(simplePredictionDF,0,1,3,4)\n",
    "simpleUnpackedPredictionDF = simpleUnpackedPredictionDF.withColumn(\"correct\", when($\"label\" === $\"prediction\",1).otherwise(0))\n",
    "\n",
    "//var complexUnpackedPredictionDF = convertVectorDF(complexPredictionDF,0,1,3,4)\n",
    "//complexPredictionDF = complexPredictionDF.withColumn(\"correct\", when($\"label\" === $\"prediction\",1).otherwise(0))\n",
    "\n",
    "simpleUnpackedPredictionDF.write.format(\"json\").save(\"swift://notebooks.spark/test/simple001.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simpleUnpackedPredictionDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%AddJar https://github.com/dustinvanstee/random-public-files/raw/master/SystemML.jar\n",
    "import org.apache.sysml.api.MLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.sysml.api.MLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
