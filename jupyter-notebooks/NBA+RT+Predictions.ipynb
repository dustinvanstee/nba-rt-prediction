{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load in Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Jars (might remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached version of spark-csv_2.11-1.4.0.jar\n",
      "Using cached version of commons-csv-1.2.dv2.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar  http://repo1.maven.org/maven2/com/databricks/spark-csv_2.11/1.4.0/spark-csv_2.11-1.4.0.jar\n",
    "%AddJar  https://github.com/dustinvanstee/bluemix_jars/raw/master/commons-csv-1.2.dv2.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.commons.io.IOUtils\n",
    "import java.net.URL\n",
    "import java.nio.charset.Charset\n",
    "import com.databricks.spark.csv\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,DoubleType,DateType,TimestampType};\n",
    "import scala.util.matching.Regex\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types.{StructType,StructField,StringType,DoubleType};\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.functions.{udf,col,concat,lit,when}\n",
    "// Need this for my shorthand $ notation\n",
    "import sqlContext.implicits._  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Boilerplate for loading data from Swift object store - REVAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setConfig(credentials : scala.collection.mutable.HashMap[String, String]) = {\n",
    "    val prefix = \"fs.swift.service.\" + credentials(\"name\") \n",
    "    var hconf = sc.getConf\n",
    "    hconf.set(prefix + \".auth.url\", credentials(\"auth_url\")+\"/v3/auth/tokens\")\n",
    "    hconf.set(prefix + \".auth.endpoint.prefix\", \"endpoints\")\n",
    "    hconf.set(prefix + \".tenant\", credentials(\"project_id\"))\n",
    "    hconf.set(prefix + \".username\", credentials(\"user_id\"))\n",
    "    hconf.set(prefix + \".password\", credentials(\"password\"))\n",
    "    hconf.set(prefix + \".http.port\", \"8080\")\n",
    "    hconf.set(prefix + \".region\", credentials(\"region\"))\n",
    "    hconf.set(prefix + \".public\", \"True\")   \n",
    "}\n",
    "\n",
    "var credentials_1 = scala.collection.mutable.HashMap[String, String](\n",
    "  \"auth_url\"->\"https://identity.open.softlayer.com\",\n",
    "  \"project\"->\"object_storage_0dc58f04_2c63_4761_9aed_ef4449bae858\",\n",
    "  \"project_id\"->\"f4162fef734849528c38937ca26ba3a7\",\n",
    "  \"region\"->\"dallas\",\n",
    "  \"user_id\"->\"e4fc729439f044199998e68ef469e3c5\",\n",
    "  \"domain_id\"->\"7527e49e459a4de4a41373a7e8fb444f\",\n",
    "  \"domain_name\"->\"984499\",\n",
    "  \"username\"->\"Admin_66474152b3dcb39af2943f3f79bb2618f028659f\",\n",
    "  \"password\"->\"\"\"pIv/Y^MOZ17^LQ{j\"\"\",\n",
    "  \"filename\"->\"scores_nba.test.dat\",\n",
    "  \"container\"->\"notebooks\",\n",
    "  \"tenantId\"->\"s69c-1c345cae431193-591ae43264f0\"\n",
    ")\n",
    "\n",
    "credentials_1(\"name\") = \"spark\"\n",
    "setConfig(credentials_1)\n",
    "//val nbafile = sc.textFile(\"swift://notebooks.\" + credentials_1(\"name\") + \"/\" + credentials_1(\"filename\"))\n",
    "\n",
    "val nbafile  = \"swift://notebooks.spark/scores_nba.test.dat\"\n",
    "val oddsfile = \"swift://notebooks.spark/nbaodds_042516.xml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Lookup Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version = 1.6.0\n"
     ]
    }
   ],
   "source": [
    "println(\"Spark version = \" + sc.version)\n",
    "val teamMap = Map(\n",
    "  \"Atlanta\" -> \"atl\",\n",
    "  \"Boston\"  -> \"bos\",\n",
    "  \"Brooklyn\"  -> \"bkn\",\n",
    "  \"Charlotte\"  -> \"cha\",\n",
    "  \"Chicago\"  -> \"chi\",\n",
    "  \"Cleveland\"  -> \"cle\",\n",
    "  \"Dallas\"  -> \"dal\",\n",
    "  \"Denver\"  -> \"den\",\n",
    "  \"Detroit\"  -> \"det\",\n",
    "  \"Golden State\"  -> \"gst\",\n",
    "  \"Houston\"  -> \"hou\",\n",
    "  \"Indiana\"  -> \"ind\",\n",
    "  \"LA Clippers\"  -> \"lac\",\n",
    "  \"LA Lakers\"  -> \"lal\",\n",
    "  \"Memphis\"  -> \"mem\",\n",
    "  \"Miami\"  -> \"mia\",\n",
    "  \"Milwaukee\"  -> \"mil\",\n",
    "  \"Minnesota\"  -> \"min\",\n",
    "  \"New Orleans\"  -> \"nor\",\n",
    "  \"New York\"  -> \"nyk\",\n",
    "  \"Oklahoma City\"  -> \"okc\",\n",
    "  \"Orlando\"  -> \"orl\",\n",
    "  \"Philadelphia\"  -> \"phi\",\n",
    "  \"Phila.\"  -> \"phi\",\n",
    "  \"Phoenix\"  -> \"pho\",\n",
    "  \"Portland\"  -> \"por\",\n",
    "  \"Sacramento\" -> \"sac\",\n",
    "  \"San Antonio\"  -> \"san\",\n",
    "  \"Toronto\"  -> \"tor\",\n",
    "  \"Utah\"  -> \"uta\",\n",
    "  \"Washington\"  -> \"wsh\")\n",
    "  \n",
    "val monthMap = Map(\n",
    "    \"Jan\"-> \"01\",\n",
    "    \"Feb\"-> \"02\",\n",
    "    \"Mar\"-> \"03\",\n",
    "    \"Apr\"-> \"04\",\n",
    "    \"May\"-> \"05\",\n",
    "    \"Jun\"-> \"06\",\n",
    "    \"Jul\"-> \"07\",\n",
    "    \"Aug\"-> \"08\",\n",
    "    \"Sep\"-> \"09\",\n",
    "    \"Oct\"-> \"10\",\n",
    "    \"Nov\"-> \"11\",\n",
    "    \"Dec\"-> \"12\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swift://notebooks.spark/scores_nba.test.dat\n",
      "head: cannot open 'swift://notebooks.spark/scores_nba.test.dat' for reading: No such file or directory\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "/dev/sda3       930G  118G  813G  13% /\n",
      "devtmpfs        189G     0  189G   0% /dev\n",
      "tmpfs           189G     0  189G   0% /dev/shm\n",
      "tmpfs           189G  218M  189G   1% /run\n",
      "tmpfs           189G     0  189G   0% /sys/fs/cgroup\n",
      "/dev/sde1       3.6T   89M  3.4T   1% /disk4\n",
      "/dev/sdh1       3.6T   89M  3.4T   1% /disk7\n",
      "/dev/sdg1       3.6T   89M  3.4T   1% /disk6\n",
      "/dev/sdj1       3.6T   89M  3.4T   1% /disk9\n",
      "/dev/sdb1       3.6T  9.4G  3.4T   1% /disk1\n",
      "/dev/sdi1       3.6T   89M  3.4T   1% /disk8\n",
      "/dev/sdf1       3.6T   89M  3.4T   1% /disk5\n",
      "/dev/sdd1       3.6T   89M  3.4T   1% /disk3\n",
      "/dev/sdc1       3.6T   89M  3.4T   1% /disk2\n",
      "/dev/sda1       253M  161M   93M  64% /boot\n",
      "tmpfs            38G     0   38G   0% /run/user/0\n",
      "/dev/fs01       246T   80T  166T  33% /gpfs/global_fs01\n",
      "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s69c-1c345cae431193-591ae43264f0/notebook/notebooks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO ## cat a file in swift ?? Prob not possible\n",
    "println(nbafile)\n",
    "import sys.process._\n",
    "\"head swift://notebooks.spark/scores_nba.test.dat\".!\n",
    "\"df -h\".!\n",
    "\"pwd\".!\n",
    "\"touch /gpfs/global_fs01/sym_shared/YPProdSpark/user/s69c-1c345cae431193-591ae43264f0/notebook/notebooks/junk\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Load In NBA Score Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+------+------------+------+------------+--------+---------+\n",
      "|  dateOrig|      ts|    teamlonga|scorea|   teamlongb|scoreb| time-string|timeleft|   gameid|\n",
      "+----------+--------+-------------+------+------------+------+------------+--------+---------+\n",
      "|2016-04-05|15:06:16|      Phoenix|     0|     Atlanta|     0|(8:00 PM ET)|    48.0|400829044|\n",
      "|2016-04-05|15:06:16|      Chicago|     0|     Memphis|     0|(8:00 PM ET)|    48.0|400829045|\n",
      "|2016-04-05|15:06:16|    Cleveland|     0|   Milwaukee|     0|(8:00 PM ET)|    48.0|400829046|\n",
      "|2016-04-05|15:06:16|Oklahoma City|     0|      Denver|     0|(9:00 PM ET)|    48.0|400829047|\n",
      "|2016-04-05|15:06:16|  New Orleans|     0|Philadelphia|     0|(7:00 PM ET)|    48.0|400829041|\n",
      "+----------+--------+-------------+------+------------+------+------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Since I dont have a header in the data set, I want to specify the column metadata\n",
    "val sqlContext = new SQLContext(sc)\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "    StructField(\"dateOrig\", DateType, true),\n",
    "    StructField(\"ts\", StringType, true),\n",
    "    StructField(\"teamlonga\", StringType, true),\n",
    "    StructField(\"scorea\", IntegerType, true),\n",
    "    StructField(\"teamlongb\", StringType, true),\n",
    "    StructField(\"scoreb\", IntegerType, true),\n",
    "    StructField(\"time-string\", StringType, true),\n",
    "    StructField(\"timeleft\", DoubleType, true),\n",
    "    StructField(\"gameid\", IntegerType, true)\n",
    "    ))\n",
    "\n",
    "// This line reads in the file and parses it with a CSV reader\n",
    "// Use first line of all files as header.\n",
    "// Automatically infer data types)\n",
    "var rtscoresAndFinalDF = sqlContext.read.format(\"com.databricks.spark.csv\").\n",
    "    option(\"header\", \"false\").\n",
    "    option(\"inferSchema\", \"false\").\n",
    "    option(\"nullValue\", \"empty\").\n",
    "    option(\"dateFormat\", \"yyyy-MM-dd\").\n",
    "    option(\"mode\",\"DROPMALFORMED\").\n",
    "    schema(customSchema).load(nbafile) \n",
    "    \n",
    "rtscoresAndFinalDF.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+------+------------+------+-----------+--------+---------+\n",
      "|  dateOrig|      ts|  teamlonga|scorea|   teamlongb|scoreb|time-string|timeleft|   gameid|\n",
      "+----------+--------+-----------+------+------------+------+-----------+--------+---------+\n",
      "|2016-04-05|21:22:09|New Orleans|    93|Philadelphia|   107|    (FINAL)|     0.0|400829041|\n",
      "|2016-04-05|22:08:42|  Charlotte|    90|     Toronto|    96|    (FINAL)|     0.0|400829043|\n",
      "|2016-04-05|22:25:25|    Chicago|    92|     Memphis|   108|    (FINAL)|     0.0|400829045|\n",
      "|2016-04-05|22:28:58|    Phoenix|    90|     Atlanta|   103|    (FINAL)|     0.0|400829044|\n",
      "|2016-04-05|22:30:29|  Cleveland|   109|   Milwaukee|    80|    (FINAL)|     0.0|400829046|\n",
      "+----------+--------+-----------+------+------------+------+-----------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rtscoresAndFinalDF.filter($\"time-string\".contains(\"FINAL\")).show(5)\n",
    "rtscoresAndFinalDF.filter(rtscoresAndFinalDF(\"time-string\").contains(\"1ST\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Interpret the Odds data\n",
    "````How to interpret the odds data ...\n",
    "Example Golden State -12.5 O (207.0) -125.0 | Detroit 12.5 U (207.0) 145.0\n",
    "Here Golden State is a 12.5 pt favorite to win.  The over under is in parentheses (207) and is the 50/50 line between teams sum of scores\n",
    "being above/below that line.  \n",
    "Finally the -125 / +145 numbers are whats known at the moneyline odds. \n",
    "    A negative number means you need to bet 125$ to get a 100$ payout\n",
    "    A positive number means you need to bet 100$ to get a 145$ payout\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDFs For Creating Extra Columns In Real Time Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create new team name column.. do simple lookup conversion with a UDF\n",
    "val mapper = teamin => teamMap(teamin)\n",
    "val mapperudf = udf(mapper)\n",
    "\n",
    "\n",
    "// Date Logic to adjust for games that finish on the day after ....\n",
    "// This is due to not having a great key to join my tables ...\n",
    "val datecrossregex = new Regex(\"^0[0-3]\")\n",
    "val dateadjust : ((String, String) => String) = (datein, tsin ) => {\n",
    "    val datetest =  datecrossregex.findFirstIn(tsin)\n",
    "    val dateary = datein.split(\"-\")\n",
    "    val rv = datetest match {\n",
    "      case Some(s) => { \n",
    "         val day = \"%02d\".format(dateary(2).toInt -1)\n",
    "         val newdate = dateary(0) + \"-\" + dateary(1) + \"-\" + day  \n",
    "         newdate\n",
    "      }\n",
    "      case None => datein\n",
    "    }\n",
    "    rv.asInstanceOf[String]\n",
    "}\n",
    "val dateadjustudf = udf(dateadjust)\n",
    "\n",
    "// UDFs to create some extra features ... this one is for an experiemental combination of Time left and Score difference.  \n",
    "// Made this via intuition.  This can be extended to add other custom features\n",
    "val scoredivtimeXform: ((Double,Double) => Double) = (num:Double, den:Double) => (num/(Math.pow(den+1,0.5)))\n",
    "val scoredivtimeUdf = udf(scoredivtimeXform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Wrangle The Real Time And Final Score Data.  Add Columns To The Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Remove Overtime games from this analysis\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.filter(!$\"time-string\".like(\"%OT%\"))\n",
    "\n",
    "// Create short 3 character team names \n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teama\", mapperudf(col(\"teamlonga\")))\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teamb\", mapperudf(col(\"teamlongb\")))\n",
    "\n",
    "// Add a score differential Column \n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"scorea-scoreb\", $\"scorea\" - $\"scoreb\")\n",
    "\n",
    "// Transform the Date.  This is for games that spanned multiple days and gave me a headache.  \n",
    "// Games adjusted to the day they started on.\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"date\",  dateadjustudf($\"dateOrig\",$\"ts\"))\n",
    "\n",
    "// Create a Key for me to use to join with my odds data later.  Key = date.teama.teamb\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"key\", concat($\"date\",lit(\".\"),$\"teama\",lit(\".\"),$\"teamb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Separate The Real Time And Final Data From One Common Dataframe To Two Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Currently based on the way the data was sampled, both real time scores and final scores are written as seperate records to the same file.  I need to pull these apart, and then join the dataframes so that I have a real time score and features, and know if the game was won or lost ....\n",
    "\n",
    "// Create Final Score DF\n",
    "// Note a shortcut for repeating the dataframe within the filter is to use a $   df.filter(df(\"foo\").contains ... is equiv to df.filter($\"foo\".contains)\n",
    "\n",
    "var finalscoresDF = rtscoresAndFinalDF.filter($\"time-string\".like(\"%FINAL%\"))\n",
    "\n",
    "// Rename some columns so that join later doesnt have name overlaps\n",
    "finalscoresDF = finalscoresDF.withColumnRenamed(\"scorea\", \"fscorea\")\n",
    "finalscoresDF = finalscoresDF.withColumnRenamed(\"scoreb\", \"fscoreb\")\n",
    "\n",
    "// Create final score difference\n",
    "finalscoresDF = finalscoresDF.withColumn(\"fscorea-fscoreb\", $\"fscorea\" - $\"fscoreb\")\n",
    "\n",
    "// Add a Win/loss column Win = 1, Loss = 0\n",
    "finalscoresDF = finalscoresDF.withColumn(\"win-loss-enc\", ($\"fscorea-fscoreb\" > 0).cast(\"double\"))\n",
    "\n",
    "// Remove Halftime records as this particular case isn't handled well... (for now)\n",
    "var rtscoresDF = rtscoresAndFinalDF.filter(!$\"time-string\".like(\"%FINAL%\")).filter(!$\"time-string\".like(\"HALFTIME\"))\n",
    "\n",
    "// Create final score difference\n",
    "rtscoresDF = rtscoresDF.withColumn(\"scorea-scoreb\", $\"scorea\" - $\"scoreb\")\n",
    "\n",
    "// Create a unique feature based on my custom UDF.  Idea here is that I have intuition that timeleft and score difference are a strong predictor when combined\n",
    "rtscoresDF = rtscoresDF.withColumn(\"score-div-time\", scoredivtimeUdf($\"scorea\" - $\"scoreb\", $\"timeleft\"*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lets Take A Look Of What We Have For The Two Dataframes We Just Wrangled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final scores data frame\n",
      "+----------+--------+-----------+-------+------------+-------+-----------+--------+---------+-----+-----+-------------+----------+------------------+---------------+------------+\n",
      "|  dateOrig|      ts|  teamlonga|fscorea|   teamlongb|fscoreb|time-string|timeleft|   gameid|teama|teamb|scorea-scoreb|      date|               key|fscorea-fscoreb|win-loss-enc|\n",
      "+----------+--------+-----------+-------+------------+-------+-----------+--------+---------+-----+-----+-------------+----------+------------------+---------------+------------+\n",
      "|2016-04-05|21:22:09|New Orleans|     93|Philadelphia|    107|    (FINAL)|     0.0|400829041|  nor|  phi|          -14|2016-04-05|2016-04-05.nor.phi|            -14|         0.0|\n",
      "|2016-04-05|22:08:42|  Charlotte|     90|     Toronto|     96|    (FINAL)|     0.0|400829043|  cha|  tor|           -6|2016-04-05|2016-04-05.cha.tor|             -6|         0.0|\n",
      "|2016-04-05|22:25:25|    Chicago|     92|     Memphis|    108|    (FINAL)|     0.0|400829045|  chi|  mem|          -16|2016-04-05|2016-04-05.chi.mem|            -16|         0.0|\n",
      "|2016-04-05|22:28:58|    Phoenix|     90|     Atlanta|    103|    (FINAL)|     0.0|400829044|  pho|  atl|          -13|2016-04-05|2016-04-05.pho.atl|            -13|         0.0|\n",
      "|2016-04-05|22:30:29|  Cleveland|    109|   Milwaukee|     80|    (FINAL)|     0.0|400829046|  cle|  mil|           29|2016-04-05|2016-04-05.cle.mil|             29|         1.0|\n",
      "+----------+--------+-----------+-------+------------+-------+-----------+--------+---------+-----+-----+-------------+----------+------------------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "real time scores data frame\n"
     ]
    }
   ],
   "source": [
    "// Some Printouts .....\n",
    "println(\"final scores data frame\")\n",
    "finalscoresDF.show(5)\n",
    "println(\"real time scores data frame\")\n",
    "rtscoresDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Load In Odds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Here, the data is very raw, and needs to be pre-processed .  I will start by loading it as an RDD and perform a lot of transformations.  Once I have it properly parsed, I will convert to a dataframe.\n",
    "// This is not beautiful, but gets the job done\n",
    "// Data format .....\n",
    "//       <title>New Orleans 2.5 O (207.0) 125.0 | Phila. -2.5 U (207.0) -145.0 (Apr 05, 2016 07:10 PM)</title>\n",
    "//       <title>Detroit 4.0 O (202.0) 160.0 | Miami -4.0 U (202.0) -190.0 (Apr 05, 2016 08:05 PM)</title>\n",
    "\n",
    "// Reading the data in as an RDD first.  There isn't a dataframe parser for this XML I have, so I will write a custom parser ....\n",
    "val oddsrdd = sc.textFile(oddsfile)\n",
    "\n",
    "// just grabbing the text within the < ... > tags.  I can do this, because the format is super simple and not nested\n",
    "val gameStringRdd = oddsrdd.map(x => x.substring(x.indexOf('>')+1,x.lastIndexOf('<')))\n",
    "\n",
    "// String to Double converter helper\n",
    "def parseDouble(s: String) = try { Some(s.toDouble) } catch { case _ : Throwable => None }\n",
    "\n",
    "// This is where I do the heavy lifting of parsing my XML .. and then finally convert my RDD to a dataframe .....\n",
    "// just lots of string parsing and data type conversions\n",
    "val oddsDF = gameStringRdd.map(x => {\n",
    "  // find the period, and then find the space prior ot the period\n",
    "  // Philadelphia special case.  since later on I index on a decimal point, I am assuming its part of a number and not a team abbrev\n",
    "  // also removing commas with the cryptic filterNot section ... essentially \n",
    "   val x1 = x.replace(\"Phila.\", \"Philadelphia\").filterNot(\",\" contains _)\n",
    "   val ss1 = x1.substring(0,x1.indexOf('.'))\n",
    "   val teamlonga = ss1.substring(0,ss1.lastIndexOf(' '))\n",
    "   val teama = teamMap(teamlonga)\n",
    "   val ss2 = x1.substring(ss1.lastIndexOf(' ')+1,x1.length)\n",
    "   val teamaspread = parseDouble(ss2.substring(0,ss2.indexOf(' ')) )\n",
    "   val ss3 = ss2.substring(ss2.indexOf(' ')+1,ss2.length)\n",
    "   val overunder = parseDouble(ss3.substring(ss3.indexOf('(')+1,ss3.indexOf(')')))\n",
    "   val ss4 = ss3.substring(ss3.indexOf(')')+2,ss3.length)\n",
    "   val teamaml = parseDouble(ss4.substring(0,ss4.indexOf(' ')))\n",
    "   val ss5x = ss4.substring(ss4.indexOf('|')+2,ss4.length)\n",
    "   val ss5 = ss5x.substring(0,ss5x.indexOf('.'))\n",
    "   \n",
    "   val teamlongb = ss5.substring(0,ss5.lastIndexOf(' '))\n",
    "   val teamb = teamMap(teamlongb)\n",
    "   val ss6 = ss5x.substring(ss5.lastIndexOf(' ')+1,ss5x.length)\n",
    "\n",
    "   val teambml = parseDouble(ss6.substring(ss6.indexOf(')')+2,ss6.lastIndexOf('(')-1))\n",
    "   val ss7 = ss6.substring(ss6.lastIndexOf('(')+1,ss6.length)\n",
    "   val dateInfo = ss7.split(' ')\n",
    "   val dateStr = dateInfo(2) + \"-\" + monthMap(dateInfo(0)) + \"-\" + dateInfo(1)\n",
    "   // This will become my join key for the other data sets\n",
    "   val key = dateStr +\".\" + teama + \".\" + teamb\n",
    "  (key,teamlonga,teama,teamaspread,overunder,teamaml,teamlongb,teamb,teambml,dateStr)\n",
    "   \n",
    "  }\n",
    ").toDF(\"key\",\"teamlonga\",\"teama\",\"teamaspread\",\"overunder\",\"teamaml\",\"teamlongb\",\"teamb\",\"teambml\",\"dateStr\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Inspect Some Of The Odds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+-----+-----------+---------+-------+------------+-----+-------+----------+\n",
      "|               key|  teamlonga|teama|teamaspread|overunder|teamaml|   teamlongb|teamb|teambml|   dateStr|\n",
      "+------------------+-----------+-----+-----------+---------+-------+------------+-----+-------+----------+\n",
      "|2016-04-05.nor.phi|New Orleans|  nor|        2.5|    207.0|  125.0|Philadelphia|  phi| -145.0|2016-04-05|\n",
      "|2016-04-05.det.mia|    Detroit|  det|        4.0|    202.0|  160.0|       Miami|  mia| -190.0|2016-04-05|\n",
      "|2016-04-05.cha.tor|  Charlotte|  cha|        4.0|    200.5|  155.0|     Toronto|  tor| -175.0|2016-04-05|\n",
      "|2016-04-05.pho.atl|    Phoenix|  pho|       14.5|    207.5| -110.0|     Atlanta|  atl| -110.0|2016-04-05|\n",
      "|2016-04-05.chi.mem|    Chicago|  chi|       -3.0|    201.5| -150.0|     Memphis|  mem|  130.0|2016-04-05|\n",
      "+------------------+-----------+-----+-----------+---------+-------+------------+-----+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total Home Teams      = 30\n",
      "Total Away Teams      = 30\n"
     ]
    }
   ],
   "source": [
    "oddsDF.show(5)\n",
    "printf(\"Total Home Teams      = %2d\\n\",oddsDF.select(\"teama\").distinct.sort(\"teama\").count())\n",
    "printf(\"Total Away Teams      = %2d\\n\",oddsDF.select(\"teamb\").distinct.sort(\"teamb\").count())\n",
    "printf(\"Total Games Collected = %d\\n \",oddsDF.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Join The Odds And Final Score Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gameDF\n",
      "+-----------+---------+-------+-------+----------+------------------+-------+-------+------------+\n",
      "|teamaspread|overunder|teamaml|teambml|   dateStr|               key|fscorea|fscoreb|win-loss-enc|\n",
      "+-----------+---------+-------+-------+----------+------------------+-------+-------+------------+\n",
      "|        5.0|    206.0|  170.0| -200.0|2016-04-13|2016-04-13.lac.pho|    105|    114|         0.0|\n",
      "|        5.0|    201.0|  175.0| -210.0|2016-04-20|2016-04-20.cha.mia|    103|    115|         0.0|\n",
      "|        4.5|    201.0|  170.0| -200.0|2016-04-20|2016-04-20.cha.mia|    103|    115|         0.0|\n",
      "+-----------+---------+-------+-------+----------+------------------+-------+-------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Here is where we join the Odds/Realtime scores/ Final Scores into one wholistic data set as input for Logistic Machine Learning\n",
    "\n",
    "// Create a smaller Final Score Dataframe.  Just keep the key, final score a and b, the win/loss indicator\n",
    "var finalslicedscoresDF = finalscoresDF.select($\"key\",$\"fscorea\",$\"fscoreb\",$\"win-loss-enc\")\n",
    "\n",
    "// First Join the 2 smallest data frames ... odd and final.\n",
    "var gameDF = oddsDF.join(finalslicedscoresDF, oddsDF(\"key\") === finalscoresDF(\"key\")).drop(oddsDF(\"key\"))\n",
    "// Drop these redundant columns prior to joining with Realtime score dataframe\n",
    "gameDF = gameDF.drop(\"teamlonga\")\n",
    "gameDF = gameDF.drop(\"teamlongb\")\n",
    "gameDF = gameDF.drop(\"teama\")\n",
    "gameDF = gameDF.drop(\"teamb\")\n",
    "\n",
    "// Print Out the Game Dataframe ... notice we have the odds data merged with the win loss data ....\n",
    "println(\"gameDF\")\n",
    "gameDF.show(3)\n",
    "printf(\"Total Joined Games Collected = %d\\n \",gameDF.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Join The Game Dataframe With The Real Time Score Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrDF : Logistic Regression Data Frame\n",
      "+----------+--------+-----------+------+---------+------+--------------+-------------+---------+-----+-----+-------------+----------+--------------------+-----------+---------+-------+-------+----------+------------------+-------+-------+------------+\n",
      "|  dateOrig|      ts|  teamlonga|scorea|teamlongb|scoreb|   time-string|     timeleft|   gameid|teama|teamb|scorea-scoreb|      date|      score-div-time|teamaspread|overunder|teamaml|teambml|   dateStr|               key|fscorea|fscoreb|win-loss-enc|\n",
      "+----------+--------+-----------+------+---------+------+--------------+-------------+---------+-----+-----+-------------+----------+--------------------+-----------+---------+-------+-------+----------+------------------+-------+-------+------------+\n",
      "|2016-04-13|12:12:41|LA Clippers|     0|  Phoenix|     0| (10:30 PM ET)|         48.0|400829116|  lac|  pho|            0|2016-04-13|                 0.0|        5.0|    206.0|  170.0| -200.0|2016-04-13|2016-04-13.lac.pho|    105|    114|         0.0|\n",
      "|2016-04-13|22:43:25|LA Clippers|     2|  Phoenix|     4|(10:25 IN 1ST)|46.4166666667|400829116|  lac|  pho|           -2|2016-04-13|-0.20646736890536738|        5.0|    206.0|  170.0| -200.0|2016-04-13|2016-04-13.lac.pho|    105|    114|         0.0|\n",
      "|2016-04-13|22:43:55|LA Clippers|     2|  Phoenix|     4| (9:53 IN 1ST)|45.8833333333|400829116|  lac|  pho|           -2|2016-04-13| -0.2076509966255876|        5.0|    206.0|  170.0| -200.0|2016-04-13|2016-04-13.lac.pho|    105|    114|         0.0|\n",
      "+----------+--------+-----------+------+---------+------+--------------+-------------+---------+-----+-----+-------------+----------+--------------------+-----------+---------+-------+-------+----------+------------------+-------+-------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val lrDF = rtscoresDF.join(gameDF, rtscoresDF(\"key\") === gameDF(\"key\")).drop(gameDF(\"key\"))\n",
    "println(\"lrDF : Logistic Regression Data Frame\")\n",
    "lrDF.show(3)\n",
    "printf(\"Total Data Points in DataSet = %d\\n \",lrDF.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lets Look At Some Stats From 'Unpacked' Logistic Regression Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " lrDF.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###'Pack' Logistic Dataframe Into Required Format For Logistic Regression [R] - Creating A Simple/Complex Dataframe For Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|          [0.0,48.0]|\n",
      "|  0.0|[-2.0,46.4166666667]|\n",
      "|  0.0|[-2.0,45.8833333333]|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Logistic regression requires that\n",
    "// The best way I found to modify data in a custom way is with map, however it returns an RDD, so you will need to run toDF at the end\n",
    "// Logistic DF requires a DF of type  => [label: double, features: vector]\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "// These is a helper function that converts an 'Any(Int)' type to a Double or Any(Double) to a Double\n",
    "val ai2d : (Any => Double) = (in:Any) => in.asInstanceOf[java.lang.Integer].doubleValue\n",
    "val ad2d : (Any => Double) = (in:Any) => in.asInstanceOf[java.lang.Double]\n",
    "\n",
    "var nbaSimpleLrDF = lrDF.map {\n",
    "    dfrow => {\n",
    "    // get the indexes ... prob should move this out of the looop!\n",
    "      val tgt = ad2d( dfrow(dfrow.fieldIndex(\"win-loss-enc\")))\n",
    "      val f1  = ai2d( dfrow(dfrow.fieldIndex(\"scorea-scoreb\")))\n",
    "      val f2 =  ad2d( dfrow(dfrow.fieldIndex(\"timeleft\")))\n",
    "      LabeledPoint(tgt,Vectors.dense(f1,f2))\n",
    "    }\n",
    "}.toDF(\"label\",\"features\")\n",
    "\n",
    "var nbaComplexLrDF = lrDF.map {\n",
    "    dfrow => {\n",
    "    // get the indexes ... prob should move this out of the looop!\n",
    "      val tgt = ad2d( dfrow(dfrow.fieldIndex(\"win-loss-enc\")))\n",
    "      val f1  = ai2d( dfrow(dfrow.fieldIndex(\"scorea-scoreb\")))\n",
    "      val f2  = ad2d( dfrow(dfrow.fieldIndex(\"teamaspread\")))\n",
    "      val f3  = ad2d( dfrow(dfrow.fieldIndex(\"overunder\")))\n",
    "      val f4  = ad2d( dfrow(dfrow.fieldIndex(\"teamaml\")))\n",
    "      val f5 =  ad2d( dfrow(dfrow.fieldIndex(\"timeleft\")))\n",
    "      // Add a new feature that weight point differential more as time left gets smaller...\n",
    "      // I played around with excel and this looked ok ....\n",
    "      val f6  = ad2d( dfrow(dfrow.fieldIndex(\"score-div-time\")))\n",
    "      LabeledPoint(tgt,Vectors.dense(f1,f2,f3,f4,f5,f6))\n",
    "    }\n",
    "}.toDF(\"label\",\"features\")\n",
    "\n",
    "\n",
    "nbaSimpleLrDF.show(3)\n",
    "nbaComplexLrDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Create The Model And Train It And Test It "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainAndTest( indf : org.apache.spark.sql.DataFrame, modelPath : String ) : (org.apache.spark.sql.DataFrame, org.apache.spark.ml.classification.LogisticRegressionModel)  = {\n",
    "    val splits = indf.randomSplit(Array(0.60,0.39,0.01), seed = 11L)\n",
    "    val trainingdf = splits(0).cache()\n",
    "    val testdf = splits(1).cache()\n",
    "    val crossvaldf = splits(2).cache()\n",
    "    \n",
    "    println(\"Tranining Samples = \" + trainingdf.count())\n",
    "    println(\"Test      Samples = \" + testdf.count())\n",
    "    println(\"Cross Val Samples = \" + crossvaldf.count())\n",
    "    \n",
    "    // Setup some of the configurations for the Logistic regression model ..\n",
    "    // Here we could try a pipeline with params to select the 'best setting' but in the interest of time will go with this\n",
    "    val lr = new LogisticRegression()\n",
    "      .setMaxIter(50)\n",
    "    .setRegParam(0.10)\n",
    "    .setElasticNetParam(0.0)\n",
    "\n",
    "    // Fit the model\n",
    "    val lrModel   = lr.fit(trainingdf)\n",
    "\n",
    "    println(\"Reg Parameter:    =\" + lrModel.getRegParam)\n",
    "    println(\"lrModel.intercept = \" + lrModel.intercept)\n",
    "    println(\"lrModel.weights   = \" + lrModel.weights)\n",
    "\n",
    "    // Save the model for later use ....\n",
    "    // Argh ! -> in 1.6.1 api, but not 1.5.2 :(  \n",
    "    // lrModel.save(\"modelPath\" )\n",
    "    \n",
    "    ////  Create a logistic regression summary object ////\n",
    "    // val lrSummary = lrModel.summary\n",
    "    // println(\"lrSummary.objectiveHistory = \" + lrSummary.objectiveHistory.length)\n",
    "    // println(lrSummary.objectiveHistory.deep.mkString(\"\\n\"))\n",
    "    ////\n",
    "    \n",
    "    // transform is now used in lieu of predict from mllib.  Found this after studying the API for a while\n",
    "    val predictions = lrModel.transform(testdf)\n",
    "\n",
    "    // Select (prediction, true label) and compute test error\n",
    "    val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setPredictionCol(\"prediction\")\n",
    "      .setMetricName(\"precision\")\n",
    "    \n",
    "    val accuracy = evaluator.evaluate(predictions)\n",
    "    println(\"Test Error = \" + (1.0 - accuracy))\n",
    "\n",
    "    // return the \n",
    "    (predictions,lrModel)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test And Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tranining Samples = 11670\n",
      "Test      Samples = 7651\n",
      "Cross Val Samples = 194\n",
      "Reg Parameter:    =0.1\n",
      "lrModel.intercept = -0.34924242031919606\n",
      "lrModel.weights   = [0.08611036164606282,-0.0033414664739177748]\n",
      "Test Error = 0.2155273820415632\n",
      "Tranining Samples = 11670\n",
      "Test      Samples = 7651\n",
      "Cross Val Samples = 194\n",
      "Reg Parameter:    =0.1\n",
      "lrModel.intercept = 3.2121519330684998\n",
      "lrModel.weights   = [0.05739260226071391,-0.05881970056358269,-0.016919516416235663,-0.0014126299426265973,-0.0028658093470096263,0.11223670171397161]\n"
     ]
    }
   ],
   "source": [
    "def time[R](block: => R): R = {\n",
    "  val t0 = System.nanoTime()\n",
    "  val result = block\n",
    "  println(\"Elapsed time: \" + (System.nanoTime - t0) + \"ns\")\n",
    "  result\n",
    " }\n",
    " \n",
    "val (simplePredictionDF, simpleModel)   = trainAndTest(nbaSimpleLrDF, \"/data/resources/nbaSimpleModel\")\n",
    "val (complexPredictionDF, complexModel) = trainAndTest(nbaComplexLrDF, \"/data/resources/nbaComplexModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexPredictionDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper To Unpack MLLIB Label and Features Vector Into A Standard Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// The whole point to doing this is so that I can visualize the features and the outcomes.  Zeppelin cannot render the Vector type well at all, so converting the data back to a flattened style of layout\n",
    "// Expect a column name to be passed that is the features vector, all other columns should come back as-is\n",
    "// I could extend the MLLIB DF to do this !! and  do it\n",
    "\n",
    "\n",
    "\n",
    " // Create a Row from values.\n",
    "// Row(value1, value2, value3, ...)\n",
    " // Create a Row from a Seq of values.\n",
    "// Row.fromSeq(Seq(value1, value2, ...))\n",
    "\n",
    "// Goal of this function to return a new flattened data frame with the Vector datatype replaced with multiple columns.  This\n",
    "// is done so that I can use zeppelin to graph my data since it doesnt really handle vectors well for graphing.\n",
    "// not super flexible as of now, but someday might make it better ...\n",
    "\n",
    "def convertVectorDF (indf : org.apache.spark.sql.DataFrame, labelIndex : Int, featureIndex : Int, probIndex : Int, predictionIndex : Int) = {\n",
    "\n",
    "    //indf.select(colname).show(5)\n",
    "    val myCols = indf.columns\n",
    "    \n",
    "    // debug stuff\n",
    "    println(\"featureIndex value = \" + myCols(featureIndex))\n",
    "    println(\"probIndex value = \" + myCols(probIndex))\n",
    "    println(\"predictionIndex value = \" + myCols(predictionIndex))\n",
    "    \n",
    "    if(myCols(labelIndex) != \"label\") {\n",
    "        println(\"labelIndex value = \" + myCols(labelIndex))\n",
    "        println(\"Error\")\n",
    "    }\n",
    "    \n",
    "    \n",
    "    println(\" myCols.length = \" + myCols.length)\n",
    "    //println(\"Vector Size = \" + indf[0].features)\n",
    "    indf.show(3)\n",
    "    \n",
    "    // build an RDD of sql Rows here, then use toDF to convert back to a Dataframe..\n",
    "    val tmprdd = indf.map { line => \n",
    "        var reind = 0\n",
    "        var newRow : Seq[Double]= Seq(line(labelIndex).asInstanceOf[Double])\n",
    "        var labeledPointVector = line(featureIndex).asInstanceOf[org.apache.spark.mllib.linalg.Vector]\n",
    "        for(i <- 0 to labeledPointVector.size -1 ) {\n",
    "            newRow = newRow :+ labeledPointVector(i).asInstanceOf[Double]\n",
    "        }\n",
    "        var probArray = line(probIndex).asInstanceOf[org.apache.spark.mllib.linalg.Vector]\n",
    "        newRow = newRow :+ probArray(0).asInstanceOf[Double]\n",
    "        newRow = newRow :+ line(predictionIndex).asInstanceOf[Double]\n",
    "        //  https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/sql/Row.html see example where \n",
    "        //  Row.fromSeq is used to make a Row\n",
    "        Row.fromSeq(newRow)\n",
    "    }\n",
    "    //http://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema\n",
    "    //val schema =\n",
    "      // StructType(\n",
    "      // schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n",
    "    //tmprdd\n",
    "    //https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/types/StructType.html\n",
    "\n",
    "    // Take the first row. Each row just has a single 'Row' object at position 0\n",
    "    val numCols = tmprdd.take(1)(0).length\n",
    "   //val struct =\n",
    "    //StructType(\n",
    "     //StructField(\"label\", IntegerType, true) ::\n",
    "     //StructField(\"f1\", LongType, false) ::\n",
    "     //StructField(\"c\", BooleanType, false) :: Nil)\n",
    "     //sqlContext.createDataFrame(tmprdd,schema)\n",
    "    var schemaString = myCols(labelIndex);\n",
    "    for(i <-0 to numCols - 4){\n",
    "        schemaString = schemaString + \",f\" +i \n",
    "    }\n",
    "    schemaString = schemaString + \",\" + myCols(probIndex);\n",
    "    schemaString = schemaString + \",\" + myCols(predictionIndex);\n",
    "    \n",
    "    //schemaString = schemaString + tmprdd.take(1)(0).toString\n",
    "    println(schemaString)\n",
    "    val schema  = StructType(schemaString.split(\",\").map(fieldName => StructField(fieldName,DoubleType,true)))\n",
    "    val finalDF = sqlContext.createDataFrame(tmprdd, schema)\n",
    "    finalDF\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lets Publish The Model and Analysis And Vizualize In Jupyter ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureIndex value = features\n",
      "probIndex value = probability\n",
      "predictionIndex value = prediction\n",
      " myCols.length = 5\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|          [0.0,48.0]|[0.50963281106724...|[0.62472039286701...|       0.0|\n",
      "|  0.0|[-2.0,46.4166666667]|[0.67656287910911...|[0.66297113641384...|       0.0|\n",
      "|  0.0|          [0.0,45.3]|[0.50061085158767...|[0.62260287285054...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "label,f0,f1,probability,prediction\n"
     ]
    }
   ],
   "source": [
    "var simpleUnpackedPredictionDF = convertVectorDF(simplePredictionDF,0,1,3,4)\n",
    "simpleUnpackedPredictionDF = simpleUnpackedPredictionDF.withColumn(\"correct\", when($\"label\" === $\"prediction\",1).otherwise(0))\n",
    "\n",
    "//var complexUnpackedPredictionDF = convertVectorDF(complexPredictionDF,0,1,3,4)\n",
    "//complexPredictionDF = complexPredictionDF.withColumn(\"correct\", when($\"label\" === $\"prediction\",1).otherwise(0))\n",
    "\n",
    "simpleUnpackedPredictionDF.write.format(\"json\").save(\"swift://notebooks.spark/test/simple001.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7651"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleUnpackedPredictionDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached version of SystemML.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar https://github.com/dustinvanstee/random-public-files/raw/master/SystemML.jar\n",
    "import org.apache.sysml.api.MLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.sysml.api.MLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10 with Spark 1.6 (Unsupported)",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
