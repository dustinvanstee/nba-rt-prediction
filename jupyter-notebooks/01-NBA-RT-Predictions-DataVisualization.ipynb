{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load in Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e4c2341a7d22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "#import org.apache.commons.io.IOUtils\n",
    "#import java.net.URL\n",
    "#import java.nio.charset.Charset\n",
    "#import com.databricks.spark.csv\n",
    "#import org.apache.spark.sql.SQLContext\n",
    "#import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,DoubleType,DateType,TimestampType};\n",
    "#import scala.util.matching.Regex\n",
    "#import org.apache.spark.mllib.regression.LabeledPoint\n",
    "#import org.apache.spark.mllib.linalg.Vectors\n",
    "#import org.apache.spark.ml.classification.LogisticRegression\n",
    "#import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "#import org.apache.spark.sql._\n",
    "#import org.apache.spark.sql.types.{StructType,StructField,StringType,DoubleType};\n",
    "#import org.apache.spark.sql.DataFrame\n",
    "#import org.apache.spark.sql.Column\n",
    "from pyspark.sql.functions import *\n",
    "#// Need this for my shorthand $ notation\n",
    "#import sqlContext.implicits._  \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load In NBA Score Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+------+------------+------+------------+--------+---------+\n",
      "|  dateOrig|      ts|    teamlonga|scorea|   teamlongb|scoreb|  timestring|timeleft|   gameid|\n",
      "+----------+--------+-------------+------+------------+------+------------+--------+---------+\n",
      "|2016-04-05|15:06:16|      Phoenix|     0|     Atlanta|     0|(8:00 PM ET)|    48.0|400829044|\n",
      "|2016-04-05|15:06:16|      Chicago|     0|     Memphis|     0|(8:00 PM ET)|    48.0|400829045|\n",
      "|2016-04-05|15:06:16|    Cleveland|     0|   Milwaukee|     0|(8:00 PM ET)|    48.0|400829046|\n",
      "|2016-04-05|15:06:16|Oklahoma City|     0|      Denver|     0|(9:00 PM ET)|    48.0|400829047|\n",
      "|2016-04-05|15:06:16|  New Orleans|     0|Philadelphia|     0|(7:00 PM ET)|    48.0|400829041|\n",
      "+----------+--------+-------------+------+------------+------+------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType,DateType\n",
    "\n",
    "cleaned_dir = '/data2/nba-rt-prediction/sparkfiles/cleanedDF'\n",
    "df = spark.read.format('csv')\\\n",
    "                    .option(\"header\", \"false\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .option(\"dateFormat\", \"yyyy-MM-dd\")\\\n",
    "                    .load(cleaned_dir).coalesce(2)\n",
    "    \n",
    "rtscoresAndFinalDF.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+------+------------+------+-------------+--------+---------+----+\n",
      "|  dateOrig|      ts|    teamlonga|scorea|   teamlongb|scoreb|   timestring|timeleft|   gameid|blah|\n",
      "+----------+--------+-------------+------+------------+------+-------------+--------+---------+----+\n",
      "|2016-04-05|15:06:16|      Phoenix|     0|     Atlanta|     0| (8:00 PM ET)|    48.0|400829044|   5|\n",
      "|2016-04-05|15:06:16|      Chicago|     0|     Memphis|     0| (8:00 PM ET)|    48.0|400829045|   5|\n",
      "|2016-04-05|15:06:16|    Cleveland|     0|   Milwaukee|     0| (8:00 PM ET)|    48.0|400829046|   5|\n",
      "|2016-04-05|15:06:16|Oklahoma City|     0|      Denver|     0| (9:00 PM ET)|    48.0|400829047|   5|\n",
      "|2016-04-05|15:06:16|  New Orleans|     0|Philadelphia|     0| (7:00 PM ET)|    48.0|400829041|   5|\n",
      "|2016-04-05|15:06:16|      Detroit|     0|       Miami|     0| (8:00 PM ET)|    48.0|400829042|   5|\n",
      "|2016-04-05|15:06:16|    Charlotte|     0|     Toronto|     0| (7:30 PM ET)|    48.0|400829043|   5|\n",
      "|2016-04-05|15:06:16|  San Antonio|     0|        Utah|     0| (9:00 PM ET)|    48.0|400829048|   5|\n",
      "|2016-04-05|15:06:16|     Portland|     0|  Sacramento|     0|(10:00 PM ET)|    48.0|400829049|   5|\n",
      "|2016-04-05|15:06:16|    LA Lakers|     0| LA Clippers|     0|(10:30 PM ET)|    48.0|400829051|   5|\n",
      "|2016-04-05|15:06:16|    Minnesota|     0|Golden State|     0|(10:30 PM ET)|    48.0|400829050|   5|\n",
      "|2016-04-05|15:07:51|      Phoenix|     0|     Atlanta|     0| (8:00 PM ET)|    48.0|400829044|   5|\n",
      "|2016-04-05|15:07:51|      Chicago|     0|     Memphis|     0| (8:00 PM ET)|    48.0|400829045|   5|\n",
      "|2016-04-05|15:07:51|    Cleveland|     0|   Milwaukee|     0| (8:00 PM ET)|    48.0|400829046|   5|\n",
      "|2016-04-05|15:07:51|Oklahoma City|     0|      Denver|     0| (9:00 PM ET)|    48.0|400829047|   5|\n",
      "|2016-04-05|15:07:51|  New Orleans|     0|Philadelphia|     0| (7:00 PM ET)|    48.0|400829041|   5|\n",
      "|2016-04-05|15:07:51|      Detroit|     0|       Miami|     0| (8:00 PM ET)|    48.0|400829042|   5|\n",
      "|2016-04-05|15:07:51|    Charlotte|     0|     Toronto|     0| (7:30 PM ET)|    48.0|400829043|   5|\n",
      "|2016-04-05|15:07:51|  San Antonio|     0|        Utah|     0| (9:00 PM ET)|    48.0|400829048|   5|\n",
      "|2016-04-05|15:07:51|     Portland|     0|  Sacramento|     0|(10:00 PM ET)|    48.0|400829049|   5|\n",
      "+----------+--------+-------------+------+------------+------+-------------+--------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rtscoresAndFinalDF.withColumn(\"blah\", dayofmonth(rtscoresAndFinalDF[\"dateOrig\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+------+------------+------+----------+--------+---------+\n",
      "|  dateOrig|      ts|  teamlonga|scorea|   teamlongb|scoreb|timestring|timeleft|   gameid|\n",
      "+----------+--------+-----------+------+------------+------+----------+--------+---------+\n",
      "|2016-04-05|21:22:09|New Orleans|    93|Philadelphia|   107|   (FINAL)|     0.0|400829041|\n",
      "|2016-04-05|22:08:42|  Charlotte|    90|     Toronto|    96|   (FINAL)|     0.0|400829043|\n",
      "|2016-04-05|22:25:25|    Chicago|    92|     Memphis|   108|   (FINAL)|     0.0|400829045|\n",
      "|2016-04-05|22:28:58|    Phoenix|    90|     Atlanta|   103|   (FINAL)|     0.0|400829044|\n",
      "|2016-04-05|22:30:29|  Cleveland|   109|   Milwaukee|    80|   (FINAL)|     0.0|400829046|\n",
      "+----------+--------+-----------+------+------------+------+----------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+--------+-----------+------+------------+------+-------------+-------------+---------+\n",
      "|  dateOrig|      ts|  teamlonga|scorea|   teamlongb|scoreb|   timestring|     timeleft|   gameid|\n",
      "+----------+--------+-----------+------+------------+------+-------------+-------------+---------+\n",
      "|2016-04-05|19:23:42|New Orleans|    23|Philadelphia|    12|(4:39 IN 1ST)|        40.65|400829041|\n",
      "|2016-04-05|19:23:57|New Orleans|    23|Philadelphia|    14|(4:05 IN 1ST)|40.0833333333|400829041|\n",
      "|2016-04-05|19:24:13|New Orleans|    23|Philadelphia|    14|(3:41 IN 1ST)|39.6833333333|400829041|\n",
      "|2016-04-05|19:24:28|New Orleans|    23|Philadelphia|    14|(3:32 IN 1ST)|39.5333333333|400829041|\n",
      "|2016-04-05|19:24:43|New Orleans|    23|Philadelphia|    16|(3:24 IN 1ST)|         39.4|400829041|\n",
      "+----------+--------+-----------+------+------------+------+-------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rtscoresAndFinalDF.filter(rtscoresAndFinalDF.timestring.contains(\"FINAL\")).show(5)\n",
    "rtscoresAndFinalDF.filter(rtscoresAndFinalDF.timestring.contains(\"1ST\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDFs For Creating Extra Columns In Real Time Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new team name column.. do simple lookup conversion with a UDF\n",
    "def mapper(teamin) :\n",
    "    return teamMap[teamin]\n",
    "\n",
    "mapperudf = udf(mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Date Logic to adjust for games that finish on the day after .... \n",
    "# This is so that I can join them against the spread which was dated the day prior...\n",
    "# This is due to not having a great key to join my tables ...\n",
    "\n",
    "datecrossregex = re.compile(\"^0[0-3]\") # midnight to 3am\n",
    "def dateadjust(datein, tsin ) : \n",
    "    #dateary = datein.split(\"-\")\n",
    "    tsary   = tsin.split(\":\")\n",
    "    sub_one_day = datetime.timedelta(days=1)\n",
    "    newdate = datein\n",
    "    if datecrossregex.match(tsary[0]) :\n",
    "        #day = \"%02d\".format(int(dateary[2]) -1)\n",
    "        #newdate = dateary(0) + \"-\" + dateary(1) + \"-\" + day   \n",
    "        newdate = datein - sub_one_day\n",
    "    return str(newdate)\n",
    "\n",
    "dateadjustudf = udf(dateadjust)\n",
    "\n",
    "\n",
    "# UDFs to create some extra features ... this one is for an experiemental combination of Time left and Score difference.  \n",
    "# Made this via intuition.  This can be extended to add other custom features\n",
    "import math\n",
    "def scoredivtimeXform(numerator, denominator):\n",
    "    rv = numerator/(math.pow(denominator+1,0.5))\n",
    "    return rv\n",
    "scoredivtimeUdf = udf(scoredivtimeXform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle The Real Time And Final Score Data.  Add Columns To The Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Overtime games from this analysis\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.filter(~col(\"timestring\").like(\"%OT%\"))\n",
    "# Create short 3 character team names \n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teama\", mapperudf(col(\"teamlonga\")))\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teamb\", mapperudf(col(\"teamlongb\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a score differential Column \n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"scorea-scoreb\", col(\"scorea\") - col(\"scoreb\"))\n",
    "\n",
    "# Transform the Date.  This is for games that spanned multiple days and gave me a headache.  \n",
    "# Games adjusted to the day they started on.\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"date\",  dateadjustudf(col(\"dateOrig\"),col(\"ts\")))\n",
    "\n",
    "# Create a Key for me to use to join with my odds data later.  Key = date.teama.teamb\n",
    "rtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"key\", concat(col(\"date\"),lit(\".\"),col(\"teama\"),lit(\".\"),col(\"teamb\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------+------+------------+------+-------------+--------+---------+-----+-----+-------------+----------+------------------+\n",
      "|  dateOrig|      ts|    teamlonga|scorea|   teamlongb|scoreb|   timestring|timeleft|   gameid|teama|teamb|scorea-scoreb|      date|               key|\n",
      "+----------+--------+-------------+------+------------+------+-------------+--------+---------+-----+-----+-------------+----------+------------------+\n",
      "|2016-04-05|15:06:16|      Phoenix|     0|     Atlanta|     0| (8:00 PM ET)|    48.0|400829044|  pho|  atl|            0|2016-04-05|2016-04-05.pho.atl|\n",
      "|2016-04-05|15:06:16|      Chicago|     0|     Memphis|     0| (8:00 PM ET)|    48.0|400829045|  chi|  mem|            0|2016-04-05|2016-04-05.chi.mem|\n",
      "|2016-04-05|15:06:16|    Cleveland|     0|   Milwaukee|     0| (8:00 PM ET)|    48.0|400829046|  cle|  mil|            0|2016-04-05|2016-04-05.cle.mil|\n",
      "|2016-04-05|15:06:16|Oklahoma City|     0|      Denver|     0| (9:00 PM ET)|    48.0|400829047|  okc|  den|            0|2016-04-05|2016-04-05.okc.den|\n",
      "|2016-04-05|15:06:16|  New Orleans|     0|Philadelphia|     0| (7:00 PM ET)|    48.0|400829041|  nor|  phi|            0|2016-04-05|2016-04-05.nor.phi|\n",
      "|2016-04-05|15:06:16|      Detroit|     0|       Miami|     0| (8:00 PM ET)|    48.0|400829042|  det|  mia|            0|2016-04-05|2016-04-05.det.mia|\n",
      "|2016-04-05|15:06:16|    Charlotte|     0|     Toronto|     0| (7:30 PM ET)|    48.0|400829043|  cha|  tor|            0|2016-04-05|2016-04-05.cha.tor|\n",
      "|2016-04-05|15:06:16|  San Antonio|     0|        Utah|     0| (9:00 PM ET)|    48.0|400829048|  san|  uta|            0|2016-04-05|2016-04-05.san.uta|\n",
      "|2016-04-05|15:06:16|     Portland|     0|  Sacramento|     0|(10:00 PM ET)|    48.0|400829049|  por|  sac|            0|2016-04-05|2016-04-05.por.sac|\n",
      "|2016-04-05|15:06:16|    LA Lakers|     0| LA Clippers|     0|(10:30 PM ET)|    48.0|400829051|  lal|  lac|            0|2016-04-05|2016-04-05.lal.lac|\n",
      "|2016-04-05|15:06:16|    Minnesota|     0|Golden State|     0|(10:30 PM ET)|    48.0|400829050|  min|  gst|            0|2016-04-05|2016-04-05.min.gst|\n",
      "|2016-04-05|15:07:51|      Phoenix|     0|     Atlanta|     0| (8:00 PM ET)|    48.0|400829044|  pho|  atl|            0|2016-04-05|2016-04-05.pho.atl|\n",
      "|2016-04-05|15:07:51|      Chicago|     0|     Memphis|     0| (8:00 PM ET)|    48.0|400829045|  chi|  mem|            0|2016-04-05|2016-04-05.chi.mem|\n",
      "|2016-04-05|15:07:51|    Cleveland|     0|   Milwaukee|     0| (8:00 PM ET)|    48.0|400829046|  cle|  mil|            0|2016-04-05|2016-04-05.cle.mil|\n",
      "|2016-04-05|15:07:51|Oklahoma City|     0|      Denver|     0| (9:00 PM ET)|    48.0|400829047|  okc|  den|            0|2016-04-05|2016-04-05.okc.den|\n",
      "|2016-04-05|15:07:51|  New Orleans|     0|Philadelphia|     0| (7:00 PM ET)|    48.0|400829041|  nor|  phi|            0|2016-04-05|2016-04-05.nor.phi|\n",
      "|2016-04-05|15:07:51|      Detroit|     0|       Miami|     0| (8:00 PM ET)|    48.0|400829042|  det|  mia|            0|2016-04-05|2016-04-05.det.mia|\n",
      "|2016-04-05|15:07:51|    Charlotte|     0|     Toronto|     0| (7:30 PM ET)|    48.0|400829043|  cha|  tor|            0|2016-04-05|2016-04-05.cha.tor|\n",
      "|2016-04-05|15:07:51|  San Antonio|     0|        Utah|     0| (9:00 PM ET)|    48.0|400829048|  san|  uta|            0|2016-04-05|2016-04-05.san.uta|\n",
      "|2016-04-05|15:07:51|     Portland|     0|  Sacramento|     0|(10:00 PM ET)|    48.0|400829049|  por|  sac|            0|2016-04-05|2016-04-05.por.sac|\n",
      "+----------+--------+-------------+------+------------+------+-------------+--------+---------+-----+-----+-------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rtscoresAndFinalDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate The Real Time And Final Data From One Common Dataframe To Two Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Currently based on the way the data was sampled, both real time scores and final scores are written as seperate records to the same file.  I need to pull these apart, and then join the dataframes so that I have a real time score and features, and know if the game was won or lost ....\n",
    "\n",
    "# Create Final Score DF\n",
    "# Note a shortcut for repeating the dataframe within the filter is to use a $   df.filter(df(\"foo\").contains ... is equiv to df.filter($\"foo\".contains)\n",
    "\n",
    "finalscoresDF = rtscoresAndFinalDF.filter(col(\"timestring\").like(\"%FINAL%\"))\n",
    "\n",
    "# Rename some columns so that join later doesnt have name overlaps\n",
    "finalscoresDF = finalscoresDF.withColumnRenamed(\"scorea\", \"fscorea\")\n",
    "finalscoresDF = finalscoresDF.withColumnRenamed(\"scoreb\", \"fscoreb\")\n",
    "\n",
    "# Create final score difference\n",
    "finalscoresDF = finalscoresDF.withColumn(\"fscorea-fscoreb\", col(\"fscorea\") - col(\"fscoreb\"))\n",
    "\n",
    "# Add a Win/loss column Win = 1, Loss = 0\n",
    "finalscoresDF = finalscoresDF.withColumn(\"win-loss-enc\", (when(col(\"fscorea-fscoreb\") > 0.0, 1.0).otherwise(0)))\n",
    "\n",
    "#(when(df['age'] == 2, 3).otherwise(4)\n",
    "\n",
    "# Remove Halftime records as this particular case isn't handled well... (for now)\n",
    "rtscoresDF = rtscoresAndFinalDF.filter(~col(\"timestring\").like(\"%FINAL%\")).filter(~col(\"timestring\").like(\"HALFTIME\"))\n",
    "\n",
    "# Create final score difference\n",
    "rtscoresDF = rtscoresDF.withColumn(\"scorea-scoreb\", col(\"scorea\") - col(\"scoreb\"))\n",
    "\n",
    "# Create a unique feature based on my custom UDF.  Idea here is that I have intuition that timeleft and score difference are a strong predictor when combined\n",
    "rtscoresDF = rtscoresDF.withColumn(\"score-div-time\", scoredivtimeUdf(col(\"scorea\") - col(\"scoreb\"), col(\"timeleft\")*2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Take A Look Of What We Have For The Two Dataframes We Just Wrangled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final scores data frame\n",
      "+----------+--------+-----------+-------+------------+-------+----------+--------+---------+-----+-----+-------------+----------+------------------+---------------+------------+\n",
      "|  dateOrig|      ts|  teamlonga|fscorea|   teamlongb|fscoreb|timestring|timeleft|   gameid|teama|teamb|scorea-scoreb|      date|               key|fscorea-fscoreb|win-loss-enc|\n",
      "+----------+--------+-----------+-------+------------+-------+----------+--------+---------+-----+-----+-------------+----------+------------------+---------------+------------+\n",
      "|2016-04-05|21:22:09|New Orleans|     93|Philadelphia|    107|   (FINAL)|     0.0|400829041|  nor|  phi|          -14|2016-04-05|2016-04-05.nor.phi|            -14|         0.0|\n",
      "|2016-04-05|22:08:42|  Charlotte|     90|     Toronto|     96|   (FINAL)|     0.0|400829043|  cha|  tor|           -6|2016-04-05|2016-04-05.cha.tor|             -6|         0.0|\n",
      "|2016-04-05|22:25:25|    Chicago|     92|     Memphis|    108|   (FINAL)|     0.0|400829045|  chi|  mem|          -16|2016-04-05|2016-04-05.chi.mem|            -16|         0.0|\n",
      "|2016-04-05|22:28:58|    Phoenix|     90|     Atlanta|    103|   (FINAL)|     0.0|400829044|  pho|  atl|          -13|2016-04-05|2016-04-05.pho.atl|            -13|         0.0|\n",
      "|2016-04-05|22:30:29|  Cleveland|    109|   Milwaukee|     80|   (FINAL)|     0.0|400829046|  cle|  mil|           29|2016-04-05|2016-04-05.cle.mil|             29|         1.0|\n",
      "+----------+--------+-----------+-------+------------+-------+----------+--------+---------+-----+-----+-------------+----------+------------------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "real time scores data frame\n",
      "+----------+--------+-------------+------+------------+------+------------+--------+---------+-----+-----+-------------+----------+------------------+--------------+\n",
      "|  dateOrig|      ts|    teamlonga|scorea|   teamlongb|scoreb|  timestring|timeleft|   gameid|teama|teamb|scorea-scoreb|      date|               key|score-div-time|\n",
      "+----------+--------+-------------+------+------------+------+------------+--------+---------+-----+-----+-------------+----------+------------------+--------------+\n",
      "|2016-04-05|15:06:16|      Phoenix|     0|     Atlanta|     0|(8:00 PM ET)|    48.0|400829044|  pho|  atl|            0|2016-04-05|2016-04-05.pho.atl|           0.0|\n",
      "|2016-04-05|15:06:16|      Chicago|     0|     Memphis|     0|(8:00 PM ET)|    48.0|400829045|  chi|  mem|            0|2016-04-05|2016-04-05.chi.mem|           0.0|\n",
      "|2016-04-05|15:06:16|    Cleveland|     0|   Milwaukee|     0|(8:00 PM ET)|    48.0|400829046|  cle|  mil|            0|2016-04-05|2016-04-05.cle.mil|           0.0|\n",
      "|2016-04-05|15:06:16|Oklahoma City|     0|      Denver|     0|(9:00 PM ET)|    48.0|400829047|  okc|  den|            0|2016-04-05|2016-04-05.okc.den|           0.0|\n",
      "|2016-04-05|15:06:16|  New Orleans|     0|Philadelphia|     0|(7:00 PM ET)|    48.0|400829041|  nor|  phi|            0|2016-04-05|2016-04-05.nor.phi|           0.0|\n",
      "+----------+--------+-------------+------+------------+------+------------+--------+---------+-----+-----+-------------+----------+------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "##########################################\n",
      "Total Data Points in rtscoresDF = 14405\n",
      "Total Data Points in rtscoresDF uniq = 14405\n",
      "Total Data Points in finalscoresDF = 116\n",
      "##########################################\n"
     ]
    }
   ],
   "source": [
    "# Some Printouts .....\n",
    "print(\"final scores data frame\")\n",
    "finalscoresDF.show(5)\n",
    "print(\"real time scores data frame\")\n",
    "rtscoresDF.show(5)\n",
    "finalscoresDF.printSchema\n",
    "\n",
    "\n",
    "print \"##########################################\"\n",
    "print \"Total Data Points in rtscoresDF = {0}\".format(rtscoresDF.count())\n",
    "print \"Total Data Points in rtscoresDF uniq = {0}\".format(rtscoresDF.sort(\"key\").distinct().count())\n",
    "print \"Total Data Points in finalscoresDF = {0}\".format(finalscoresDF.count())\n",
    "print \"##########################################\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Interpret the Odds data\n",
    "````How to interpret the odds data ...\n",
    "Example Golden State -12.5 O (207.0) -125.0 | Detroit 12.5 U (207.0) 145.0\n",
    "Here Golden State the away team is a 12.5 pt favorite to win.  The over under is in parentheses (207) and is the 50/50 line between teams sum of scores\n",
    "being above/below that line.  \n",
    "Finally the -125 / +145 numbers are whats known at the moneyline odds. \n",
    "    A negative number means you need to bet 125$ to get a 100$ payout\n",
    "    A positive number means you need to bet 100$ to get a 145$ payout\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load In Odds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, the data is very raw, and needs to be pre-processed .  I will start by loading it as an RDD and perform a lot of transformations.  Once I have it properly parsed, I will convert to a dataframe.\n",
    "# This is not beautiful, but gets the job done\n",
    "# Data format .....\n",
    "#       <title>New Orleans 2.5 O (207.0) 125.0 | Phila. -2.5 U (207.0) -145.0 (Apr 05, 2016 07:10 PM)</title>\n",
    "#       <title>Detroit 4.0 O (202.0) 160.0 | Miami -4.0 U (202.0) -190.0 (Apr 05, 2016 08:05 PM)</title>\n",
    "oddsfile = \"/data2/nba-rt-prediction/nbaodds_042516.xml\"\n",
    "\n",
    "# Reading the data in as an RDD first.  There isn't a dataframe parser for this XML I have, so I will write a custom parser ....\n",
    "oddsrdd = spark.read.text(oddsfile).rdd\n",
    "# just grabbing the text within the < ... > tags.  I can do this, because the format is super simple and not nested\n",
    "# the subscript [0] is due to the fact that spark.read.text read in the oddsfile of Type Row.  Need to index\n",
    "# into it to get the string\n",
    "gameStringRdd = oddsrdd.map(lambda x : x[0][x[0].find('>')+1:x[0].rfind('<')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#oddsrdd.map(lambda x : x[0].find('>')).take(2)\n",
    "#gameStringRdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is where I do the heavy lifting of parsing my XML .. and then finally convert my RDD to a dataframe .....\n",
    "# just lots of string parsing and data type conversions\n",
    "def parseOdds(line_in) : \n",
    "    away_str = line_in[0:line_in.find('|')]  \n",
    "    home_and_date_str = line_in[line_in.find('|')+2:-1]\n",
    "    home_str = home_and_date_str[0:home_and_date_str.rfind('(')]\n",
    "    date_str = home_and_date_str[home_and_date_str.rfind('(')+1:len(home_and_date_str)]\n",
    "    date_str = date_str.replace(',', '')\n",
    "    \n",
    "    # parse away string\n",
    "    overunder = away_str[away_str.find('(')+1:away_str.find(')')]\n",
    "    teamaml = away_str[away_str.find(')')+2:len(away_str)-1]\n",
    "    away_str_2 = away_str[0:away_str.find('(')-3]\n",
    "    teamaspread = away_str_2[away_str_2.rfind(' ')+1:len(away_str_2)]\n",
    "    teamlonga = away_str_2[0:away_str_2.rfind(' ')]\n",
    "    teama = teamMap[teamlonga]\n",
    "    \n",
    "     # parse home string\n",
    "    overunder = home_str[home_str.find('(')+1:home_str.find(')')]\n",
    "    teamhml = home_str[home_str.find(')')+2:len(home_str)-1]\n",
    "    home_str_2 = home_str[0:home_str.find('(')-3]\n",
    "    teamhspread = home_str_2[home_str_2.rfind(' ')+1:len(home_str_2)]\n",
    "    teamlongh = home_str_2[0:home_str_2.rfind(' ')]\n",
    "    teamh = teamMap[teamlongh]\n",
    "   \n",
    "    # parse date string\n",
    "    dateInfo = date_str.split(' ')\n",
    "    dateStr = dateInfo[2] + \"-\" + monthMap[dateInfo[0]] + \"-\" + dateInfo[1]\n",
    "    # This will become my join key for the other data sets\n",
    "    key = dateStr +\".\" + teama + \".\" + teamh\n",
    "    return (key,teamlonga,teama,teamaspread,overunder,teamaml,teamlongh,teamh,teamhml,dateStr)\n",
    "\n",
    "#def parseOdds(line_in) : \n",
    "#    away_str = line_in[0:line_in.find('|')]  \n",
    "#    return (away_str,away_str)\n",
    "\n",
    "\n",
    "oddsDF = gameStringRdd.map(lambda x : parseOdds(x))\\\n",
    "        .toDF([\"key\",\"teamlonga\",\"teama\",\"teamaspread\",\"overunder\",\"teamaml\",\"teamlongh\",\"teamh\",\"teamhml\",\"dateStr\"]).distinct()\n",
    "\n",
    "# OddsDF has some dups due to the fact that I have multiple readings ...\n",
    "oddsDF.registerTempTable(\"odds_table\")\n",
    "\n",
    "oddsDF = spark.sql(\"SELECT key, FIRST(teamlonga) as teamlonga, FIRST(teama) as teama,\\\n",
    "  AVG(teamaspread) as teamaspread, AVG(overunder) as overunder, AVG(teamaml) as teamaml,\\\n",
    "  FIRST(teamlongh) as teamlongh,FIRST(teamh) as teamh,AVG(teamhml) as teamhml, FIRST(dateStr) as dateStr FROM odds_table GROUP BY key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "+------------------+------------+-----+-----------+------------------+-----------------+------------+-----+------------------+----------+\n",
      "|               key|   teamlonga|teama|teamaspread|         overunder|          teamaml|   teamlongh|teamh|           teamhml|   dateStr|\n",
      "+------------------+------------+-----+-----------+------------------+-----------------+------------+-----+------------------+----------+\n",
      "|2016-04-24.atl.bos|     Atlanta|  atl|        1.5|             203.5|              0.0|      Boston|  bos|            -120.0|2016-04-24|\n",
      "|2016-04-19.mem.san|     Memphis|  mem|      18.25|             187.0|           -110.0| San Antonio|  san|            -110.0|2016-04-19|\n",
      "|2016-04-26.ind.tor|     Indiana|  ind|        7.0|             192.0|            260.0|     Toronto|  tor|            -320.0|2016-04-26|\n",
      "|2016-04-13.mia.bos|       Miami|  mia|        5.0|             206.5|            170.0|      Boston|  bos|            -200.0|2016-04-13|\n",
      "|2016-04-13.sac.hou|  Sacramento|  sac|       15.0|             222.0|           -110.0|     Houston|  hou|            -110.0|2016-04-13|\n",
      "|2016-04-13.san.dal| San Antonio|  san|        4.5|             187.5|            165.0|      Dallas|  dal|            -195.0|2016-04-13|\n",
      "|2016-04-08.lac.uta| LA Clippers|  lac|       12.0|             182.5|           -110.0|        Utah|  uta|            -110.0|2016-04-08|\n",
      "|2016-04-08.wsh.det|  Washington|  wsh|        6.0|             206.5|            195.0|     Detroit|  det|            -235.0|2016-04-08|\n",
      "|2016-04-16.bos.atl|      Boston|  bos|       5.25|             204.0|            175.0|     Atlanta|  atl|            -210.0|2016-04-16|\n",
      "|2016-04-09.gst.mem|Golden State|  gst|      -13.0|             210.0|           -110.0|     Memphis|  mem|            -110.0|2016-04-09|\n",
      "|2016-04-13.uta.lal|        Utah|  uta|       -5.0|             193.5|           -210.0|   LA Lakers|  lal|             175.0|2016-04-13|\n",
      "|2016-04-13.mem.gst|     Memphis|  mem|       19.0|             213.0|           -110.0|Golden State|  gst|            -110.0|2016-04-13|\n",
      "|2016-04-11.wsh.bkn|  Washington|  wsh|       -6.0|             211.5|           -240.0|    Brooklyn|  bkn|             200.0|2016-04-11|\n",
      "|2016-04-11.hou.min|     Houston|  hou|       -3.5|             215.5|           -155.0|   Minnesota|  min|             135.0|2016-04-11|\n",
      "|2016-04-13.orl.cha|     Orlando|  orl|        9.0|             209.5|            380.0|   Charlotte|  cha|            -480.0|2016-04-13|\n",
      "|2016-04-05.cle.mil|   Cleveland|  cle|       -7.5|             203.0|           -340.0|   Milwaukee|  mil|             280.0|2016-04-05|\n",
      "|2016-04-11.sac.pho|  Sacramento|  sac|        5.5|             219.0|            200.0|     Phoenix|  pho|            -240.0|2016-04-11|\n",
      "|2016-04-20.det.cle|     Detroit|  det|       10.0|200.83333333333334|446.6666666666667|   Cleveland|  cle|-593.3333333333334|2016-04-20|\n",
      "|2016-04-08.mem.dal|     Memphis|  mem|        6.0|             193.0|            210.0|      Dallas|  dal|            -250.0|2016-04-08|\n",
      "|2016-04-09.min.por|   Minnesota|  min|        8.5|             213.5|            330.0|    Portland|  por|            -420.0|2016-04-09|\n",
      "+------------------+------------+-----+-----------+------------------+-----------------+------------+-----+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[key: string, teamlonga: string, teama: string, teamaspread: double, overunder: double, teamaml: double, teamlongh: string, teamh: string, teamhml: double, dateStr: string]>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#oddsrdd.take(2)\n",
    "#gameStringRdd.take(2)\n",
    "print oddsDF.count()\n",
    "oddsDF.show()\n",
    "oddsDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Some Of The Odds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+-----+-----------+---------+-------+-----------+-----+-------+----------+\n",
      "|               key| teamlonga|teama|teamaspread|overunder|teamaml|  teamlongh|teamh|teamhml|   dateStr|\n",
      "+------------------+----------+-----+-----------+---------+-------+-----------+-----+-------+----------+\n",
      "|2016-04-24.atl.bos|   Atlanta|  atl|        1.5|    203.5|    0.0|     Boston|  bos| -120.0|2016-04-24|\n",
      "|2016-04-19.mem.san|   Memphis|  mem|      18.25|    187.0| -110.0|San Antonio|  san| -110.0|2016-04-19|\n",
      "|2016-04-26.ind.tor|   Indiana|  ind|        7.0|    192.0|  260.0|    Toronto|  tor| -320.0|2016-04-26|\n",
      "|2016-04-13.mia.bos|     Miami|  mia|        5.0|    206.5|  170.0|     Boston|  bos| -200.0|2016-04-13|\n",
      "|2016-04-13.sac.hou|Sacramento|  sac|       15.0|    222.0| -110.0|    Houston|  hou| -110.0|2016-04-13|\n",
      "+------------------+----------+-----+-----------+---------+-------+-----------+-----+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total Home Teams      = 30\n",
      "Total Away Teams      = 30\n",
      "Total Games Collected = 111 \n"
     ]
    }
   ],
   "source": [
    "oddsDF.show(5)\n",
    "print \"Total Home Teams      = {0}\".format(oddsDF.select(\"teamh\").distinct().count())\n",
    "print \"Total Away Teams      = {0}\".format(oddsDF.select(\"teama\").distinct().count())\n",
    "print \"Total Games Collected = {0} \".format(oddsDF.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join The Odds And Final Score Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is where we join the Odds/Realtime scores/ Final Scores into one wholistic data set as input for Logistic Machine Learning\n",
    "\n",
    "# Create a smaller Final Score Dataframe.  Just keep the key, final score a and b, the win/loss indicator\n",
    "finalslicedscoresDF1 = finalscoresDF.select([\"key\",\"fscorea\",\"fscoreb\",\"win-loss-enc\"]).distinct()\n",
    "# First Join the 2 smallest data frames ... odd and final.\n",
    "gameDF = oddsDF.join(finalslicedscoresDF1, oddsDF[\"key\"] == finalslicedscoresDF1[\"key\"], \"inner\").drop(oddsDF[\"key\"])\n",
    "# Drop these redundant columns prior to joining with Realtime score dataframe\n",
    "gameDF = gameDF.drop(\"teamlonga\")\n",
    "gameDF = gameDF.drop(\"teamlongb\")\n",
    "gameDF = gameDF.drop(\"teama\")\n",
    "gameDF = gameDF.drop(\"teamb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total finalscoresDF = 116\n",
      "Total oddsDF = 111\n",
      "Total Games after joining odds and score data = 101\n",
      "+----------+--------+------------+-------+---------+-------+----------+--------+---------+-----+-----+-------------+----------+------------------+---------------+------------+\n",
      "|  dateOrig|      ts|   teamlonga|fscorea|teamlongb|fscoreb|timestring|timeleft|   gameid|teama|teamb|scorea-scoreb|      date|               key|fscorea-fscoreb|win-loss-enc|\n",
      "+----------+--------+------------+-------+---------+-------+----------+--------+---------+-----+-----+-------------+----------+------------------+---------------+------------+\n",
      "|2016-04-24|18:29:52|Golden State|    121|  Houston|     94|   (FINAL)|     0.0|400874348|  gst|  hou|           27|2016-04-24|2016-04-24.gst.hou|             27|         1.0|\n",
      "+----------+--------+------------+-------+---------+-------+----------+--------+---------+-----+-----+-------------+----------+------------------+---------------+------------+\n",
      "\n",
      "+------------------+------------+-----+-----------+---------+-------+---------+-----+-------+----------+\n",
      "|               key|   teamlonga|teama|teamaspread|overunder|teamaml|teamlongh|teamh|teamhml|   dateStr|\n",
      "+------------------+------------+-----+-----------+---------+-------+---------+-----+-------+----------+\n",
      "|2016-04-24.gst.hou|Golden State|  gst|       -8.5|    217.5| -425.0|  Houston|  hou|  340.0|2016-04-24|\n",
      "+------------------+------------+-----+-----------+---------+-------+---------+-----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Out the Game Dataframe ... notice we have the odds data merged with the win loss data ....\n",
    "#print(\"gameDF\")\n",
    "#gameDF.sort([\"key\"]).show(152)\n",
    "print \"Total finalscoresDF = {0}\".format(finalscoresDF.count())\n",
    "print \"Total oddsDF = {0}\".format(oddsDF.count())\n",
    "print \"Total Games after joining odds and score data = {0}\".format(gameDF.count())\n",
    "\n",
    "#gameDF.select(\"key\").sort([\"key\"]).show(144)\n",
    "\n",
    "finalscoresDF.filter(col(\"key\") == \"2016-04-24.gst.hou\").show()\n",
    "oddsDF.filter(col(\"key\") == \"2016-04-24.gst.hou\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join The Game Dataframe With The Real Time Score Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrDF : Logistic Regression Data Frame\n",
      "+----------+--------+---------+------+-----------+------+-------------+--------------+---------+-----+-----+-------------+----------+-------------------+-----------+---------+-------+-----------+-----+-------+----------+------------------+-------+-------+------------+\n",
      "|  dateOrig|      ts|teamlonga|scorea|  teamlongb|scoreb|   timestring|      timeleft|   gameid|teama|teamb|scorea-scoreb|      date|     score-div-time|teamaspread|overunder|teamaml|  teamlongh|teamh|teamhml|   dateStr|               key|fscorea|fscoreb|win-loss-enc|\n",
      "+----------+--------+---------+------+-----------+------+-------------+--------------+---------+-----+-----+-------------+----------+-------------------+-----------+---------+-------+-----------+-----+-------+----------+------------------+-------+-------+------------+\n",
      "|2016-04-20|00:01:44|  Memphis|    68|San Antonio|    94|(0:00 IN 4TH)|           0.0|400874376|  mem|  san|          -26|2016-04-19|              -26.0|      18.25|    187.0| -110.0|San Antonio|  san| -110.0|2016-04-19|2016-04-19.mem.san|     68|     94|         0.0|\n",
      "|2016-04-20|00:00:13|  Memphis|    66|San Antonio|    94|(0:42 IN 4TH)|           0.7|400874376|  mem|  san|          -28|2016-04-19| -18.07392228230128|      18.25|    187.0| -110.0|San Antonio|  san| -110.0|2016-04-19|2016-04-19.mem.san|     68|     94|         0.0|\n",
      "|2016-04-19|23:59:43|  Memphis|    66|San Antonio|    94|(0:56 IN 4TH)|0.933333333333|400874376|  mem|  san|          -28|2016-04-19|-16.537483850123238|      18.25|    187.0| -110.0|San Antonio|  san| -110.0|2016-04-19|2016-04-19.mem.san|     68|     94|         0.0|\n",
      "+----------+--------+---------+------+-----------+------+-------------+--------------+---------+-----+-----+-------------+----------+-------------------+-----------+---------+-------+-----------+-----+-------+----------+------------------+-------+-------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Total Data Points in rtscoresDF = 14405\n",
      "Total Data Points in gameDF = 101\n",
      "Total Data Points in joined lrDF = 13544\n"
     ]
    }
   ],
   "source": [
    "lrDF = rtscoresDF.join(gameDF, rtscoresDF[\"key\"] == gameDF[\"key\"], \"inner\").drop(gameDF[\"key\"])\n",
    "print(\"lrDF : Logistic Regression Data Frame\")\n",
    "lrDF.show(3)\n",
    "print \"Total Data Points in rtscoresDF = {0}\".format(rtscoresDF.count())\n",
    "print \"Total Data Points in gameDF = {0}\".format(gameDF.count())\n",
    "print \"Total Data Points in joined lrDF = {0}\".format(lrDF.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lets Look At Some Stats From 'Unpacked' Logistic Regression Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " lrDF.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###'Pack' Logistic Dataframe Into Required Format For Logistic Regression [R] - Creating A Simple/Complex Dataframe For Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic regression requires that\n",
    "# The best way I found to modify data in a custom way is with map, however it returns an RDD, so you will need to run toDF at the end\n",
    "# Logistic DF requires a DF of type  => [label: double, features: vector]\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "# These is a helper function that converts an 'Any(Int)' type to a Double or Any(Double) to a Double\n",
    "val ai2d : (Any => Double) = (in:Any) => in.asInstanceOf[java.lang.Integer].doubleValue\n",
    "val ad2d : (Any => Double) = (in:Any) => in.asInstanceOf[java.lang.Double]\n",
    "\n",
    "var nbaSimpleLrDF = lrDF.map {\n",
    "    dfrow => {\n",
    "    # get the indexes ... prob should move this out of the looop!\n",
    "      val tgt = ad2d( dfrow(dfrow.fieldIndex(\"win-loss-enc\")))\n",
    "      val f1  = ai2d( dfrow(dfrow.fieldIndex(\"scorea-scoreb\")))\n",
    "      val f2 =  ad2d( dfrow(dfrow.fieldIndex(\"timeleft\")))\n",
    "      LabeledPoint(tgt,Vectors.dense(f1,f2))\n",
    "    }\n",
    "}.toDF(\"label\",\"features\")\n",
    "\n",
    "var nbaComplexLrDF = lrDF.map {\n",
    "    dfrow => {\n",
    "    # get the indexes ... prob should move this out of the looop!\n",
    "      val tgt = ad2d( dfrow(dfrow.fieldIndex(\"win-loss-enc\")))\n",
    "      val f1  = ai2d( dfrow(dfrow.fieldIndex(\"scorea-scoreb\")))\n",
    "      val f2  = ad2d( dfrow(dfrow.fieldIndex(\"teamaspread\")))\n",
    "      val f3  = ad2d( dfrow(dfrow.fieldIndex(\"overunder\")))\n",
    "      val f4  = ad2d( dfrow(dfrow.fieldIndex(\"teamaml\")))\n",
    "      val f5 =  ad2d( dfrow(dfrow.fieldIndex(\"timeleft\")))\n",
    "      // Add a new feature that weight point differential more as time left gets smaller...\n",
    "      // I played around with excel and this looked ok ....\n",
    "      val f6  = ad2d( dfrow(dfrow.fieldIndex(\"score-div-time\")))\n",
    "      LabeledPoint(tgt,Vectors.dense(f1,f2,f3,f4,f5,f6))\n",
    "    }\n",
    "}.toDF(\"label\",\"features\")\n",
    "\n",
    "\n",
    "nbaSimpleLrDF.show(3)\n",
    "nbaComplexLrDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Create The Model And Train It And Test It "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainAndTest( indf : org.apache.spark.sql.DataFrame, modelPath : String ) : (org.apache.spark.sql.DataFrame, org.apache.spark.ml.classification.LogisticRegressionModel)  = {\n",
    "    val splits = indf.randomSplit(Array(0.60,0.39,0.01), seed = 11L)\n",
    "    val trainingdf = splits(0).cache()\n",
    "    val testdf = splits(1).cache()\n",
    "    val crossvaldf = splits(2).cache()\n",
    "    \n",
    "    println(\"Tranining Samples = \" + trainingdf.count())\n",
    "    println(\"Test      Samples = \" + testdf.count())\n",
    "    println(\"Cross Val Samples = \" + crossvaldf.count())\n",
    "    \n",
    "    # Setup some of the configurations for the Logistic regression model ..\n",
    "    # Here we could try a pipeline with params to select the 'best setting' but in the interest of time will go with this\n",
    "    val lr = new LogisticRegression()\n",
    "      .setMaxIter(50)\n",
    "    .setRegParam(0.10)\n",
    "    .setElasticNetParam(0.0)\n",
    "\n",
    "    # Fit the model\n",
    "    val lrModel   = lr.fit(trainingdf)\n",
    "\n",
    "    println(\"Reg Parameter:    =\" + lrModel.getRegParam)\n",
    "    println(\"lrModel.intercept = \" + lrModel.intercept)\n",
    "    println(\"lrModel.weights   = \" + lrModel.weights)\n",
    "\n",
    "    # Save the model for later use ....\n",
    "    # Argh ! -> in 1.6.1 api, but not 1.5.2 :(  \n",
    "    # lrModel.save(\"modelPath\" )\n",
    "    \n",
    "    ####  Create a logistic regression summary object ////\n",
    "    # val lrSummary = lrModel.summary\n",
    "    # println(\"lrSummary.objectiveHistory = \" + lrSummary.objectiveHistory.length)\n",
    "    # println(lrSummary.objectiveHistory.deep.mkString(\"\\n\"))\n",
    "    ####\n",
    "    \n",
    "    # transform is now used in lieu of predict from mllib.  Found this after studying the API for a while\n",
    "    val predictions = lrModel.transform(testdf)\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"label\")\n",
    "      .setPredictionCol(\"prediction\")\n",
    "      .setMetricName(\"precision\")\n",
    "    \n",
    "    val accuracy = evaluator.evaluate(predictions)\n",
    "    println(\"Test Error = \" + (1.0 - accuracy))\n",
    "\n",
    "    # return the \n",
    "    (predictions,lrModel)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test And Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time[R](block: => R): R = {\n",
    "  val t0 = System.nanoTime()\n",
    "  val result = block\n",
    "  println(\"Elapsed time: \" + (System.nanoTime - t0) + \"ns\")\n",
    "  result\n",
    " }\n",
    " \n",
    "val (simplePredictionDF, simpleModel)   = trainAndTest(nbaSimpleLrDF, \"/data/resources/nbaSimpleModel\")\n",
    "val (complexPredictionDF, complexModel) = trainAndTest(nbaComplexLrDF, \"/data/resources/nbaComplexModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complexPredictionDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper To Unpack MLLIB Label and Features Vector Into A Standard Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// The whole point to doing this is so that I can visualize the features and the outcomes.  Zeppelin cannot render the Vector type well at all, so converting the data back to a flattened style of layout\n",
    "// Expect a column name to be passed that is the features vector, all other columns should come back as-is\n",
    "// I could extend the MLLIB DF to do this !! and  do it\n",
    "\n",
    "\n",
    "\n",
    " // Create a Row from values.\n",
    "// Row(value1, value2, value3, ...)\n",
    " // Create a Row from a Seq of values.\n",
    "// Row.fromSeq(Seq(value1, value2, ...))\n",
    "\n",
    "// Goal of this function to return a new flattened data frame with the Vector datatype replaced with multiple columns.  This\n",
    "// is done so that I can use zeppelin to graph my data since it doesnt really handle vectors well for graphing.\n",
    "// not super flexible as of now, but someday might make it better ...\n",
    "\n",
    "def convertVectorDF (indf : org.apache.spark.sql.DataFrame, labelIndex : Int, featureIndex : Int, probIndex : Int, predictionIndex : Int) = {\n",
    "\n",
    "    //indf.select(colname).show(5)\n",
    "    val myCols = indf.columns\n",
    "    \n",
    "    // debug stuff\n",
    "    println(\"featureIndex value = \" + myCols(featureIndex))\n",
    "    println(\"probIndex value = \" + myCols(probIndex))\n",
    "    println(\"predictionIndex value = \" + myCols(predictionIndex))\n",
    "    \n",
    "    if(myCols(labelIndex) != \"label\") {\n",
    "        println(\"labelIndex value = \" + myCols(labelIndex))\n",
    "        println(\"Error\")\n",
    "    }\n",
    "    \n",
    "    \n",
    "    println(\" myCols.length = \" + myCols.length)\n",
    "    //println(\"Vector Size = \" + indf[0].features)\n",
    "    indf.show(3)\n",
    "    \n",
    "    // build an RDD of sql Rows here, then use toDF to convert back to a Dataframe..\n",
    "    val tmprdd = indf.map { line => \n",
    "        var reind = 0\n",
    "        var newRow : Seq[Double]= Seq(line(labelIndex).asInstanceOf[Double])\n",
    "        var labeledPointVector = line(featureIndex).asInstanceOf[org.apache.spark.mllib.linalg.Vector]\n",
    "        for(i <- 0 to labeledPointVector.size -1 ) {\n",
    "            newRow = newRow :+ labeledPointVector(i).asInstanceOf[Double]\n",
    "        }\n",
    "        var probArray = line(probIndex).asInstanceOf[org.apache.spark.mllib.linalg.Vector]\n",
    "        newRow = newRow :+ probArray(0).asInstanceOf[Double]\n",
    "        newRow = newRow :+ line(predictionIndex).asInstanceOf[Double]\n",
    "        //  https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/sql/Row.html see example where \n",
    "        //  Row.fromSeq is used to make a Row\n",
    "        Row.fromSeq(newRow)\n",
    "    }\n",
    "    //http://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema\n",
    "    //val schema =\n",
    "      // StructType(\n",
    "      // schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n",
    "    //tmprdd\n",
    "    //https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/types/StructType.html\n",
    "\n",
    "    // Take the first row. Each row just has a single 'Row' object at position 0\n",
    "    val numCols = tmprdd.take(1)(0).length\n",
    "   //val struct =\n",
    "    //StructType(\n",
    "     //StructField(\"label\", IntegerType, true) ::\n",
    "     //StructField(\"f1\", LongType, false) ::\n",
    "     //StructField(\"c\", BooleanType, false) :: Nil)\n",
    "     //sqlContext.createDataFrame(tmprdd,schema)\n",
    "    var schemaString = myCols(labelIndex);\n",
    "    for(i <-0 to numCols - 4){\n",
    "        schemaString = schemaString + \",f\" +i \n",
    "    }\n",
    "    schemaString = schemaString + \",\" + myCols(probIndex);\n",
    "    schemaString = schemaString + \",\" + myCols(predictionIndex);\n",
    "    \n",
    "    //schemaString = schemaString + tmprdd.take(1)(0).toString\n",
    "    println(schemaString)\n",
    "    val schema  = StructType(schemaString.split(\",\").map(fieldName => StructField(fieldName,DoubleType,true)))\n",
    "    val finalDF = sqlContext.createDataFrame(tmprdd, schema)\n",
    "    finalDF\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lets Publish The Model and Analysis And Vizualize In Jupyter ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var simpleUnpackedPredictionDF = convertVectorDF(simplePredictionDF,0,1,3,4)\n",
    "simpleUnpackedPredictionDF = simpleUnpackedPredictionDF.withColumn(\"correct\", when($\"label\" === $\"prediction\",1).otherwise(0))\n",
    "\n",
    "//var complexUnpackedPredictionDF = convertVectorDF(complexPredictionDF,0,1,3,4)\n",
    "//complexPredictionDF = complexPredictionDF.withColumn(\"correct\", when($\"label\" === $\"prediction\",1).otherwise(0))\n",
    "\n",
    "simpleUnpackedPredictionDF.write.format(\"json\").save(\"swift://notebooks.spark/test/simple001.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simpleUnpackedPredictionDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%AddJar https://github.com/dustinvanstee/random-public-files/raw/master/SystemML.jar\n",
    "import org.apache.sysml.api.MLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.sysml.api.MLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
