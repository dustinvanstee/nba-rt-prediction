{
    "metadata": {
        "language_info": {
            "file_extension": ".py", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "version": "2.7.11", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "name": "python"
        }, 
        "kernelspec": {
            "name": "python2-spark21", 
            "language": "python", 
            "display_name": "Python 2 with Spark 2.1"
        }
    }, 
    "nbformat": 4, 
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "### Imports", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 1, 
            "source": "spark.version\nDSX=True", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 13, 
            "source": "import re\nimport datetime\nfrom pyspark.sql.functions import *\nimport pandas as pd\npd.set_option('display.max_colwidth', 80)\n\nimport numpy as np\nimport seaborn as sns\nsns.set_palette(\"deep\", desat=0.6)\nsns.set_context(rc={\"figure.figsize\": (8,4)})\n\nimport matplotlib.pyplot as plt\n%matplotlib notebook\n%matplotlib inline", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### Load In NBA Score Data Set", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 50, 
            "source": "cleaned_dir = ''\nif(DSX) :\n    cleaned_dir = './nba-rt-prediction/sparkfiles/cleanedDF'\nelse :\n    cleaned_dir = '/data2/nba-rt-prediction/sparkfiles/cleanedDF'\n\ndf = spark.read.format('csv')\\\n                    .option(\"header\", \"true\")\\\n                    .option(\"inferSchema\", \"true\")\\\n                    .option(\"dateFormat\", \"yyyy-MM-dd\")\\\n                    .load(cleaned_dir).coalesce(2)\n\n# For some reason my key is none upon load ! Rebuild\ndf = df.withColumn(\"key\", concat(date_format(df.dateOrig, \"yyyy-MM-dd\"),lit(\".\"),col(\"away_team\"),lit(\".\"),col(\"home_team\")))\n            \n", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 51, 
            "source": "from pyspark.sql.functions import sum as sum_, lag, col, coalesce, lit, mean\nfrom pyspark.sql.window import Window\n\n#Define window for rolling average - use last 4 mins (8 data points)\nw = Window.partitionBy(\"key\").orderBy(\"pct_complete\")\n\n#score differences = away score momentum\nfor i in range(0,48,1) :\n    df = df.withColumn(\"min_dly_\" + str(i), lag(col(\"home_score\"), i*2,default=0).over(w))\n\ndf = df.cache()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 52, 
            "source": "f_list = []\nfor i in range(0,48,1) :\n    f_list.append(\"min_dly_\" + str(i))\n    \n    \nf_list2 = [\"cf1\",\"cf2\",\"home_score\", \"away_score\", \"score_diff_amh\", \"home_team_spread\",\"pct_complete\",\"overunder\"]\n\nfeatl = list(f_list[:] + f_list2[:])\nlen(featl)", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 52, 
                    "data": {
                        "text/plain": "56"
                    }
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 53, 
            "source": "#df.printSchema()\ndf_pd_X= df.select(featl).toPandas()\ndf_pd_Y= df.select([\"home_win\"]).toPandas()", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### Sanity Check with simple RF ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 54, 
            "source": "from sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\ntrain_X, test_X, train_y, test_y = train_test_split(df_pd_X, df_pd_Y, train_size=0.7, random_state=0)\n\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(train_X, train_y)\n\n#RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n#            max_depth=2, max_features='auto', max_leaf_nodes=None,\n#            min_impurity_decrease=0.0, min_impurity_split=None,\n#            min_samples_leaf=1, min_samples_split=2,\n#            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n#            oob_score=False, random_state=0, verbose=0, warm_start=False)\nprint(clf.feature_importances_)\npred_y = clf.predict(test_X)\nprint(\"Test fraction correct (Accuracy) = {:.3f}\".format(clf.score(test_X, test_y)))\n", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "[ 0.02131294  0.          0.07908173  0.01174947  0.          0.          0.\n  0.03048018  0.          0.          0.00324764  0.          0.          0.\n  0.          0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.00769305  0.0075895   0.          0.\n  0.          0.          0.          0.0102238   0.          0.          0.\n  0.          0.          0.          0.          0.          0.\n  0.02960714  0.          0.          0.          0.          0.01210472\n  0.          0.30453333  0.0253332   0.06680222  0.00761712  0.25083343\n  0.082144    0.          0.04964654]\nTest fraction correct (Accuracy) = 0.783\n"
                }, 
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "/usr/local/src/bluemix_jupyter_bundle.v71/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "### Sanity Check with simple Logistic Reg", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 55, 
            "source": "train_X.head()", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 55, 
                    "data": {
                        "text/plain": "        min_dly_0   min_dly_1  min_dly_2  min_dly_3   min_dly_4  min_dly_5  \\\n994     22.615385   20.296296      18.00    15.0000   12.600000    11.0000   \n7201    47.000000   45.666667      42.00    39.0000   37.000000    34.0000   \n5125   100.000000   95.000000      91.00    89.0000   83.551724    82.0000   \n21144  117.875000  115.312500     112.75   110.1875  107.625000   105.0625   \n14358   71.935484   70.000000      66.00    64.0000   62.560000    60.3200   \n\n        min_dly_6  min_dly_7  min_dly_8  min_dly_9    ...      min_dly_46  \\\n994      9.909091   7.000000   4.081081   2.000000    ...        0.000000   \n7201    32.000000  30.000000  27.000000  25.944444    ...        0.000000   \n5125    78.866667  74.736842  72.833333  70.769231    ...        3.447038   \n21144  102.500000  99.937500  97.375000  94.812500    ...        0.000000   \n14358   56.323529  56.000000  56.000000  53.783784    ...        0.000000   \n\n       min_dly_47         cf1          cf2  home_score  away_score  \\\n994      0.000000    0.349891     0.005337   22.615385   23.230769   \n7201     0.000000   -2.711215    -0.054870   47.000000   43.000000   \n5125     1.723519  210.000000  8360.250582  100.000000  121.000000   \n21144    0.000000  -50.160513    -8.069551  117.875000   96.791667   \n14358    0.000000    7.960298     0.299678   71.935484   79.935484   \n\n       score_diff_amh  home_team_spread  pct_complete  overunder  \n994          0.615385               2.5     22.916667      208.5  \n7201        -4.000000              -4.0     45.833333      203.0  \n5125        21.000000               1.0    100.000000      209.0  \n21144      -21.083333               4.0     95.833333      217.5  \n14358        8.000000              -4.5     75.000000      201.5  \n\n[5 rows x 56 columns]", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_dly_0</th>\n      <th>min_dly_1</th>\n      <th>min_dly_2</th>\n      <th>min_dly_3</th>\n      <th>min_dly_4</th>\n      <th>min_dly_5</th>\n      <th>min_dly_6</th>\n      <th>min_dly_7</th>\n      <th>min_dly_8</th>\n      <th>min_dly_9</th>\n      <th>...</th>\n      <th>min_dly_46</th>\n      <th>min_dly_47</th>\n      <th>cf1</th>\n      <th>cf2</th>\n      <th>home_score</th>\n      <th>away_score</th>\n      <th>score_diff_amh</th>\n      <th>home_team_spread</th>\n      <th>pct_complete</th>\n      <th>overunder</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>994</th>\n      <td>22.615385</td>\n      <td>20.296296</td>\n      <td>18.00</td>\n      <td>15.0000</td>\n      <td>12.600000</td>\n      <td>11.0000</td>\n      <td>9.909091</td>\n      <td>7.000000</td>\n      <td>4.081081</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.349891</td>\n      <td>0.005337</td>\n      <td>22.615385</td>\n      <td>23.230769</td>\n      <td>0.615385</td>\n      <td>2.5</td>\n      <td>22.916667</td>\n      <td>208.5</td>\n    </tr>\n    <tr>\n      <th>7201</th>\n      <td>47.000000</td>\n      <td>45.666667</td>\n      <td>42.00</td>\n      <td>39.0000</td>\n      <td>37.000000</td>\n      <td>34.0000</td>\n      <td>32.000000</td>\n      <td>30.000000</td>\n      <td>27.000000</td>\n      <td>25.944444</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-2.711215</td>\n      <td>-0.054870</td>\n      <td>47.000000</td>\n      <td>43.000000</td>\n      <td>-4.000000</td>\n      <td>-4.0</td>\n      <td>45.833333</td>\n      <td>203.0</td>\n    </tr>\n    <tr>\n      <th>5125</th>\n      <td>100.000000</td>\n      <td>95.000000</td>\n      <td>91.00</td>\n      <td>89.0000</td>\n      <td>83.551724</td>\n      <td>82.0000</td>\n      <td>78.866667</td>\n      <td>74.736842</td>\n      <td>72.833333</td>\n      <td>70.769231</td>\n      <td>...</td>\n      <td>3.447038</td>\n      <td>1.723519</td>\n      <td>210.000000</td>\n      <td>8360.250582</td>\n      <td>100.000000</td>\n      <td>121.000000</td>\n      <td>21.000000</td>\n      <td>1.0</td>\n      <td>100.000000</td>\n      <td>209.0</td>\n    </tr>\n    <tr>\n      <th>21144</th>\n      <td>117.875000</td>\n      <td>115.312500</td>\n      <td>112.75</td>\n      <td>110.1875</td>\n      <td>107.625000</td>\n      <td>105.0625</td>\n      <td>102.500000</td>\n      <td>99.937500</td>\n      <td>97.375000</td>\n      <td>94.812500</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-50.160513</td>\n      <td>-8.069551</td>\n      <td>117.875000</td>\n      <td>96.791667</td>\n      <td>-21.083333</td>\n      <td>4.0</td>\n      <td>95.833333</td>\n      <td>217.5</td>\n    </tr>\n    <tr>\n      <th>14358</th>\n      <td>71.935484</td>\n      <td>70.000000</td>\n      <td>66.00</td>\n      <td>64.0000</td>\n      <td>62.560000</td>\n      <td>60.3200</td>\n      <td>56.323529</td>\n      <td>56.000000</td>\n      <td>56.000000</td>\n      <td>53.783784</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.960298</td>\n      <td>0.299678</td>\n      <td>71.935484</td>\n      <td>79.935484</td>\n      <td>8.000000</td>\n      <td>-4.5</td>\n      <td>75.000000</td>\n      <td>201.5</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 56 columns</p>\n</div>"
                    }
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 21, 
            "source": "from sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.utils import np_utils", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 22, 
            "source": "train_X, test_X, train_y, test_y = train_test_split(X, Y, train_size=0.7, random_state=0)\nlr = LogisticRegressionCV()\nlr.fit(train_X, train_y)\npred_y = lr.predict(test_X)\nprint(\"Test fraction correct (Accuracy) = {:.3f}\".format(lr.score(test_X, test_y)))", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "/usr/local/src/bluemix_jupyter_bundle.v71/notebook/lib/python2.7/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n"
                }, 
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Test fraction correct (Accuracy) = 0.793\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "source": "### Use DNN Keras", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 41, 
            "source": "model = Sequential()\nmodel.add(Dense(56, input_shape=(56,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 57, 
            "source": "train_X_np = train_X.values\ntrain_y_np = train_y.values\n\ncb = model.fit(train_X_np, train_y_np, verbose=0, batch_size=100, epochs=500)", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 59, 
            "source": "test_X_np = test_X.values\ntest_y_np = test_y.values\n(loss, accuracy) = model.evaluate(test_X_np, test_y_np, verbose=1,)\n\n#print('Test score:', score[0])\n#print('Test accuracy:', score[1])", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "9216/9260 [============================>.] - ETA: 0s"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 60, 
            "source": "cb.history ", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 60, 
                    "data": {
                        "text/plain": "{'acc': [0.81484909898328506,\n  0.81670060836039438,\n  0.81383077024729289,\n  0.81744121285726179,\n  0.81563599143640086,\n  0.81651545713133689,\n  0.81563599060871184,\n  0.81424736098169592,\n  0.81267357745494606,\n  0.81591371756265663,\n  0.81383077024729289,\n  0.81563599198819359,\n  0.81350675644068515,\n  0.81855211856519261,\n  0.8179503787647594,\n  0.81420107026648425,\n  0.81859840491020641,\n  0.81609886796402498,\n  0.80998888919142042,\n  0.81526568921003884,\n  0.81720977417960516,\n  0.81137752125735985,\n  0.81225698860767392,\n  0.81359933019015462,\n  0.81415478377800432,\n  0.81452508545257363,\n  0.81309016538624235,\n  0.80938714773560916,\n  0.8158211421578091,\n  0.81683946887424008,\n  0.81605258166315464,\n  0.81508053977982553,\n  0.81313645232719223,\n  0.81336789422731803,\n  0.81406221030443116,\n  0.81526568700286817,\n  0.81332160741879833,\n  0.81572856628945567,\n  0.81716348488801849,\n  0.81267357607546442,\n  0.81577485475335332,\n  0.81503425214361691,\n  0.81355304200215328,\n  0.81619144387652176,\n  0.816561745639378,\n  0.81989446248727427,\n  0.81392334620393314,\n  0.81688576105722033,\n  0.80980373736642686,\n  0.81239585192462649,\n  0.81309016409504742,\n  0.81489538574766129,\n  0.8127198637558164,\n  0.81614515596441684,\n  0.81359932881067298,\n  0.81447879855576721,\n  0.81665431957645696,\n  0.81674689548895374,\n  0.81707091039914681,\n  0.81540455386232968,\n  0.81920014503067939,\n  0.81012775218833311,\n  0.81216441241928083,\n  0.81535826700966663,\n  0.8154508410350324,\n  0.81549713060251539,\n  0.81346046691734553,\n  0.81290501392543191,\n  0.81072949322478194,\n  0.81133123357700787,\n  0.81707090842372909,\n  0.81642288117469675,\n  0.81646916885504872,\n  0.81225698529691792,\n  0.81253471299026481,\n  0.81674689769612441,\n  0.81808924184995901,\n  0.8173486380924937,\n  0.81688575774646432,\n  0.81688575880590619,\n  0.8166080346109249,\n  0.81882984694276251,\n  0.8173486362053628,\n  0.81776522560442755,\n  0.81836696765617489,\n  0.81711719996662979,\n  0.81753378692677103,\n  0.81540455031982062,\n  0.81540455059571704,\n  0.81383077084322897,\n  0.81813552815082935,\n  0.81753378775446006,\n  0.81591371807030588,\n  0.81549712839534472,\n  0.81716348571570752,\n  0.81369190858571849,\n  0.81693204648625828,\n  0.81540455528595479,\n  0.81674689659253907,\n  0.81744121345319787,\n  0.81702462271879484,\n  0.81688575857415324,\n  0.81679318648006183,\n  0.81957044762122455,\n  0.81596000432703275,\n  0.81554341552390408,\n  0.8156822777372712,\n  0.82017219077655723,\n  0.81702462405413312,\n  0.81961673612926567,\n  0.81808924042633391,\n  0.81869098059095025,\n  0.81855212086065021,\n  0.81683947448045358,\n  0.81827439087184572,\n  0.81947787166458441,\n  0.81378448311873353,\n  0.81887613237180046,\n  0.81670060973987602,\n  0.81730235018038877,\n  0.81725606130816453,\n  0.8114238092577517,\n  0.81781151232466043,\n  0.81614515592027337,\n  0.81901499596464922,\n  0.81521940088960732,\n  0.81720977426789199,\n  0.8159137170108639,\n  0.81577485203853339,\n  0.81702462547775812,\n  0.82007961399222806,\n  0.81651545851081853,\n  0.81248842724118708,\n  0.81091464390204671,\n  0.81133123219752612,\n  0.81776522532853124,\n  0.81906128286145563,\n  0.81734863643711575,\n  0.81443250843649151,\n  0.81521939978602198,\n  0.81781151411246866,\n  0.81387705484864181,\n  0.8126272903263867,\n  0.8173486380924937,\n  0.81850582793826776,\n  0.81739492416161119,\n  0.81373819654196677,\n  0.81355304274155549,\n  0.81744121239375589,\n  0.81952415989672911,\n  0.81924643767716554,\n  0.81332160360039307,\n  0.81540455358643327,\n  0.81545083965555076,\n  0.81554341497211147,\n  0.81670060950812307,\n  0.82086650285869145,\n  0.81702462428588596,\n  0.81813552649545129,\n  0.82017218856938656,\n  0.82007961569174948,\n  0.81614515757565143,\n  0.81614515403314247,\n  0.81642288255417839,\n  0.81679318372109844,\n  0.81924643326282409,\n  0.81813552861433514,\n  0.81355304310573862,\n  0.81859840463431011,\n  0.81693204680629805,\n  0.81702462267465137,\n  0.81730235211166313,\n  0.8148953871712864,\n  0.81808924038219044,\n  0.8175800773660864,\n  0.81517311297750239,\n  0.81808924102226999,\n  0.81952415934493639,\n  0.82026476365419432,\n  0.81804295302187813,\n  0.8174412145567832,\n  0.81535826484663931,\n  0.81989446110779263,\n  0.81341417868520083,\n  0.81651545874257148,\n  0.81591371719847339,\n  0.81697833495015593,\n  0.81591371669082413,\n  0.81586743149353913,\n  0.81790408998082198,\n  0.8178115154919503,\n  0.81424735937046133,\n  0.81637659519386607,\n  0.81730234930855639,\n  0.8193852949243986,\n  0.81924643464230584,\n  0.81966302463730667,\n  0.82003332387295236,\n  0.81929272015963062,\n  0.8188761307164224,\n  0.81447879864405404,\n  0.81896870777664788,\n  0.81915385872980906,\n  0.81841325391290187,\n  0.82160710803978176,\n  0.82007961100151172,\n  0.82031105078275368,\n  0.81984817480692218,\n  0.81758007626250107,\n  0.81943158292479035,\n  0.81924643684947651,\n  0.81850583028890456,\n  0.81420107145835641,\n  0.81466394798598052,\n  0.81401592207228646,\n  0.81637659464207346,\n  0.8199870370202893,\n  0.81947787387175508,\n  0.81845954246508634,\n  0.81753378665087462,\n  0.81577485424570406,\n  0.8132290295750273,\n  0.81457137317706907,\n  0.81401591848563404,\n  0.81406220841730026,\n  0.81836696738027859,\n  0.81591371866624196,\n  0.82058877957968601,\n  0.81984817425512957,\n  0.81077578030919795,\n  0.81887613347538579,\n  0.81887613149996796,\n  0.81795038129196984,\n  0.81753378908979824,\n  0.8168857605054276,\n  0.81813552865847861,\n  0.81707091122683584,\n  0.81952416021676888,\n  0.81878355728699259,\n  0.81906128396504108,\n  0.81628402034081127,\n  0.81882984303607032,\n  0.82151453208314162,\n  0.82095907941126778,\n  0.81952415911318355,\n  0.82211627197186166,\n  0.8229031636414319,\n  0.8200333257159399,\n  0.81813552833843883,\n  0.81813552810668588,\n  0.81609886874757065,\n  0.81753378577904223,\n  0.80980373653873783,\n  0.81545084131092871,\n  0.81429364976563334,\n  0.81540455110336629,\n  0.82086650593769461,\n  0.81980188621059435,\n  0.81596000607069763,\n  0.81494167572347087,\n  0.8185521182892963,\n  0.81762636357866991,\n  0.81785780363580829,\n  0.81498796497091408,\n  0.82072764238898921,\n  0.81660803217200129,\n  0.81693204896932525,\n  0.81540455473416207,\n  0.81758007649425402,\n  0.81836696784378449,\n  0.81869098165039222,\n  0.81864469456597622,\n  0.81498796340382296,\n  0.81489538965435337,\n  0.81512682695252836,\n  0.81498796340382296,\n  0.81475652606150462,\n  0.81517311302164575,\n  0.81957044895656284,\n  0.81850583037719138,\n  0.81915385762622372,\n  0.8145713747441603,\n  0.81836696646430274,\n  0.8195704490007063,\n  0.81771893953531016,\n  0.81850583124902376,\n  0.82068135305325907,\n  0.81943158540785743,\n  0.82049620205595453,\n  0.81924643275517484,\n  0.81646917133811581,\n  0.81980188515115238,\n  0.81920014696195376,\n  0.81535826337887074,\n  0.81790408887723665,\n  0.81628402010905832,\n  0.81707091260631759,\n  0.81628402057256422,\n  0.81299759195681254,\n  0.81720977371609937,\n  0.82003332548418695,\n  0.81859840523024618,\n  0.82035733818720946,\n  0.8208665037746673,\n  0.82244028706966432,\n  0.82058877677657915,\n  0.81484909976683062,\n  0.81697833476254644,\n  0.81947787221637702,\n  0.81813553003796025,\n  0.8155897040319452,\n  0.81822810516691147,\n  0.8203110513345464,\n  0.81818181665887035,\n  0.820357340030197,\n  0.81281244049600221,\n  0.81457137372886179,\n  0.81790408919727642,\n  0.81887613177586427,\n  0.82077392722209108,\n  0.81813552787493304,\n  0.81781151296473986,\n  0.78726161579096054,\n  0.79429735010188063,\n  0.79920384812589007,\n  0.8084613941285681,\n  0.80591557032972372,\n  0.81572856573766306,\n  0.81285872872814691,\n  0.81216441186748811,\n  0.81387705985891934,\n  0.81642288227828208,\n  0.81498796423151187,\n  0.81679318459293093,\n  0.8142473585427723,\n  0.81998703725204225,\n  0.81841325529238362,\n  0.81003517742356512,\n  0.81517311297750239,\n  0.81822810378742983,\n  0.81957044927660261,\n  0.81665432068004229,\n  0.81683947365276466,\n  0.8216996841840315,\n  0.81758007598660476,\n  0.81688576128897328,\n  0.81836696733613523,\n  0.81947787115693516,\n  0.81609886938765008,\n  0.81549712871538449,\n  0.82054249028809934,\n  0.82040362917831755,\n  0.82109794410770187,\n  0.81707091122683584,\n  0.81933901051065916,\n  0.81827439201957453,\n  0.81730234907680344,\n  0.82040362641935416,\n  0.82040362747879603,\n  0.81623773187691362,\n  0.81540455275874424,\n  0.81443251151549467,\n  0.81984817227971174,\n  0.81933901059894598,\n  0.81785780206871705,\n  0.81970931121407342,\n  0.82044991543504442,\n  0.81994075016762635,\n  0.82072764073361115,\n  0.81790408998082198,\n  0.82142195837781551,\n  0.81929272089903271,\n  0.81975559723904745,\n  0.81906128392089761,\n  0.82054248969216326,\n  0.82049620371133258,\n  0.81984817333915372,\n  0.816052580283673,\n  0.82082021632606816,\n  0.82109794272822023,\n  0.81910757463610939,\n  0.81878355760703236,\n  0.81975559747080029,\n  0.82044991377966647,\n  0.82026476223056921,\n  0.81920014507482286,\n  0.81864469360585701,\n  0.81549712867124102,\n  0.82017218820520343,\n  0.81878355866647434,\n  0.81711720001077326,\n  0.81799666557327899,\n  0.81970931392889335,\n  0.81253471538504507,\n  0.81568227971268903,\n  0.82044991318373039,\n  0.81818181578703797,\n  0.82063506560466004,\n  0.82054248914037053,\n  0.81947787336410582,\n  0.81924643188334245,\n  0.82072764151715671,\n  0.82054249056399564,\n  0.82105165859037721,\n  0.82244028725727381,\n  0.81378448224690114,\n  0.81596000662249035,\n  0.82281059076379481,\n  0.82077392892161249,\n  0.82072764128540387,\n  0.81933901115073871,\n  0.81586742790688671,\n  0.81818181578703797,\n  0.81961673695695469,\n  0.82327346669548285,\n  0.81401592101284448,\n  0.82146824440278954,\n  0.82230142283673602,\n  0.81591371613903152,\n  0.82044991409970613,\n  0.81707091063089976,\n  0.81980188653063413,\n  0.81790409002496545,\n  0.8179966651097732,\n  0.81697833421075372,\n  0.81776522606793345,\n  0.8210516564273499,\n  0.8174412142808869,\n  0.81549712807530494,\n  0.81906128120607768,\n  0.81943158444773823,\n  0.82040362775469244,\n  0.81836696678434251,\n  0.81744121317730156,\n  0.818783555675758,\n  0.82211627252365438,\n  0.82077392892161249,\n  0.82262543935816368,\n  0.82063506643234896,\n  0.8182743914677818,\n  0.81646917051042678,\n  0.8140159204169084,\n  0.81577485479749678,\n  0.813877056868203,\n  0.81605257922423102,\n  0.82049620477077456,\n  0.82003332387295236,\n  0.81855211860933608,\n  0.81947787226052049,\n  0.81753378803035637,\n  0.81822810516691147,\n  0.81906128281731228,\n  0.81633030746937063,\n  0.82031105041857055,\n  0.81637659381438443,\n  0.81947787244812997,\n  0.81822810571870419,\n  0.81924643385876017,\n  0.81906128424093738,\n  0.82114423040857232,\n  0.82160711052284885,\n  0.81702462023572775,\n  0.81910756999001511,\n  0.81882984556328076,\n  0.81656174582698748,\n  0.81179411097646448,\n  0.81433993422351614,\n  0.81637659625330805,\n  0.81549712729175938,\n  0.81859840463431011,\n  0.81591371503544619,\n  0.81813552672720424,\n  0.81961673636101851,\n  0.81776522670801288,\n  0.81545083988730371,\n  0.81535826342301421,\n  0.81873726909899136,\n  0.82100536543624181,\n  0.81447879722042893,\n  0.81929272066727987,\n  0.81605258221494736,\n  0.81512682584894303,\n  0.81989446303906699,\n  0.82068135438859735,\n  0.81929272121907248,\n  0.82239399966520854,\n  0.82063506583641288,\n  0.82410664746903017,\n  0.81832067997582292,\n  0.81855211833343977,\n  0.81984817527042808,\n  0.8158211435814342,\n  0.81540455188691185,\n  0.81836696843972057,\n  0.81873726845891182,\n  0.81859840412666085,\n  0.82063506454521806,\n  0.81688575931355545,\n  0.81938529662392001,\n  0.81373819654196677],\n 'loss': [0.38362440768435496,\n  0.38754991946717454,\n  0.38935204708031029,\n  0.38350924968697409,\n  0.38457868881544299,\n  0.38388703991099432,\n  0.38553675257636538,\n  0.38554671854249772,\n  0.38619798029535152,\n  0.38417679445817721,\n  0.38799078183148972,\n  0.3833611820653906,\n  0.38417707500612708,\n  0.38220193691881382,\n  0.38597358583609409,\n  0.38637782363202083,\n  0.38295174221642875,\n  0.38436077917820655,\n  0.38981901616040171,\n  0.38740049520428849,\n  0.38467208868131619,\n  0.38901955873143823,\n  0.38841792152581711,\n  0.38858786271669493,\n  0.38708741278134989,\n  0.38822532303795465,\n  0.38724674507382401,\n  0.38720887459068248,\n  0.38405815797080772,\n  0.38248499591711205,\n  0.38108609774569147,\n  0.38507677645313365,\n  0.38790400130917374,\n  0.3876498385096806,\n  0.38421026310905709,\n  0.38400650967979538,\n  0.38744495417029523,\n  0.3841152012491641,\n  0.38622860256510078,\n  0.38930370441091833,\n  0.38450735777070227,\n  0.38446438031668045,\n  0.38559696991288878,\n  0.38510468766062905,\n  0.38201650843600876,\n  0.38216200086324881,\n  0.38512811901237848,\n  0.38169843933467973,\n  0.40772062950588073,\n  0.38844429535890945,\n  0.38838274672094969,\n  0.38539413074567747,\n  0.38513041181578456,\n  0.38486994849854189,\n  0.38541665447020923,\n  0.38362747959418775,\n  0.38444098909129437,\n  0.38396012153730991,\n  0.38355740389433685,\n  0.38215081080576374,\n  0.38344464374237469,\n  0.38926692542886759,\n  0.387636596746322,\n  0.38289485688596669,\n  0.38416351374377367,\n  0.38593552408781656,\n  0.39410468410659866,\n  0.38843202933881532,\n  0.38643756529243539,\n  0.38813685949280008,\n  0.38348526563760948,\n  0.38399384629724204,\n  0.38125875986487617,\n  0.38557146295665085,\n  0.38502009069780535,\n  0.38124388263217168,\n  0.38174236891523949,\n  0.38340220255910012,\n  0.38641275763421989,\n  0.38299682642040245,\n  0.38697566108292197,\n  0.38089800912281957,\n  0.37950143832599559,\n  0.38266435751394529,\n  0.38259957971903508,\n  0.38150557265902335,\n  0.38103685397403014,\n  0.38380277256428413,\n  0.38384686664251721,\n  0.38627072209187113,\n  0.38530168024461992,\n  0.38188413909255875,\n  0.38431582473379838,\n  0.38608011350082572,\n  0.38455373763077966,\n  0.38535149944499564,\n  0.38487103693442443,\n  0.38402354063104421,\n  0.385344173743358,\n  0.38423819927965308,\n  0.37997153823496216,\n  0.38248130056984314,\n  0.38663384297383624,\n  0.38238943983583268,\n  0.38581562841783912,\n  0.38564468173931271,\n  0.38455100340702825,\n  0.38017210617338676,\n  0.38047586268317896,\n  0.38092109109600081,\n  0.3821971320461463,\n  0.37874688575873,\n  0.38045212611834711,\n  0.38005047322969437,\n  0.38069152669804202,\n  0.38346266355299991,\n  0.38687757340719736,\n  0.38033181131493049,\n  0.38451941865635147,\n  0.38232220524749233,\n  0.3851227069084257,\n  0.38984144624295752,\n  0.38200442925843237,\n  0.38383400840817544,\n  0.37988290160624016,\n  0.38596794888766206,\n  0.38578132712698671,\n  0.38502916797424463,\n  0.38362512031493862,\n  0.38378331874381877,\n  0.38047525744662419,\n  0.38339728441719123,\n  0.38923796202447364,\n  0.39010993793946108,\n  0.38697007451316112,\n  0.38046758755051058,\n  0.38104792722804176,\n  0.38100296652906362,\n  0.38544702619573534,\n  0.38666905662214907,\n  0.3816369714957214,\n  0.38208133290154339,\n  0.38611413686612295,\n  0.37977475798355165,\n  0.38160081255758188,\n  0.3845834813941697,\n  0.38543426461339858,\n  0.38381893946338286,\n  0.38017423895139912,\n  0.38108452215688665,\n  0.37900746611947594,\n  0.38536813790611463,\n  0.38217482723984048,\n  0.3849221005928205,\n  0.38312901210793743,\n  0.38436702438336917,\n  0.37807843776637373,\n  0.3821810678053158,\n  0.38184377209519133,\n  0.37607956851627888,\n  0.37904418515201144,\n  0.3841273189694589,\n  0.37929050962895383,\n  0.38412846522493688,\n  0.38266115851522353,\n  0.37992346562118229,\n  0.38172012413569983,\n  0.38976848429771421,\n  0.38283930476937683,\n  0.38624091183746373,\n  0.37973764544834671,\n  0.3800157076246185,\n  0.38532984807985265,\n  0.37966974486556015,\n  0.38000512181597934,\n  0.38284583515291898,\n  0.3858827736102482,\n  0.38321765270128977,\n  0.37847131805643069,\n  0.38098658885895775,\n  0.38049212704817692,\n  0.38372070064612657,\n  0.37832727838480745,\n  0.38663530269721508,\n  0.38285843755686377,\n  0.38277995086746996,\n  0.38536971674497822,\n  0.39172760824697189,\n  0.3873165152102041,\n  0.38361104137587165,\n  0.38157110680167133,\n  0.38638860231151273,\n  0.38136976933318184,\n  0.38018589369832662,\n  0.38110334549076447,\n  0.38000763710770996,\n  0.37940569809908514,\n  0.38074270977264996,\n  0.37727580933940785,\n  0.3800736805853458,\n  0.38386139551333159,\n  0.38015906460592869,\n  0.37934629086365723,\n  0.38193296170018376,\n  0.3794394156670895,\n  0.38447816870517587,\n  0.38025356135838917,\n  0.3828514651377275,\n  0.38398358655117149,\n  0.38187374001883773,\n  0.37837999459880961,\n  0.38142342201384588,\n  0.38393261115201377,\n  0.3843525292514276,\n  0.38728480527984105,\n  0.38522015680700206,\n  0.37992399973166119,\n  0.37876698872741121,\n  0.37798563467117618,\n  0.37704029293871216,\n  0.38310915545959262,\n  0.38536984956975207,\n  0.38535130726388372,\n  0.38689668240138414,\n  0.38627588339501717,\n  0.38452452544789911,\n  0.38545004087363188,\n  0.38039712540817755,\n  0.38026440247874022,\n  0.386935242898507,\n  0.38280957051613357,\n  0.38368163095748198,\n  0.38198827827475873,\n  0.38162830266173825,\n  0.38094627551101268,\n  0.37827509579846558,\n  0.38229786755420214,\n  0.37637891575564414,\n  0.38108984815635144,\n  0.38014701557002717,\n  0.38387088156192306,\n  0.38068847993064248,\n  0.37924240017197702,\n  0.37973908148326602,\n  0.37825631215227595,\n  0.37468844426492698,\n  0.37652799968277167,\n  0.37854119299893202,\n  0.37751697653577276,\n  0.38091811200597292,\n  0.38062559236223842,\n  0.37938676844579472,\n  0.38978290560068501,\n  0.38181792455919361,\n  0.38516598733801066,\n  0.38200287241954273,\n  0.37946959474730813,\n  0.37957418197791809,\n  0.38620848976071509,\n  0.38377606236941281,\n  0.38091824214627534,\n  0.38173451747105885,\n  0.37927297969986568,\n  0.38555386214427562,\n  0.37722326480190171,\n  0.38111434484318302,\n  0.38073684569531691,\n  0.38398241946276107,\n  0.38489937100823646,\n  0.37779089971794683,\n  0.38058948527329761,\n  0.37936547023899975,\n  0.38035060403259169,\n  0.38106195353022593,\n  0.38336963750735142,\n  0.38227163282528431,\n  0.38075849103764281,\n  0.38198360222740274,\n  0.3799939743017095,\n  0.38046672436191603,\n  0.38062008038381145,\n  0.38728462961112498,\n  0.38106519813185336,\n  0.3817457350436857,\n  0.37923541961665774,\n  0.3799290590055826,\n  0.37812238062582421,\n  0.3753127650052745,\n  0.37571738453689185,\n  0.37838262930951105,\n  0.38595463092725291,\n  0.37880746223180078,\n  0.38385356416681965,\n  0.38543796353628407,\n  0.37982294690143265,\n  0.38300398267064795,\n  0.38143120401767733,\n  0.3821008911211829,\n  0.38597894527143606,\n  0.37939210038806659,\n  0.38002164841492825,\n  0.38135723372236091,\n  0.37779007083430288,\n  0.37870385001332996,\n  0.37755877837342833,\n  0.37886138924447016,\n  0.38734719093210984,\n  0.387921706832486,\n  0.38026618205977908,\n  0.37780888270192886,\n  0.38372705574996646,\n  0.38109776177997945,\n  0.37845409956809506,\n  0.37648197726019444,\n  0.3793934455316964,\n  0.38436891619440938,\n  0.37963047988369297,\n  0.37656719539534533,\n  0.37804679818957676,\n  0.38176581021856981,\n  0.38158175841986391,\n  0.37897012192791113,\n  0.42344015933831208,\n  0.40775110993732283,\n  0.39788778901784383,\n  0.38989947088734572,\n  0.39354257042806162,\n  0.38362076632724029,\n  0.384565742949527,\n  0.38579542322266747,\n  0.38316532787935004,\n  0.38238929748711731,\n  0.38517039563176458,\n  0.38292965434465426,\n  0.3835897902342329,\n  0.3757257821095959,\n  0.3783273524464203,\n  0.38986302864505196,\n  0.38179309820934615,\n  0.37746415428929009,\n  0.37818601411308456,\n  0.38309574526695162,\n  0.37762331187316739,\n  0.3782988353760855,\n  0.3788663987701435,\n  0.37989024756418688,\n  0.37747424907694038,\n  0.37742689237829008,\n  0.37745117136359857,\n  0.3793567481266748,\n  0.3757832643748919,\n  0.37906273216467129,\n  0.37284452443188199,\n  0.38005201672761668,\n  0.37782031894723389,\n  0.37900209681256308,\n  0.38026968261457728,\n  0.37427376759447362,\n  0.37778555119706403,\n  0.38219923276500423,\n  0.38282428315184203,\n  0.38203779296386331,\n  0.374502746992874,\n  0.3781968432705704,\n  0.38618754073936085,\n  0.37730105702503702,\n  0.37709185434720888,\n  0.37751268732439608,\n  0.37662774557273621,\n  0.37934674998827056,\n  0.37676271041925913,\n  0.37794080627138937,\n  0.37840980365030902,\n  0.37509689948124436,\n  0.37660560273965227,\n  0.37747292157274287,\n  0.3802574758639139,\n  0.38092396945075796,\n  0.37599189818688972,\n  0.37662822818854325,\n  0.37932997987217826,\n  0.37896679465118721,\n  0.37774001732068557,\n  0.37610141956967308,\n  0.38060133769719207,\n  0.3755308900644837,\n  0.37980995448490706,\n  0.38274323311785241,\n  0.37762760978631454,\n  0.38104648140156144,\n  0.3795241944531868,\n  0.37848522627912085,\n  0.37941773237177362,\n  0.38553701636085375,\n  0.37943174108556632,\n  0.37724776708667002,\n  0.38011062555215114,\n  0.37841931493651798,\n  0.37391376743668031,\n  0.37895184252090397,\n  0.376600997632487,\n  0.37376678144737563,\n  0.37378541463763598,\n  0.37835711475832112,\n  0.38016071515025046,\n  0.38658602000209846,\n  0.37934548467524604,\n  0.3751356913202053,\n  0.37663740323022921,\n  0.37457872181992863,\n  0.37779433600131374,\n  0.38238595513541751,\n  0.37637535126049021,\n  0.374779642111084,\n  0.37448078277507901,\n  0.38593632692960517,\n  0.37507263743045305,\n  0.37667791045191024,\n  0.38318706638083327,\n  0.375567507760941,\n  0.37892449855076077,\n  0.37765100761665898,\n  0.37865808610010315,\n  0.3790693412641975,\n  0.37776640065702721,\n  0.37996565246665903,\n  0.37322797239864119,\n  0.37896256649567006,\n  0.38044578745241098,\n  0.37734939101679948,\n  0.37875967893947432,\n  0.37540191988043597,\n  0.37779874991783496,\n  0.379564888636021,\n  0.37608763532514067,\n  0.37768050320301644,\n  0.37806777155326488,\n  0.37429034751314871,\n  0.37584822315779953,\n  0.37811016921665114,\n  0.37975129874971836,\n  0.38410944404303643,\n  0.38009799103249314,\n  0.3827856544283138,\n  0.38140406373628222,\n  0.37739789571459259,\n  0.37760559650449438,\n  0.3742226830369299,\n  0.37896846424063318,\n  0.38255368196849493,\n  0.37799909337182502,\n  0.37842854083555538,\n  0.38587189963892021,\n  0.37935161874749401,\n  0.37979723183430247,\n  0.37848807736980189,\n  0.38089557551295861,\n  0.37754139004813547,\n  0.37724497500469761,\n  0.37615849579005212,\n  0.37358926032880041,\n  0.37909673546126893,\n  0.37618926709072609,\n  0.37667510911085678,\n  0.38354269029850918,\n  0.3898197790468827,\n  0.3857392491284779,\n  0.38360999165100423,\n  0.38258769430621559,\n  0.37972984754736833,\n  0.38089250752637177,\n  0.37947391368927413,\n  0.37899004864573499,\n  0.37765607393697287,\n  0.38358660178578707,\n  0.38770505598332566,\n  0.37658918217318388,\n  0.37504341749764442,\n  0.38024787278147543,\n  0.37772745493550453,\n  0.38089949965554243,\n  0.38254122641865268,\n  0.37534345784896206,\n  0.37423886405640322,\n  0.37644605555703875,\n  0.37246430710639539,\n  0.3747913538747486,\n  0.37250719606942589,\n  0.37876040969506786,\n  0.37471691800850099,\n  0.37396837313502745,\n  0.37723714798286079,\n  0.38223287765570557,\n  0.37520131254401434,\n  0.37865718043726743,\n  0.37797224810224533,\n  0.378139725997506,\n  0.37707833841186478,\n  0.37968493568480094,\n  0.38221851212517949]}"
                    }
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 89, 
            "source": "# Merge the results back to the Pandas DF for quick vis\n#len(model.predict(test_X))\nvis_pd = test_X\nvis_pd[\"quarter\"] = np.where(vis_pd['pct_complete'] > 75, 'Q4', np.where(vis_pd['pct_complete'] > 50, 'Q3', np.where(vis_pd['pct_complete'] > 25, 'Q2', 'Q1')))\nvis_pd[\"probability\"] = model.predict(test_X_np)\nvis_pd[\"prediction\"]  = np.where(vis_pd['probability'] > 0.5, 1.0, 0.0)\nvis_pd[\"label\"] = test_y_np\nvis_pd[\"correct\"] = np.where(vis_pd['prediction'] == vis_pd['label'], 'yes', 'no')\n\n\n\n", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "/usr/local/src/bluemix_jupyter_bundle.v71/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/usr/local/src/bluemix_jupyter_bundle.v71/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/usr/local/src/bluemix_jupyter_bundle.v71/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/usr/local/src/bluemix_jupyter_bundle.v71/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/usr/local/src/bluemix_jupyter_bundle.v71/notebook/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 90, 
            "source": "vis_pd.head(20)", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 90, 
                    "data": {
                        "text/plain": "        min_dly_0   min_dly_1  min_dly_2  min_dly_3  min_dly_4  min_dly_5  \\\n8966    64.703704   61.000000  61.000000  58.000000  57.918919  48.935484   \n21948   42.864865   42.000000  38.871795  36.774194  34.947368  34.000000   \n9055    56.741514   47.371429  45.000000  44.142857  43.000000  39.000000   \n26355   61.000000   61.000000  58.120000  54.157895  50.000000  49.351852   \n11599   75.285714   73.000000  69.783784  68.000000  65.562500  62.411765   \n1992    37.000000   37.000000  36.500000  33.142857  31.162162  29.000000   \n4467    85.000000   85.000000  81.400000  79.000000  79.000000  79.000000   \n25365   21.750000   18.720000  15.000000  15.000000  13.000000   8.588235   \n27958    3.000000    0.000000   0.000000   0.000000   0.000000   0.000000   \n8573    50.751938   49.201550  47.651163  46.100775  44.550388  43.000000   \n1063   102.272727  101.000000  99.000000  97.000000  95.000000  93.000000   \n17917   52.940278   52.000000  47.867925  45.000000  45.000000  43.000000   \n23760   23.604167   21.458333  19.312500  17.166667  15.020833  12.875000   \n17140   93.000000   92.650794  91.000000  91.000000  88.960784  83.193548   \n25759   82.935484   79.125000  77.629630  76.000000  70.000000  68.000000   \n24193   32.229167   30.333333  28.437500  26.541667  24.645833  22.750000   \n28244   78.000000   76.375000  74.750000  73.125000  71.500000  69.875000   \n964     85.052632   79.000000  78.000000  75.000000  75.000000  72.454545   \n16206  101.000000  101.000000  97.000000  94.285714  93.470588  90.000000   \n29535   20.000000   20.000000  18.710280  17.028037  12.280000   9.000000   \n\n       min_dly_6  min_dly_7  min_dly_8  min_dly_9   ...     away_score  \\\n8966   46.878788  45.000000  43.250000  41.000000   ...      53.000000   \n21948  34.000000  32.000000  32.000000  27.000000   ...      39.297297   \n9055   38.208333  37.000000  35.000000  31.000000   ...      49.000000   \n26355  45.000000  42.000000  39.000000  39.000000   ...      50.473684   \n11599  59.000000  59.000000  57.965517  56.000000   ...      79.000000   \n1992   25.000000  25.000000  23.846154  23.000000   ...      52.000000   \n4467   77.548387  73.048780  70.818182  67.395349   ...      76.949802   \n25365   6.312500   2.264151   0.000000   0.000000   ...      18.000000   \n27958   0.000000   0.000000   0.000000   0.000000   ...       1.000000   \n8573   43.000000  43.000000  43.000000  41.062500   ...      35.751938   \n1063   92.000000  92.000000  90.000000  89.870324   ...      91.000000   \n17917  43.000000  41.214286  41.000000  39.000000   ...      44.000000   \n23760  10.729167   8.583333   6.437500   4.291667   ...      22.687500   \n17140  83.000000  79.000000  74.555556  74.000000   ...      97.435897   \n25759  67.500000  65.333333  60.000000  60.000000   ...      86.000000   \n24193  20.854167  18.958333  17.062500  15.166667   ...      34.354167   \n28244  68.250000  66.625000  65.000000  63.375000   ...     112.000000   \n964    70.000000  70.000000  68.938889  68.772222   ...      73.000000   \n16206  88.000000  84.306122  82.629630  79.148148   ...      97.000000   \n29535   9.000000   8.270270   6.000000   6.000000   ...      24.792120   \n\n       score_diff_amh  home_team_spread  pct_complete  overunder  probability  \\\n8966       -11.703704             -7.50     58.333333     212.50     0.810699   \n21948       -3.567568             -4.50     42.708333     220.00     0.754400   \n9055        -7.741514             -3.50     50.000000     193.00     0.792305   \n26355      -10.526316            -15.00     59.375000     206.00     0.849684   \n11599        3.714286             -2.00     70.833333     195.50     0.582832   \n1992        15.000000              5.00     46.875000     193.50     0.058643   \n4467        -8.050198             -5.00     77.083333     206.00     0.934172   \n25365       -3.750000             -5.50     15.625000     207.00     0.760524   \n27958       -2.000000              5.50      2.083333     217.00     0.524831   \n8573       -15.000000             -6.25     60.416667     205.25     0.999713   \n1063       -11.272727              2.50     94.791667     208.50     1.000000   \n17917       -8.940278            -14.50     51.041667     208.00     0.680142   \n23760       -0.916667              3.00     22.916667     209.50     0.764824   \n17140        4.435897             -7.00     84.375000     210.50     0.295861   \n25759        3.064516              6.00     72.916667     198.00     0.284988   \n24193        2.125000              6.50     35.416667     202.00     0.285590   \n28244       34.000000              6.50    100.000000     210.00     0.000000   \n964        -12.052632             -9.50     93.750000     200.00     0.999263   \n16206       -4.000000             -3.00    100.000000     205.00     1.000000   \n29535        4.792120              3.50     26.041667     208.00     0.285136   \n\n       label  prediction  correct  quarter  \n8966       0           1       no       Q3  \n21948      1           1      yes       Q2  \n9055       1           1      yes       Q2  \n26355      1           1      yes       Q3  \n11599      0           1       no       Q3  \n1992       1           0       no       Q2  \n4467       1           1      yes       Q4  \n25365      1           1      yes       Q1  \n27958      0           1       no       Q1  \n8573       1           1      yes       Q3  \n1063       1           1      yes       Q4  \n17917      1           1      yes       Q3  \n23760      1           1      yes       Q1  \n17140      1           0       no       Q4  \n25759      1           0       no       Q3  \n24193      0           0      yes       Q2  \n28244      0           0      yes       Q4  \n964        1           1      yes       Q4  \n16206      1           1      yes       Q4  \n29535      0           0      yes       Q2  \n\n[20 rows x 61 columns]", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_dly_0</th>\n      <th>min_dly_1</th>\n      <th>min_dly_2</th>\n      <th>min_dly_3</th>\n      <th>min_dly_4</th>\n      <th>min_dly_5</th>\n      <th>min_dly_6</th>\n      <th>min_dly_7</th>\n      <th>min_dly_8</th>\n      <th>min_dly_9</th>\n      <th>...</th>\n      <th>away_score</th>\n      <th>score_diff_amh</th>\n      <th>home_team_spread</th>\n      <th>pct_complete</th>\n      <th>overunder</th>\n      <th>probability</th>\n      <th>label</th>\n      <th>prediction</th>\n      <th>correct</th>\n      <th>quarter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8966</th>\n      <td>64.703704</td>\n      <td>61.000000</td>\n      <td>61.000000</td>\n      <td>58.000000</td>\n      <td>57.918919</td>\n      <td>48.935484</td>\n      <td>46.878788</td>\n      <td>45.000000</td>\n      <td>43.250000</td>\n      <td>41.000000</td>\n      <td>...</td>\n      <td>53.000000</td>\n      <td>-11.703704</td>\n      <td>-7.50</td>\n      <td>58.333333</td>\n      <td>212.50</td>\n      <td>0.810699</td>\n      <td>0</td>\n      <td>1</td>\n      <td>no</td>\n      <td>Q3</td>\n    </tr>\n    <tr>\n      <th>21948</th>\n      <td>42.864865</td>\n      <td>42.000000</td>\n      <td>38.871795</td>\n      <td>36.774194</td>\n      <td>34.947368</td>\n      <td>34.000000</td>\n      <td>34.000000</td>\n      <td>32.000000</td>\n      <td>32.000000</td>\n      <td>27.000000</td>\n      <td>...</td>\n      <td>39.297297</td>\n      <td>-3.567568</td>\n      <td>-4.50</td>\n      <td>42.708333</td>\n      <td>220.00</td>\n      <td>0.754400</td>\n      <td>1</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>Q2</td>\n    </tr>\n    <tr>\n      <th>9055</th>\n      <td>56.741514</td>\n      <td>47.371429</td>\n      <td>45.000000</td>\n      <td>44.142857</td>\n      <td>43.000000</td>\n      <td>39.000000</td>\n      <td>38.208333</td>\n      <td>37.000000</td>\n      <td>35.000000</td>\n      <td>31.000000</td>\n      <td>...</td>\n      <td>49.000000</td>\n      <td>-7.741514</td>\n      <td>-3.50</td>\n      <td>50.000000</td>\n      <td>193.00</td>\n      <td>0.792305</td>\n      <td>1</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>Q2</td>\n    </tr>\n    <tr>\n      <th>26355</th>\n      <td>61.000000</td>\n      <td>61.000000</td>\n      <td>58.120000</td>\n      <td>54.157895</td>\n      <td>50.000000</td>\n      <td>49.351852</td>\n      <td>45.000000</td>\n      <td>42.000000</td>\n      <td>39.000000</td>\n      <td>39.000000</td>\n      <td>...</td>\n      <td>50.473684</td>\n      <td>-10.526316</td>\n      <td>-15.00</td>\n      <td>59.375000</td>\n      <td>206.00</td>\n      <td>0.849684</td>\n      <td>1</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>Q3</td>\n    </tr>\n    <tr>\n      <th>11599</th>\n      <td>75.285714</td>\n      <td>73.000000</td>\n      <td>69.783784</td>\n      <td>68.000000</td>\n      <td>65.562500</td>\n      <td>62.411765</td>\n      <td>59.000000</td>\n      <td>59.000000</td>\n      <td>57.965517</td>\n      <td>56.000000</td>\n      <td>...</td>\n      <td>79.000000</td>\n      <td>3.714286</td>\n      <td>-2.00</td>\n      <td>70.833333</td>\n      <td>195.50</td>\n      <td>0.582832</td>\n      <td>0</td>\n      <td>1</td>\n      <td>no</td>\n      <td>Q3</td>\n    </tr>\n    <tr>\n      <th>1992</th>\n      <td>37.000000</td>\n      <td>37.000000</td>\n      <td>36.500000</td>\n      <td>33.142857</td>\n      <td>31.162162</td>\n      <td>29.000000</td>\n      <td>25.000000</td>\n      <td>25.000000</td>\n      <td>23.846154</td>\n      <td>23.000000</td>\n      <td>...</td>\n      <td>52.000000</td>\n      <td>15.000000</td>\n      <td>5.00</td>\n      <td>46.875000</td>\n      <td>193.50</td>\n      <td>0.058643</td>\n      <td>1</td>\n      <td>0</td>\n      <td>no</td>\n      <td>Q2</td>\n    </tr>\n    <tr>\n      <th>4467</th>\n      <td>85.000000</td>\n      <td>85.000000</td>\n      <td>81.400000</td>\n      <td>79.000000</td>\n      <td>79.000000</td>\n      <td>79.000000</td>\n      <td>77.548387</td>\n      <td>73.048780</td>\n      <td>70.818182</td>\n      <td>67.395349</td>\n      <td>...</td>\n      <td>76.949802</td>\n      <td>-8.050198</td>\n      <td>-5.00</td>\n      <td>77.083333</td>\n      <td>206.00</td>\n      <td>0.934172</td>\n      <td>1</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>Q4</td>\n    </tr>\n    <tr>\n      <th>25365</th>\n      <td>21.750000</td>\n      <td>18.720000</td>\n      <td>15.000000</td>\n      <td>15.000000</td>\n      <td>13.000000</td>\n      <td>8.588235</td>\n      <td>6.312500</td>\n      <td>2.264151</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>18.000000</td>\n      <td>-3.750000</td>\n      <td>-5.50</td>\n      <td>15.625000</td>\n      <td>207.00</td>\n      <td>0.760524</td>\n      <td>1</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>Q1</td>\n    </tr>\n    <tr>\n      <th>27958</th>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>-2.000000</td>\n      <td>5.50</td>\n      <td>2.083333</td>\n      <td>217.00</td>\n      <td>0.524831</td>\n      <td>0</td>\n      <td>1</td>\n      <td>no</td>\n      <td>Q1</td>\n    </tr>\n    <tr>\n      <th>8573</th>\n      <td>50.751938</td>\n      <td>49.201550</td>\n      <td>47.651163</td>\n      <td>46.100775</td>\n      <td>44.550388</td>\n      <td>43.000000</td>\n      <td>43.000000</td>\n      <td>43.000000</td>\n      <td>43.000000</td>\n      <td>41.062500</td>\n      <td>...</td>\n      <td>35.751938</td>\n      <td>-15.000000</td>\n      <td>-6.25</td>\n      <td>60.416667</td>\n      <td>205.25</td>\n      <td>0.999713</td>\n      <td>1</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>Q3</td>\n    </tr>\n    <tr>\n      <th>1063</th>\n      <td>102.272727</td>\n      <td>101.000000</td>\n      <td>99.000000</td>\n      <td>97.000000</td>\n      <td>95.000000</td>\n      <td>93.000000</td>\n      <td>92.000000</td>\n      <td>92.000000</td>\n      <td>90.000000</td>\n      <td>89.870324</td>\n      <td>...</td>\n      <td>91.000000</td>\n      <td>-11.272727</td>\n      <td>2.50</td>\n      <td>94.791667</td>\n      <td>208.50</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>Q4</td>\n    </tr>\n    <tr>\n      <th>17917</th>\n      <td>52.940278</td>\n      <td>52.000000</td>\n      <td>47.867925</td>\n      <td>45.000000</td>\n      <td>45.000000</td>\n      <td>43.000000</td>\n      <td>43.000000</td>\n      <td>41.214286</td>\n      <td>41.000000</td>\n      <td>39.000000</td>\n      <td>...</td>\n      <td>44.000000</td>\n      <td>-8.940278</td>\n      <td>-14.50</td>\n      <td>51.041667</td>\n      <td>208.00</td>\n      <td>0.680142</td>\n      <td>1</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>Q3</td>\n    </tr>\n    <tr>\n      <th>23760</th>\n      <td>23.604167</td>\n      <td>21.458333</td>\n      <td>19.312500</td>\n      <td>17.166667</td>\n      <td>15.020833</td>\n      <td>12.875000</td>\n      <td>10.729167</td>\n      <td>8.583333</td>\n      <td>6.437500</td>\n      <td>4.291667</td>\n      <td>...</td>\n      <td>22.687500</td>\n      <td>-0.916667</td>\n      <td>3.00</td>\n      <td>22.916667</td>\n      <td>209.50</td>\n      <td>0.764824</td>\n      <td>1</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>Q1</td>\n    </tr>\n    <tr>\n      <th>17140</th>\n      <td>93.000000</td>\n      <td>92.650794</td>\n      <td>91.000000</td>\n      <td>91.000000</td>\n      <td>88.960784</td>\n      <td>83.193548</td>\n      <td>83.000000</td>\n      <td>79.000000</td>\n      <td>74.555556</td>\n      <td>74.000000</td>\n      <td>...</td>\n      <td>97.435897</td>\n      <td>4.435897</td>\n      <td>-7.00</td>\n      <td>84.375000</td>\n      <td>210.50</td>\n      <td>0.295861</td>\n      <td>1</td>\n      <td>0</td>\n      <td>no</td>\n      <td>Q4</td>\n    </tr>\n    <tr>\n      <th>25759</th>\n      <td>82.935484</td>\n      <td>79.125000</td>\n      <td>77.629630</td>\n      <td>76.000000</td>\n      <td>70.000000</td>\n      <td>68.000000</td>\n      <td>67.500000</td>\n      <td>65.333333</td>\n      <td>60.000000</td>\n      <td>60.000000</td>\n      <td>...</td>\n      <td>86.000000</td>\n      <td>3.064516</td>\n      <td>6.00</td>\n      <td>72.916667</td>\n      <td>198.00</td>\n      <td>0.284988</td>\n      <td>1</td>\n      <td>0</td>\n      <td>no</td>\n      <td>Q3</td>\n    </tr>\n    <tr>\n      <th>24193</th>\n      <td>32.229167</td>\n      <td>30.333333</td>\n      <td>28.437500</td>\n      <td>26.541667</td>\n      <td>24.645833</td>\n      <td>22.750000</td>\n      <td>20.854167</td>\n      <td>18.958333</td>\n      <td>17.062500</td>\n      <td>15.166667</td>\n      <td>...</td>\n      <td>34.354167</td>\n      <td>2.125000</td>\n      <td>6.50</td>\n      <td>35.416667</td>\n      <td>202.00</td>\n      <td>0.285590</td>\n      <td>0</td>\n      <td>0</td>\n      <td>yes</td>\n      <td>Q2</td>\n    </tr>\n    <tr>\n      <th>28244</th>\n      <td>78.000000</td>\n      <td>76.375000</td>\n      <td>74.750000</td>\n      <td>73.125000</td>\n      <td>71.500000</td>\n      <td>69.875000</td>\n      <td>68.250000</td>\n      <td>66.625000</td>\n      <td>65.000000</td>\n      <td>63.375000</td>\n      <td>...</td>\n      <td>112.000000</td>\n      <td>34.000000</td>\n      <td>6.50</td>\n      <td>100.000000</td>\n      <td>210.00</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>yes</td>\n      <td>Q4</td>\n    </tr>\n    <tr>\n      <th>964</th>\n      <td>85.052632</td>\n      <td>79.000000</td>\n      <td>78.000000</td>\n      <td>75.000000</td>\n      <td>75.000000</td>\n      <td>72.454545</td>\n      <td>70.000000</td>\n      <td>70.000000</td>\n      <td>68.938889</td>\n      <td>68.772222</td>\n      <td>...</td>\n      <td>73.000000</td>\n      <td>-12.052632</td>\n      <td>-9.50</td>\n      <td>93.750000</td>\n      <td>200.00</td>\n      <td>0.999263</td>\n      <td>1</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>Q4</td>\n    </tr>\n    <tr>\n      <th>16206</th>\n      <td>101.000000</td>\n      <td>101.000000</td>\n      <td>97.000000</td>\n      <td>94.285714</td>\n      <td>93.470588</td>\n      <td>90.000000</td>\n      <td>88.000000</td>\n      <td>84.306122</td>\n      <td>82.629630</td>\n      <td>79.148148</td>\n      <td>...</td>\n      <td>97.000000</td>\n      <td>-4.000000</td>\n      <td>-3.00</td>\n      <td>100.000000</td>\n      <td>205.00</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>yes</td>\n      <td>Q4</td>\n    </tr>\n    <tr>\n      <th>29535</th>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>18.710280</td>\n      <td>17.028037</td>\n      <td>12.280000</td>\n      <td>9.000000</td>\n      <td>9.000000</td>\n      <td>8.270270</td>\n      <td>6.000000</td>\n      <td>6.000000</td>\n      <td>...</td>\n      <td>24.792120</td>\n      <td>4.792120</td>\n      <td>3.50</td>\n      <td>26.041667</td>\n      <td>208.00</td>\n      <td>0.285136</td>\n      <td>0</td>\n      <td>0</td>\n      <td>yes</td>\n      <td>Q2</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows \u00d7 61 columns</p>\n</div>"
                    }
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 92, 
            "source": "import brunel\n%brunel data('vis_pd') x(quarter) y(correct) bin(correct) color(#count) label(#count) style('symbol:rect; border-radius:15')", 
            "metadata": {}, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>", 
                        "text/html": "<!--\n  ~ Copyright (c) 2015 IBM Corporation and others.\n  ~\n  ~ Licensed under the Apache License, Version 2.0 (the \"License\");\n  ~ You may not use this file except in compliance with the License.\n  ~ You may obtain a copy of the License at\n  ~\n  ~     http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing, software\n  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  ~ See the License for the specific language governing permissions and\n  ~ limitations under the License.\n  -->\n\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/6fa8e819-8549-42d4-ada7-5a2802822690/nbextensions/brunel_ext/brunel.2.3.css\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/6fa8e819-8549-42d4-ada7-5a2802822690/nbextensions/brunel_ext/sumoselect.css\">\n\n<style>\n    #visid8c5a2130-d586-11e7-8fc1-002590fb6dc0.brunel .chart1 .element1 .element {\n\tborder-radius: 15;\n\tsymbol: rect;\n}\n</style>\n\n<div id=\"controlsid8c5a2554-d586-11e7-8fc1-002590fb6dc0\" class=\"brunel\"/>\n<svg id=\"visid8c5a2130-d586-11e7-8fc1-002590fb6dc0\" width=\"500\" height=\"400\"></svg>"
                    }
                }, 
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 92, 
                    "data": {
                        "text/plain": "<IPython.core.display.Javascript object>", 
                        "application/javascript": "/*\n * Copyright (c) 2015 IBM Corporation and others.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * You may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nrequire.config({\n    waitSeconds: 60,\n    paths: {\n        'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n        'topojson': '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n        'brunel' : '/data/jupyter2/6fa8e819-8549-42d4-ada7-5a2802822690/nbextensions/brunel_ext/brunel.2.3.min',\n        'brunelControls' : '/data/jupyter2/6fa8e819-8549-42d4-ada7-5a2802822690/nbextensions/brunel_ext/brunel.controls.2.3.min'\n    },\n    shim: {\n       'brunel' : {\n            exports: 'BrunelD3',\n            deps: ['d3', 'topojson'],\n            init: function() {\n               return {\n                 BrunelD3 : BrunelD3,\n                 BrunelData : BrunelData\n              }\n            }\n        },\n       'brunelControls' : {\n            exports: 'BrunelEventHandlers',\n            init: function() {\n               return {\n                 BrunelEventHandlers: BrunelEventHandlers,\n                 BrunelJQueryControlFactory: BrunelJQueryControlFactory\n              }\n            }\n        }\n\n    }\n\n});\n\nrequire([\"d3\"], function(d3) {\n    require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n        function  BrunelVis(visId) {\n  \"use strict\";                                                                       // strict mode\n  var datasets = [],                                      // array of datasets for the original data\n      pre = function(d, i) { return d },                         // default pre-process does nothing\n      post = function(d, i) { return d },                       // default post-process does nothing\n      transitionTime = 200,                                        // transition time for animations\n      charts = [],                                                       // the charts in the system\n      vis = d3.select('#' + visId).attr('class', 'brunel');                     // the SVG container\n\n  BrunelD3.addDefinitions(vis);                                   // ensure standard symbols present\n\n  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n\n  charts[0] = function(parentNode, filterRows) {\n    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 5, 43, 37, 66),\n      elements = [];                                              // array of elements in this chart\n\n    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n\n    var chart =  vis.append('g').attr('class', 'chart1')\n      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n    var overlay = chart.append('g').attr('class', 'element').attr('class', 'overlay');\n    var zoom = d3.zoom().scaleExtent([1/3,3]);\n    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n      .style('cursor', 'default')\n      .node();\n    zoomNode.__zoom = d3.zoomIdentity;\n    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n    var interior = chart.append('g').attr('class', 'interior zoomNone')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n      .attr('clip-path', 'url(#clip_visid8c5a2130-d586-11e7-8fc1-002590fb6dc0_chart1_inner)');\n    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n    var gridGroup = interior.append('g').attr('class', 'grid');\n    var axes = chart.append('g').attr('class', 'axis')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')');\n    var legends = chart.append('g').attr('class', 'legend')\n      .attr('transform','translate(' + (geom.chart_right-geom.chart_left - 3) + ',' + 0 + ')');\n    vis.append('clipPath').attr('id', 'clip_visid8c5a2130-d586-11e7-8fc1-002590fb6dc0_chart1_inner').append('rect')\n      .attr('x', 0).attr('y', 0)\n      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n\n    // Scales //////////////////////////////////////////////////////////////////////////////////////\n\n    var scale_x = d3.scalePoint().padding(0.5)\n      .domain(['Q1', 'Q2', 'Q3', 'Q4'])\n      .range([0, geom.inner_width]);\n    var scale_inner = d3.scaleLinear().domain([0,1])\n      .range([-0.5, 0.5]);\n    var scale_y = d3.scalePoint().padding(0.5)\n      .domain(['yes', 'no'])\n      .range([geom.inner_height, 0]);\n    var base_scales = [scale_x, scale_y];                           // untransformed original scales\n\n    // Axes ////////////////////////////////////////////////////////////////////////////////////////\n\n    axes.append('g').attr('class', 'x axis')\n      .attr('transform','translate(0,' + geom.inner_rawHeight + ')')\n      .attr('clip-path', 'url(#clip_visid8c5a2130-d586-11e7-8fc1-002590fb6dc0_chart1_haxis)');\n    vis.append('clipPath').attr('id', 'clip_visid8c5a2130-d586-11e7-8fc1-002590fb6dc0_chart1_haxis').append('polyline')\n      .attr('points', '-1,-1000, -1,-1 -5,5, -1000,5, -100,1000, 10000,1000 10000,-1000');\n    axes.select('g.axis.x').append('text').attr('class', 'title').text('Quarter').style('text-anchor', 'middle')\n      .attr('x',geom.inner_rawWidth/2)\n      .attr('y', geom.inner_bottom - 2.0).attr('dy','-0.27em');\n    axes.append('g').attr('class', 'y axis')\n      .attr('clip-path', 'url(#clip_visid8c5a2130-d586-11e7-8fc1-002590fb6dc0_chart1_vaxis)');\n    vis.append('clipPath').attr('id', 'clip_visid8c5a2130-d586-11e7-8fc1-002590fb6dc0_chart1_vaxis').append('polyline')\n      .attr('points', '-1000,-10000, 10000,-10000, 10000,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+5) + ', -1000,' + (geom.inner_rawHeight+5) );\n    axes.select('g.axis.y').append('text').attr('class', 'title').text('Correct').style('text-anchor', 'middle')\n      .attr('x',-geom.inner_rawHeight/2)\n      .attr('y', 4-geom.inner_left).attr('dy', '0.7em').attr('transform', 'rotate(270)');\n\n    var axis_bottom = d3.axisBottom(scale_x).ticks(Math.min(10, Math.round(geom.inner_width / 25.5)));\n    var axis_left = d3.axisLeft(scale_y).ticks(Math.min(10, Math.round(geom.inner_width / 20)));\n\n    function buildAxes(time) {\n      axis_bottom.tickValues(BrunelD3.filterTicks(scale_x))\n      var axis_x = axes.select('g.axis.x');\n      BrunelD3.transition(axis_x, time).call(axis_bottom.scale(scale_x));\n      axis_left.tickValues(BrunelD3.filterTicks(scale_y))\n      var axis_y = axes.select('g.axis.y');\n      BrunelD3.transition(axis_y, time).call(axis_left.scale(scale_y));\n    }\n    zoom.on('zoom', function(t, time) {\n        t = t ||BrunelD3.restrictZoom(d3.event.transform, geom, this);\n        zoomNode.__zoom = t;\n        interior.attr('class', 'interior ' + BrunelD3.zoomLabel(t.k));;\n        build(time || -1);\n    });\n\n    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n\n    elements[0] = function() {\n      var original, processed,                           // data sets passed in and then transformed\n        element, data,                                 // brunel element information and brunel data\n        selection, merged;                                      // d3 selection and merged selection\n      var elementGroup = interior.append('g').attr('class', 'element1'),\n        main = elementGroup.append('g').attr('class', 'main'),\n        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n\n      function makeData() {\n        original = datasets[0];\n        if (filterRows) original = original.retainRows(filterRows);\n        processed = pre(original, 0)\n          .transform('correct=bin')\n          .summarize('#count=#count:sum; quarter=quarter:base; correct=correct');\n        processed = post(processed, 0);\n        var f0 = processed.field('quarter'),\n          f1 = processed.field('correct'),\n          f2 = processed.field('#count'),\n          f3 = processed.field('#row'),\n          f4 = processed.field('#selection');\n        var keyFunc = function(d) { return f0.value(d)+ '|' + f1.value(d) };\n        data = {\n          quarter:      function(d) { return f0.value(d.row) },\n          correct:      function(d) { return f1.value(d.row) },\n          $count:       function(d) { return f2.value(d.row) },\n          $row:         function(d) { return f3.value(d.row) },\n          $selection:   function(d) { return f4.value(d.row) },\n          quarter_f:    function(d) { return f0.valueFormatted(d.row) },\n          correct_f:    function(d) { return f1.valueFormatted(d.row) },\n          $count_f:     function(d) { return f2.valueFormatted(d.row) },\n          $row_f:       function(d) { return f3.valueFormatted(d.row) },\n          $selection_f: function(d) { return f4.valueFormatted(d.row) },\n          _split:       function(d) { return f2.value(d.row) },\n          _key:         keyFunc,\n          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n        };\n      }\n      // Aesthetic Functions\n      var scale_color = d3.scaleLinear().domain([140, 680.5, 1221, 1761.5, 2302])\n        .interpolate(d3.interpolateHcl)\n        .range([ '#f1eef6', '#bdc9e1', '#74a9cf', '#2b8cbe', '#045a8d']);\n      var color = function(d) { return scale_color(data.$count(d)) };\n      legends._legend = legends._legend || { title: ['Count'], \n        ticks: [2500, 2000, 1500, 1000, 500, 0]};\n      legends._legend.color = scale_color;\n\n      // Build element from data ///////////////////////////////////////////////////////////////////\n\n      function build(transitionMillis) {\n        element = elements[0];\n        var w = 0.9 * Math.abs(scale_x(scale_x.domain()[1]) - scale_x(scale_x.domain()[0]) );\n        var x = function(d) { return scale_x(data.quarter(d))};\n        var h = 0.9 * Math.abs(scale_y(scale_y.domain()[1]) - scale_y(scale_y.domain()[0]) );\n        var y = function(d) { return scale_y(data.correct(d))};\n        var labeling  = [{\n          index: 0, method: 'box', location: ['center', 'center'], inside: true, align: 'middle', pad: 3, dy: 0.3,\n          fit: true, granularity: 1,\n          content: function(d) {\n            return d.row == null ? null : data.$count_f(d)\n          }\n        }];\n\n        // Define selection entry operations\n        function initialState(selection) {\n          selection\n            .attr('class', 'element point filled')\n            .attr('rx',15.0).attr('ry', 15.0)\n            .style('pointer-events', 'none')\n        }\n\n        // Define selection update operations on merged data\n        function updateState(selection) {\n          selection\n            .each(function(d) {\n              var width = w, left = x(d) - width/2, \n              height = h, top = y(d) - height/2;\n              this.r = {x:left, y:top, w:width, h:height};\n            })\n            .attr('x', function(d) { return this.r.x })\n            .attr('y', function(d) { return this.r.y })\n            .attr('width', function(d) { return this.r.w })\n            .attr('height', function(d) { return this.r.h })\n            .filter(BrunelD3.hasData)                     // following only performed for data items\n            .style('fill', color);\n        }\n\n        // Define labeling for the selection\n        function label(selection, transitionMillis) {\n          BrunelD3.label(selection, labels, transitionMillis, geom, labeling);\n        }\n        // Create selections, set the initial state and transition updates\n        selection = main.selectAll('.element').data(data._rows, function(d) { return d.key });\n        var added = selection.enter().append('rect');\n        merged = selection.merge(added);\n        initialState(added);\n        selection.filter(BrunelD3.hasData)\n          .classed('selected', BrunelD3.isSelected(data))\n          .filter(BrunelD3.isSelected(data)).raise();\n        updateState(BrunelD3.transition(merged, transitionMillis));\n        label(merged, transitionMillis);\n\n        BrunelD3.transition(selection.exit(), transitionMillis/3)\n          .style('opacity', 0.5).each( function() {\n            this.remove(); BrunelD3.removeLabels(this); \n        });\n      }\n\n      return {\n        data:           function() { return processed },\n        original:       function() { return original },\n        internal:       function() { return data },\n        selection:      function() { return merged },\n        makeData:       makeData,\n        build:          build,\n        chart:          function() { return charts[0] },\n        group:          function() { return elementGroup },\n        fields: {\n          x:            ['quarter'],\n          y:            ['correct'],\n          key:          ['quarter', 'correct'],\n          color:        ['#count']\n        }\n      };\n    }();\n\n    function build(time, noData) {\n      var first = elements[0].data() == null;\n      if (first) time = 0;                                           // no transition for first call\n      buildAxes(time);\n      if ((first || time > -1) && !noData) {\n        elements[0].makeData();\n        BrunelD3.addLegend(legends, legends._legend);\n      }\n      elements[0].build(time);\n    }\n\n    // Expose the following components of the chart\n    return {\n      elements : elements,\n      interior : interior,\n      scales: {x:scale_x, y:scale_y},\n      zoom: function(params, time) {\n          if (params) zoom.on('zoom').call(zoomNode, params, time);\n          return d3.zoomTransform(zoomNode);\n      },\n      build : build\n    };\n    }();\n\n  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n  function buildAll() {\n    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n    updateAll(transitionTime);\n  }\n\n  return {\n    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n    dataPostProcess:    function(f) { if (f) post = f; return post },\n    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n    visId:              visId,\n    build:              buildAll,\n    rebuild:            updateAll,\n    charts:             charts\n  }\n}\n\n// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n\nvar table1 = {\n   summarized: true,\n   names: ['quarter', 'correct', '#count'], \n   options: ['string', 'string', 'numeric'], \n   rows: [['Q1', 'no', 672], ['Q2', 'no', 553], ['Q3', 'no', 350], ['Q4', 'no', 140],\n  ['Q1', 'yes', 1784], ['Q2', 'yes', 1688], ['Q3', 'yes', 1771], ['Q4', 'yes', 2302]]\n};\n\n// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n\nvar v  = new BrunelVis('visid8c5a2130-d586-11e7-8fc1-002590fb6dc0');\nv.build(table1);\n\n    });\n});"
                    }
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "source": "#    Q3\n# DL  16.5%\n# Log 20.8", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### Use DNN - Tflow  (sample .. not tested)", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 6, 
            "source": "#Vector Assembler\n#feature_cols  = [\"home_score\", \"away_score\", \"score_diff_amh\", \"home_team_spread\",\"pct_complete\", \"cf1\", \"cf2\"]\nfeature_cols = [\"home_score\", \"away_score\", \"score_diff_amh\", \"home_team_spread\",\"pct_complete\"]\n\n", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": 45, 
            "source": "!uname -a\n", 
            "metadata": {}, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Linux yp-spark-dal09-env5-0044 3.10.0-693.5.2.el7.x86_64 #1 SMP Fri Oct 20 20:32:50 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n"
                }
            ], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "source": "## import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport math\n\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nimport numpy as np\n\ndef load_csv(filename):\n    file = pd.read_csv(filename, header=0)\n\n    # get sample's metadata\n    n_samples = int(file.columns[0])\n    n_features = int(file.columns[1])\n\n    # divide samples into explanation variables and target variable\n    data = np.empty((n_samples, n_features))\n    target = np.empty((n_samples,), dtype=np.int)\n    for i, row in enumerate(file.itertuples()):\n        target[i] = np.asarray(row[-1], dtype=np.int)\n        data[i] = np.asarray(row[1:n_features+1], dtype=np.float64)\n    return (data, target)\n\n# output train data \ndef get_batch_data(x_train, y_train, size=None):\n    if size is None:\n        size = len(x_train)\n    batch_xs = x_train\n    batch_ys = []\n\n    # convert to 1-of-N vector\n    for i in range(len(y_train)):\n        val = np.zeros((CLASS_SIZE), dtype=np.float64)\n        val[y_train[i]] = 1.0\n        batch_ys.append(val)\n    batch_ys = np.asarray(batch_ys)\n    return batch_xs[:size], batch_ys[:size]\n\n# output test data\ndef get_test_data(x_test, y_test):\n    batch_ys = []\n\n    # convert to 1-of-N vector\n    for i in range(len(y_test)):\n        val = np.zeros((CLASS_SIZE), dtype=np.float64)\n        val[y_test[i]] = 1.0\n        batch_ys.append(val)\n    return x_test, np.asarray(batch_ys)\n\n# for parameter initialize\ndef get_stddev(in_dim, out_dim):\n    return 1.3 / math.sqrt(float(in_dim) + float(out_dim))\n\n# DNN Model Class\nclass Classifier:\n    def __init__(self, hidden_units=[10], n_classes=0, data_size = 0):\n        self._hidden_units = hidden_units\n        self._n_classes = n_classes\n        self._data_size = data_size\n        self._sess = tf.Session()\n\n    # build model\n    def inference(self, x):\n        hidden = []\n\n        # Input Layer\n        with tf.name_scope(\"input\"):\n            weights = tf.Variable(tf.truncated_normal([DATA_SIZE, self._hidden_units[0]], stddev=get_stddev(DATA_SIZE, self._hidden_units[0]), seed=42), name='weights')\n            biases = tf.Variable(tf.zeros([self._hidden_units[0]]), name='biases')\n            input = tf.matmul(x, weights) + biases\n\n        # Hidden Layers\n        for index, num_hidden in enumerate(self._hidden_units):\n            if index == len(self._hidden_units) - 1: break\n            with tf.name_scope(\"hidden{}\".format(index+1)):\n                weights = tf.Variable(tf.truncated_normal([num_hidden, self._hidden_units[index+1]], seed=42, stddev=get_stddev(num_hidden, self._hidden_units[index+1])), name='weights')\n                biases = tf.Variable(tf.zeros([self._hidden_units[index+1]]), name='biases')\n                inputs = input if index == 0 else hidden[index-1]\n                hidden.append(tf.nn.relu(tf.matmul(inputs, weights) + biases, name=\"hidden{}\".format(index+1)))\n        \n        # Output Layer\n        with tf.name_scope('output'):\n            weights = tf.Variable(tf.truncated_normal([self._hidden_units[-1], self._n_classes], seed=42, stddev=get_stddev(self._hidden_units[-1], self._n_classes)), name='weights')\n            biases = tf.Variable(tf.zeros([self._n_classes]), name='biases')\n            logits = tf.nn.softmax(tf.matmul(hidden[-1], weights) + biases)\n\n        return logits\n\n    # loss function\n    def loss(self, logits, y):        \n        #return -tf.reduce_mean(y * tf.log(logits))\n        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n\n    # fitting function for train data\n    def fit(self, x_train=None, y_train=None, steps=200):\n        # build model\n        x = tf.placeholder(tf.float32, [None, DATA_SIZE])\n        y = tf.placeholder(tf.float32, [None, CLASS_SIZE])\n        logits = self.inference(x)\n        loss = self.loss(logits, y)\n        train_op = tf.train.AdamOptimizer(0.003).minimize(loss)\n\n        # save variables\n        self._x = x\n        self._y = y\n        self._logits = logits\n \n        # init parameters\n        #init = tf.initialize_all_variables() \n        init = tf.global_variables_initializer()\n        self._sess.run(init)\n\n        # train\n        for i in range(steps):\n            batch_xs, batch_ys = get_batch_data(x_train, y_train)\n            self._sess.run(train_op, feed_dict={x: batch_xs, y: batch_ys})\n\n    # evaluation function for test data\n    def evaluate(self, x_test=None, y_test=None):\n        x_test, y_test = get_test_data(x_test, y_test)\n        \n        # build accuracy calculate step\n        correct_prediction = tf.equal(tf.argmax(self._logits, 1), tf.argmax(self._y, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        # evaluate\n        return self._sess.run([accuracy], feed_dict={self._x: x_test, self._y: y_test})\n\n    # label pridiction\n    def predict(self, samples):\n        predictions = tf.argmax(self._logits, 1)\n        return self._sess.run(predictions, {self._x: samples})", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code"
        }
    ]
}