{"paragraphs":[{"title":"Imports","text":"import org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\nimport com.databricks.spark.csv\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,DoubleType,DateType,TimestampType};\nimport scala.util.matching.Regex\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types.{StructType,StructField,StringType,DoubleType};\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions.{udf,col,concat,lit,when,ceil}\nimport org.apache.spark.sql._\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.feature.StandardScaler\n","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466994518290_384976013","id":"20160627-022838_1842099086","dateCreated":"Jun 27, 2016 2:28:38 AM","dateStarted":"Aug 5, 2016 11:59:21 PM","dateFinished":"Aug 5, 2016 11:59:53 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2243"},{"title":"Use A Custom Schema To Control Types","text":"// This line reads in the file and parses it with a CSV reader\nval sqlContext = new SQLContext(sc)\n// For some reason, the reader below doesn't infer the types properly (all strings) .  Forcing the types here\nval customSchema = StructType(Array(\n    StructField(\"scorea\", IntegerType, true),\n    StructField(\"scoreb\", IntegerType, true),\n    StructField(\"timeleft\", DoubleType, true),\n    StructField(\"teama\", StringType, true),\n    StructField(\"teamb\", StringType, true),\n    StructField(\"scorea-scoreb\", IntegerType, true),\n    StructField(\"scoreb-scorea\", IntegerType, true),\n    StructField(\"pct-complete\", DoubleType, true),\n    StructField(\"pct-left\", DoubleType, true),\n    StructField(\"cf1\", DoubleType, true),\n    StructField(\"cf2\", DoubleType, true),\n    StructField(\"teamaspread\", DoubleType, true),\n    StructField(\"overunder\", DoubleType, true),\n    StructField(\"teambspread\", DoubleType, true),\n    StructField(\"teama_vegas_fscore\", DoubleType, true),\n    StructField(\"teamb_vegas_fscore\", DoubleType, true),\n    StructField(\"key\", StringType, true),\n    StructField(\"fscorea\", DoubleType, true),\n    StructField(\"fscoreb\", DoubleType, true),\n    StructField(\"fscorea-fscoreb\", IntegerType, true),\n    StructField(\"fscoreb-fscorea\", IntegerType, true),\n    StructField(\"away-win\", DoubleType, true),\n    StructField(\"home-win\", DoubleType, true),\n    StructField(\"teama_adj_fscore\", DoubleType, true),\n    StructField(\"teamb_adj_fscore\", DoubleType, true),\n    StructField(\"pfscoreb-pfscorea\", DoubleType, true)\n))\n\n\n","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorHide":true,"tableHide":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468028008919_-455574302","id":"20160709-013328_197950034","dateCreated":"Jul 9, 2016 1:33:28 AM","dateStarted":"Aug 5, 2016 11:59:29 PM","dateFinished":"Aug 5, 2016 11:59:54 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2244"},{"title":"Read In CSV File For Logistic Regression","text":"var logisticDF = sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"false\") // Automatically infer data types)\n    .option(\"nullValue\", \"empty\")\n    .option(\"dateFormat\", \"yyyy-MM-dd\")\n    .schema(customSchema)\n    .load(\"/resources/data/nba-datawrangle-lrDF.csv\")\n    \nlogisticDF = logisticDF.withColumn(\"cf3\", $\"pct-left\"*$\"teamaspread\" / 100)\n                       .withColumn(\"cf4\", $\"scoreb-scorea\"*$\"scoreb-scorea\"*$\"scoreb-scorea\")\n","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468028015666_-627926485","id":"20160709-013335_1162054352","dateCreated":"Jul 9, 2016 1:33:35 AM","dateStarted":"Aug 5, 2016 11:59:54 PM","dateFinished":"Aug 5, 2016 11:59:55 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2245"},{"title":"Inspect the Data","text":"////////////////////////\n// Here I just wanted to make sure I had read in the data properly\n//logisticDF.filter($\"timeleft\" < 10).show(50)\nz.show(logisticDF.describe())","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"summary","index":0,"aggr":"sum"}],"values":[{"name":"scorea","index":1,"aggr":"sum"},{"name":"scoreb","index":2,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"summary","index":0,"aggr":"sum"},"yAxis":{"name":"scorea","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466994518295_383052269","id":"20160627-022838_210885740","dateCreated":"Jun 27, 2016 2:28:38 AM","dateStarted":"Aug 5, 2016 11:59:55 PM","dateFinished":"Aug 6, 2016 12:00:07 AM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2246"},{"title":"Function to Create the Model and Train it and Test it ","text":"/////////////////////////////////////////////////////////////\n//  Rather than use an ML Pipleline, I created this function so \n// that I could extract/print some intermediate results and\n// debug the model.  \n\ndef trainAndTest( indf : org.apache.spark.sql.DataFrame, featureAryString : Array[String] ) : (org.apache.spark.sql.DataFrame, org.apache.spark.ml.classification.LogisticRegressionModel, Array[Double])  = {\n    println(\"*************************************************\")\n    \n    /////////////////////////////\n    // Split the data into training and test sets\n    \n    val splits = indf.randomSplit(Array(0.7,0.3,0.0), seed = 25L)\n    val trainingdf = splits(0).cache()\n    val testdf = splits(1).cache()\n    \n    println(\"Tranining Samples = \" + trainingdf.count())\n    println(\"Test      Samples = \" + testdf.count())\n    \n\n     /////////////////////////////\n     // use a vector assembler to build features for \n     // logistic model\n     val assembler = new VectorAssembler()\n       .setInputCols(featureAryString)\n       .setOutputCol(\"featuresRaw\")\n\n     val trainingdf2  =  assembler.transform(trainingdf)\n     val testdf2      =  assembler.transform(testdf)\n\n     /////////////////////////////\n     // Standardize the data \n     // logistic model\n    val scaler = new StandardScaler()\n     .setInputCol(\"featuresRaw\")\n     .setOutputCol(\"features\")\n     .setWithStd(false)\n     .setWithMean(false)\n    \n    val scalerModel = scaler.fit(trainingdf2)\n    // Normalize each feature to have unit standard deviation.\n    val trainingdf3 = scalerModel.transform(trainingdf2)\n    val testdf3 = scalerModel.transform(testdf2)\n\n   \n    /////////////////////////////\n    //Logistic Regression Model Setup\n    // Setup some of the configurations for the Logistic regression model ..\n    val lr = new LogisticRegression()\n      .setMaxIter(25)\n      .setRegParam(0.0001)\n      .setElasticNetParam(0.0)\n      .setLabelCol(\"home-win\")\n\n    // Fit the model\n    val lrModel   = lr.fit(trainingdf3)\n\n    println(\"Reg Parameter:    =\" + lrModel.getRegParam)\n    println(\"lrModel.intercept = \" + lrModel.intercept)\n    println(\"lrModel.weights   = \" + lrModel.weights)\n\n    // Save the model for later use ....\n    // Argh ! -> in 1.6.1 api, but not 1.5.2 :(  \n    // lrModel.save(\"modelPath\" )\n    \n    ////  Create a logistic regression summary object ////\n    //val lrSummary = lrModel.summary\n    //println(\"lrSummary.objectiveHistory = \" + lrSummary.objectiveHistory.length)\n    //println(lrSummary.objectiveHistory.deep.mkString(\"\\n\"))\n    ////\n    \n     /////////////////////////////\n     //Generate Predictions\n     // transform is now used in lieu of predict from mllib.  Found this after studying the API for a while\n    val trn_predictions = lrModel.transform(trainingdf3)\n            .withColumn(\"correct\", ($\"home-win\" === $\"prediction\"))\n            .withColumn(\"pct-comp-ceil\", ceil($\"pct-complete\"))\n\n    val predictions = lrModel.transform(testdf3)\n            .withColumn(\"correct\", ($\"home-win\" === $\"prediction\"))\n            .withColumn(\"pct-comp-ceil\", ceil($\"pct-complete\"))\n\n     /////////////////////////////\n     //Evaluate Predictions and Print results\n    \n    val evaluator = new MulticlassClassificationEvaluator()\n      .setLabelCol(\"home-win\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"f1\")  // recall / precision also options\n    \n    val trn_tot_f1 = evaluator.evaluate(trn_predictions)\n    val tst_tot_f1 = evaluator.evaluate(predictions)\n    val f1q1 = evaluator.evaluate(predictions.filter($\"pct-complete\" < 25))\n    val f1q2 = evaluator.evaluate(predictions.filter($\"pct-complete\" > 25 && $\"pct-complete\" < 50))\n    val f1q3 = evaluator.evaluate(predictions.filter($\"pct-complete\" > 50 && $\"pct-complete\" < 75))\n    val f1q4 = evaluator.evaluate(predictions.filter($\"pct-complete\" > 75))\n    \n    println(\"Total Train f1 = \" + (trn_tot_f1))\n    println(\"Total Test  f1 = \" + (tst_tot_f1))\n    \n    println(\"Q1 Test f1 = \" + (f1q1))\n    println(\"Q2 Test f1 = \" + (f1q2))\n    println(\"Q3 Test f1 = \" + (f1q3))\n    println(\"Q4 Test f1 = \" + (f1q4))\n\n    val f1Ary = Array(f1q1,f1q2,f1q3,f1q4,tst_tot_f1)\n\n    // return the \n    (predictions,lrModel,f1Ary)\n}\n","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"label","index":0,"aggr":"sum"}],"values":[{"name":"features","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"label","index":0,"aggr":"sum"},"yAxis":{"name":"features","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466994518295_383052269","id":"20160627-022838_457884799","dateCreated":"Jun 27, 2016 2:28:38 AM","dateStarted":"Aug 6, 2016 12:03:07 AM","dateFinished":"Aug 6, 2016 12:03:08 AM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2247"},{"title":"Test and Train multiple models","text":"/////////////////////////////\n// Evaluate 3 different Models\n\nval (prediction_0, model_0, f1m0)   = trainAndTest(logisticDF, Array(\"scoreb-scorea\" ))\nval (prediction_1, model_1, f1m1)   = trainAndTest(logisticDF, Array(\"scoreb-scorea\",  \"teamaspread\" ))\nval (prediction_2, model_2, f1m2)   = trainAndTest(logisticDF, Array(\"scoreb-scorea\",  \"teamaspread\", \"cf1\", \"cf2\", \"cf3\" ))\n\n","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"label","index":0,"aggr":"sum"}],"values":[{"name":"prediction","index":4,"aggr":"count"}],"groups":[{"name":"prediction","index":4,"aggr":"sum"}],"scatter":{"xAxis":{"name":"label","index":0,"aggr":"sum"},"yAxis":{"name":"features","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466994518296_381128524","id":"20160627-022838_1273081586","dateCreated":"Jun 27, 2016 2:28:38 AM","dateStarted":"Aug 6, 2016 12:03:12 AM","dateFinished":"Aug 6, 2016 12:04:02 AM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2248"},{"title":"Examime F1 scores from the models","text":"////////////////////////////////////////////\n// F1 score is a metric used to evaluate different models\n// it runs on a scale from 0 to 1 with the larger value\n// meaning the model performs better\n// F1 score is a combination of precision / recall and\n// helps in situations where outcomes are highly skewed \n// in one direction.  eg 95% samples are wins, 5% are losses\n// in that example, i could make a model that blindly predicts win\n// every time and I would be 95% correct... F1 adjusts for this fact\n// and would penalize me for the false negatives\n\n\n// Build a small dataframe to hold my F1 scores....\nval lblRdd = sc.parallelize( List(\"model0\",\"model1\",\"model2\"),2)\nval errRdd = sc.parallelize( Array(f1m0,f1m1,f1m2),2)\ncase class errData(label: String, q1: Double,q2: Double,q3: Double,q4: Double, tot:Double)\nval test = lblRdd.zip(errRdd)\n\nval errDf = test.map({ \n  case (lbl: String, Array(q1: Double,q2: Double,q3: Double,q4: Double, tot:Double)) => errData(lbl,q1,q2,q3,q4,tot)\n}).toDF(\"model\", \"Q1\",\"Q2\",\"Q3\",\"q4\",\"total\")\n\nz.show(errDf)\n//test.take(3)\n\n//going from model0 -> model1 yields a decent improvement, but after that the improvement is marginally better with the extra terms","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"colWidth":12,"graph":{"mode":"table","height":294,"optionOpen":true,"keys":[{"name":"model","index":0,"aggr":"sum"}],"values":[{"name":"q1","index":1,"aggr":"sum"},{"name":"q2","index":2,"aggr":"sum"},{"name":"q3","index":3,"aggr":"sum"},{"name":"q4","index":4,"aggr":"sum"},{"name":"total","index":5,"aggr":"sum"}],"groups":[],"scatter":{"yAxis":{"name":"q1","index":1,"aggr":"sum"},"xAxis":{"name":"model","index":0,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468180392966_337972686","id":"20160710-195312_1533867977","dateCreated":"Jul 10, 2016 7:53:12 PM","dateStarted":"Aug 6, 2016 12:00:08 AM","dateFinished":"Aug 6, 2016 12:00:14 AM","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2249"},{"title":"Lets take a look at some of the Errors to see if there is any pattern","text":"prediction_4.filter($\"correct\" === false).select('teama,'scorea,'teamb,'scoreb,$\"timeleft\",$\"pct-comp-ceil\",'teamaspread,'fscorea,'fscoreb,'probability,$\"home-win\",$\"fscoreb-fscorea\",'prediction)\n  .filter($\"fscoreb-fscorea\" < 4 && $\"fscoreb-fscorea\" > -4)\n  .show(20)\n\nprediction_4.select('teama,'scorea,'teamb,'scoreb,$\"timeleft\",$\"pct-comp-ceil\",'teamaspread,'fscorea,'fscoreb,'probability,$\"home-win\",'prediction).show(5)\n//val (prediction_4, model_4, f1m4)   = trainAndTest(logisticDF, Array(\"scoreb-scorea\",  \"teamaspread\", \"cf1\", \"cf2\", \"cf3\",\"cf4\" ))\n\n// Some errors due to \n//   early in game ....\n//   close scores at the end\n//   some games the spread strongly effects game at the end... mabye scale that somehow by time left ?\n//   teams that had an early lead, even though not favored did end up winning.  maybe add a scorediff^2 term ?\n","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"teama","index":0,"aggr":"sum"}],"values":[{"name":"scorea","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"teama","index":0,"aggr":"sum"},"yAxis":{"name":"scorea","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468353206960_132335295","id":"20160712-195326_1439841859","dateCreated":"Jul 12, 2016 7:53:26 PM","dateStarted":"Aug 6, 2016 12:00:14 AM","dateFinished":"Aug 6, 2016 12:00:14 AM","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2250"},{"title":"Plot Correct / Incorrect predictions as a function of pct-Complete","text":"// this is the best I could do with this.  spark 1.6 has a pivot function that would allow me to plot a ratio\n// note how the predictions get better as I swap in model 3 for model 0\nz.show(prediction_3.groupBy(\"pct-comp-ceil\",\"correct\").agg(count(\"correct\")).orderBy(asc(\"pct-comp-ceil\")) )\n\n// as the game moves forward the number of true predictions increases as expected\n// there are some interesting artifacts to the predictions that I scrub\n// I have some redundancies at the end of quarters .... (standardizing the # of samples / game would fix that)\n","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"colWidth":12,"graph":{"mode":"lineChart","height":300,"optionOpen":true,"keys":[{"name":"pctceil","index":0,"aggr":"sum"}],"values":[{"name":"count(correct)","index":2,"aggr":"sum"}],"groups":[{"name":"correct","index":1,"aggr":"sum"}],"scatter":{"yAxis":{"name":"count(correct)","index":2,"aggr":"sum"},"xAxis":{"name":"pctceil","index":0,"aggr":"sum"},"group":{"name":"correct","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468202404933_867889131","id":"20160711-020004_1728031567","dateCreated":"Jul 11, 2016 2:00:04 AM","dateStarted":"Aug 6, 2016 12:00:14 AM","dateFinished":"Aug 6, 2016 12:00:14 AM","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2251"},{"title":"Logistic Analysis And Explanation","text":"%md \n    \n    Complex Model Discussion\n                scoreb-scorea    timeleft     teamaspread      custom(score,time)\n    weights   =  0.077           0.006        -0.105           0.091  \n\n    score difference is exp(0.077) = 1.08 or 8 % increase in win likelihood with an increase in one more point !....\n    timeleft is not a strong predictor ...\n    teamspread is exp(-0.105) = 0.90 or 10 % change in likelihood of winning/losing based on spread change\n    \n    finally I added a feature that amplies the point differential as the game gets closer to finishing ...\n    it works something like this.  \n       if the home team is up by 10 at the beginning of the game, i scale that down to something like 5 points\n       if the home team is up by 10 at the end of the game, i scale that up to something like 20 points\n\n\n","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466994518296_381128524","id":"20160627-022838_848021782","dateCreated":"Jun 27, 2016 2:28:38 AM","dateStarted":"Aug 5, 2016 11:59:21 PM","dateFinished":"Aug 5, 2016 11:59:22 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2252"},{"title":"Function to predict new examples","text":"def getPrediction(teama : String  , scorea : Int , teamb : String , scoreb : Int,  timeleft : Double, teambspread : Double, model : org.apache.spark.ml.classification.LogisticRegressionModel) : Unit = {\n  val sd = scoreb-scorea\n  val sdt = scoredivtimeXform(sd.toDouble, timeleft.toDouble)\n  val lp = LabeledPoint(0.0,Vectors.dense(sd,timeleft,teambspread,sdt))\n\n  val single = Seq(lp).toDF(\"label\",\"features\")\n  val result = model.transform(single)\n  val rv = result.select($\"prediction\").head(1)\n  val prb = result.select($\"probability\").head(1)\n\n//scala.collection.immutable.IndexedSeq  \n  //.toDF(\"label\",\"features\")\n  val rv1 = ad2d(rv(0)(0))\n  val prb1 = prb(0)(0)\n  println(\"dbg : sdt =\" + sdt)\n  println(teama + \"(away) vs \" + teamb + \"(home)\")\n  println(\"Spread(HomeTeam) : \" + teambspread + \" (+ means home team is not favored)\")\n  println(\"Time Left        : \" + timeleft)\n  val winner = {if (rv1 == 1) teamb else teama}\n  println(\"Predicted Winner : \" + winner + \" Probablity : \" + prb1)\n}\n","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466994518297_380743775","id":"20160627-022838_860093134","dateCreated":"Jun 27, 2016 2:28:38 AM","dateStarted":"Aug 6, 2016 12:00:14 AM","dateFinished":"Aug 6, 2016 12:00:14 AM","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2253"},{"title":"Simple Predictor based on a new Example ....","text":"// ADD in a game simulator here !!!\n\ngetPrediction(\"lac\",96, \"por\", 88, 2.0, -8.0, complexModel)\nprintln()\ngetPrediction(\"lac\",88, \"por\", 96, 2.0, -8.0, complexModel)\n\n","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466994518297_380743775","id":"20160627-022838_1767542213","dateCreated":"Jun 27, 2016 2:28:38 AM","dateStarted":"Aug 6, 2016 12:00:14 AM","dateFinished":"Aug 6, 2016 12:00:14 AM","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2254"},{"title":"Next Steps and ideas","text":"%md\ntopic list\n\n    Add single team dataframe ...\n    Add team features .... that will make the graph very large\n    Parameterize the model and select the best threshold // regularizaiton\n    study learning curves\n    switch to linear regression as I am losing some information with just W/L\n    use a case class for model ....\n    add an outlier checker for bad data in games .... \n    add a udf to convert END OF cases // 12:00 in cases to proper time .. but maybe not\n    or just fix my script that captures the data\n\n","dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466994518297_380743775","id":"20160627-022838_1032228347","dateCreated":"Jun 27, 2016 2:28:38 AM","dateStarted":"Aug 5, 2016 11:59:22 PM","dateFinished":"Aug 5, 2016 11:59:22 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2255"},{"dateUpdated":"Aug 6, 2016 12:27:15 AM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466994518297_380743775","id":"20160627-022838_2017945561","dateCreated":"Jun 27, 2016 2:28:38 AM","dateStarted":"Aug 6, 2016 12:00:14 AM","dateFinished":"Aug 6, 2016 12:00:14 AM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2256"}],"name":"NBA-Predictions-3-LogisticRegression","id":"2BPUJ68BG","angularObjects":{"2BTGARFS9":[],"2BU3G1UN8":[],"2BTG59SRF":[],"2BSVJ89RP":[]},"config":{"looknfeel":"default"},"info":{}}