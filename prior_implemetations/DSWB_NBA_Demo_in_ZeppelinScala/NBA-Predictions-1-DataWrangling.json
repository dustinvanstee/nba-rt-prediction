{"paragraphs":[{"title":"Imports","text":"import org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\nimport com.databricks.spark.csv\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,DoubleType,DateType,TimestampType};\nimport scala.util.matching.Regex\n\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types.{StructType,StructField,StringType,DoubleType};\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions.{udf,col,concat,lit,when}\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207531_411948428","id":"20160627-011647_661807928","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:07 PM","dateFinished":"Aug 18, 2016 7:35:09 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:752","errorMessage":""},{"title":"Cleanup old files ...","text":"%sh\n#rm -rf /data/resources/nbaSimpleModel\n#rm -rf /data/resources/nbaComplexModel","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/sh","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207534_410794181","id":"20160627-011647_2038967486","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:07 PM","dateFinished":"Aug 18, 2016 7:35:09 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:753","errorMessage":""},{"title":"Helpers ... Lookup Tables","text":"println(\"Spark version = \" + sc.version)\nval teamMap = Map(\n  \"Atlanta\" -> \"atl\",\n  \"Boston\"  -> \"bos\",\n  \"Brooklyn\"  -> \"bkn\",\n  \"Charlotte\"  -> \"cha\",\n  \"Chicago\"  -> \"chi\",\n  \"Cleveland\"  -> \"cle\",\n  \"Dallas\"  -> \"dal\",\n  \"Denver\"  -> \"den\",\n  \"Detroit\"  -> \"det\",\n  \"Golden State\"  -> \"gst\",\n  \"Houston\"  -> \"hou\",\n  \"Indiana\"  -> \"ind\",\n  \"LA Clippers\"  -> \"lac\",\n  \"LA Lakers\"  -> \"lal\",\n  \"Memphis\"  -> \"mem\",\n  \"Miami\"  -> \"mia\",\n  \"Milwaukee\"  -> \"mil\",\n  \"Minnesota\"  -> \"min\",\n  \"New Orleans\"  -> \"nor\",\n  \"New York\"  -> \"nyk\",\n  \"Oklahoma City\"  -> \"okc\",\n  \"Orlando\"  -> \"orl\",\n  \"Philadelphia\"  -> \"phi\",\n  \"Phila.\"  -> \"phi\",\n  \"Phoenix\"  -> \"pho\",\n  \"Portland\"  -> \"por\",\n  \"Sacramento\" -> \"sac\",\n  \"San Antonio\"  -> \"san\",\n  \"Toronto\"  -> \"tor\",\n  \"Utah\"  -> \"uta\",\n  \"Washington\"  -> \"wsh\")\n  \nval monthMap = Map(\n    \"Jan\"-> \"01\",\n    \"Feb\"-> \"02\",\n    \"Mar\"-> \"03\",\n    \"Apr\"-> \"04\",\n    \"May\"-> \"05\",\n    \"Jun\"-> \"06\",\n    \"Jul\"-> \"07\",\n    \"Aug\"-> \"08\",\n    \"Sep\"-> \"09\",\n    \"Oct\"-> \"10\",\n    \"Nov\"-> \"11\",\n    \"Dec\"-> \"12\"\n)","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207535_410409432","id":"20160627-011647_411671558","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:07 PM","dateFinished":"Aug 18, 2016 7:35:11 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:754","errorMessage":""},{"title":"Inspect Score Data","text":"%sh\ncd /resources/data/\ngrep FINAL scores_nba.test.dat | head -n 3\necho\ngrep -v ET scores_nba.test.dat | head -n 3\n\n\n# Note we have both final scores and real time scores in this file.  This will be important later as we manipulate this data set\n ","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207535_410409432","id":"20160627-011647_169688578","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:09 PM","dateFinished":"Aug 18, 2016 7:35:09 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:755","errorMessage":""},{"title":"Load in NBA Score Data Sets [R]","text":"\n// Since I dont have a header in the data set, I want to specify the column metadata\nval sqlContext = new SQLContext(sc)\nval customSchema = StructType(Array(\n    StructField(\"dateOrig\", DateType, true),\n    StructField(\"ts\", StringType, true),\n    StructField(\"teamlonga\", StringType, true),\n    StructField(\"scorea\", IntegerType, true),\n    StructField(\"teamlongb\", StringType, true),\n    StructField(\"scoreb\", IntegerType, true),\n    StructField(\"time-string\", StringType, true),\n    StructField(\"timeleft\", DoubleType, true),\n    StructField(\"gameid\", IntegerType, true)\n    ))\n\n// This line reads in the file and parses it with a CSV reader\nvar rtscoresAndFinalDF = sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"false\") // Use first line of all files as header\n    .option(\"inferSchema\", \"false\") // Automatically infer data types)\n    .option(\"nullValue\", \"empty\")\n    .option(\"dateFormat\", \"yyyy-MM-dd\")\n    .option(\"mode\",\"DROPMALFORMED\")\n    .schema(customSchema)\n    .load(\"/resources/data/scores_nba.test.dat\") \n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207536_420797653","id":"20160627-011647_201071444","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:10 PM","dateFinished":"Aug 18, 2016 7:35:11 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:756","errorMessage":""},{"title":"Inspect Historical score data","text":"///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// This data is the raw input that contains a record for each update of the game.  \n// Data has some errors and redundancies that must be removed.  Will discuss that as we go along ....\n//  in particular, we need to seperate the in game scores and the final score and re-merge them for our model\n\nz.show(rtscoresAndFinalDF)\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"dateOrig","index":0,"aggr":"sum"}],"values":[{"name":"ts","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"dateOrig","index":0,"aggr":"sum"},"yAxis":{"name":"ts","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true,"tableHide":true,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468266451373_-906028293","id":"20160711-194731_477991539","dateCreated":"Jul 11, 2016 7:47:31 PM","dateStarted":"Aug 18, 2016 7:35:11 PM","dateFinished":"Aug 18, 2016 7:35:13 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:757","errorMessage":""},{"title":"UDFs for creating extra columns in real time data frame","text":"////////////////////////////////////////////////////////////\n// These were a couple custom UDF's I needed to cleanse the data \n// and also to add a few features based on a proprietary way of combining\n// the score with the time left.\n\n\n\n// Create new team name column.. do simple lookup conversion with a UDF\nval mapper = teamin => teamMap(teamin)\nval mapperudf = udf(mapper)\n\n\n// Date Logic to adjust for games that finish on the day after ....\n// This is due to not having a great key to join my tables ...\nval datecrossregex = new Regex(\"^0[0-3]\")\nval dateadjust : ((String, String) => String) = (datein, tsin ) => {\n    val datetest =  datecrossregex.findFirstIn(tsin)\n    val dateary = datein.split(\"-\")\n    val rv = datetest match {\n      case Some(s) => { \n         val day = \"%02d\".format(dateary(2).toInt -1)\n         val newdate = dateary(0) + \"-\" + dateary(1) + \"-\" + day  \n         newdate\n      }\n      case None => datein\n    }\n    rv.asInstanceOf[String]\n}\nval dateadjustudf = udf(dateadjust)\n\n// UDFs to create some extra features ... this one is for an experiemental combination of Time left and Score difference.  \n// Made this via intuition.  This can be extended to add other custom features\n//val crossOverTime = 8\n//val exponentScaler = 0.5\nval scoredivtimeXform: ((Double,Double,Double,Double) => Double) = (sd:Double, tl:Double, co:Double, exp:Double) => {\n    val scaler = 1 / Math.pow( (tl / co) + 0.01 , exp)\n    sd * scaler\n}\nval scoredivtimeUdf = udf(scoredivtimeXform)\n\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207536_420797653","id":"20160627-011647_1598148481","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:11 PM","dateFinished":"Aug 18, 2016 7:35:15 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:758","errorMessage":""},{"title":"Preproces the Real Time and Final Score Data .  Add some useful columns to the data set","text":"///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// Here I create some extra columns for later use.  \n\n\n// Remove Overtime games from this analysis\nrtscoresAndFinalDF = rtscoresAndFinalDF.filter(!$\"time-string\".like(\"%OT%\"))\n\n// Create short 3 character team names \nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teama\", mapperudf(col(\"teamlonga\")))\nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teamb\", mapperudf(col(\"teamlongb\")))\n\n// Add a score differential Column \nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"scorea-scoreb\", $\"scorea\" - $\"scoreb\")\n\n// Transform the Date.  This is for games that spanned multiple days and gave me a headache.  \n// Games adjusted to the day they started on.\nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"date\",  dateadjustudf($\"dateOrig\",$\"ts\"))\n\n// Create a Key for me to use to join with odds data later.  Key = date.teama.teamb\nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"key\", concat($\"date\",lit(\".\"),$\"teama\",lit(\".\"),$\"teamb\"))","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207537_420412904","id":"20160627-011647_1020200773","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:14 PM","dateFinished":"Aug 18, 2016 7:35:16 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:759","errorMessage":""},{"title":"Separate the Real Time and Final Data from One common Dataframe to two dataframes","text":"///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// Currently based on the way the data was sampled, both real time scores and final scores are written as seperate records to the same file. \n// I need to pull these apart, and then join the dataframes so that I have a real time score and features and know if the game was won or lost ....\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// Create Final Score DF\n// Note a shortcut for repeating the dataframe within the filter is to use a $   df.filter(df(\"foo\").contains ... is equiv to df.filter($\"foo\".contains)\n\nvar finalscoresDF = rtscoresAndFinalDF.filter($\"time-string\".like(\"%FINAL%\"))\n\n// Rename some columns so that join later doesnt have name overlaps\nfinalscoresDF = finalscoresDF.withColumnRenamed(\"scorea\", \"fscorea\")\nfinalscoresDF = finalscoresDF.withColumnRenamed(\"scoreb\", \"fscoreb\")\n\n// Create final score difference\nfinalscoresDF = finalscoresDF.withColumn(\"fscorea-fscoreb\", $\"fscorea\" - $\"fscoreb\")\nfinalscoresDF = finalscoresDF.withColumn(\"fscoreb-fscorea\", $\"fscoreb\" - $\"fscorea\")\n\n// Add a Win/loss column Win = 1, Loss = 0\nfinalscoresDF = finalscoresDF.withColumn(\"away-win\", ($\"fscorea-fscoreb\" > 0).cast(\"double\"))\nfinalscoresDF = finalscoresDF.withColumn(\"home-win\", ($\"fscoreb-fscorea\" > 0).cast(\"double\"))\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// Create Real time score DF and more wrangling\n\n// Remove Halftime records and these other cases as my datasource doesnt always change the quarter well\n// as this particular case isn't handled well... (for now)\nvar rtscoresDF = rtscoresAndFinalDF.filter(!$\"time-string\".like(\"%FINAL%\")).\n  filter(not($\"time-string\" === \"(HALFTIME)\")).\n  filter(not($\"time-string\" === \"(12:00 IN 1ST)\") ).\n  filter(not($\"time-string\" === \"(12:00 IN 2ND)\") ).\n  filter(not($\"time-string\" === \"(12:00 IN 3RD)\") ).\n  filter(not($\"time-string\" === \"(12:00 IN 4TH)\") ).\n  filter(not($\"time-string\" === \"(END OF 1ST)\") ).\n  filter(not($\"time-string\" === \"(END OF 2ND)\") ).\n  filter(not($\"time-string\" === \"(END OF 3RD)\") ).\n  filter(not($\"time-string\" === \"(END OF 4TH)\") )\n\n// Create real time score difference\nrtscoresDF = rtscoresDF.withColumn(\"scorea-scoreb\", $\"scorea\" - $\"scoreb\")\nrtscoresDF = rtscoresDF.withColumn(\"scoreb-scorea\", $\"scoreb\" - $\"scorea\")\n\n// Create a game PCT complete and PCT left indictor\nrtscoresDF = rtscoresDF.withColumn(\"pct-complete\", ((($\"timeleft\" * -1) + 48 )/48.0)*100)\nrtscoresDF = rtscoresDF.withColumn(\"pct-left\", lit(100) - $\"pct-complete\" )\n\n// Create a unique feature based on my custom UDF.  Idea here is that I have intuition that timeleft and score difference are a strong predictor when combined\nrtscoresDF = rtscoresDF.withColumn(\"cf1\", scoredivtimeUdf($\"scoreb-scorea\", $\"pct-left\", lit(25.0), lit(0.5)))\nrtscoresDF = rtscoresDF.withColumn(\"cf2\", scoredivtimeUdf($\"scoreb-scorea\", $\"pct-left\", lit(2.0), lit(1.3)))\n\na.+(b)  a + b\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207537_420412904","id":"20160627-011647_1191079723","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:16 PM","dateFinished":"Aug 18, 2016 7:35:19 PM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:760","errorMessage":""},{"title":"Custom feature explanation","text":"%md\nAfter building my initial model, I noticed that the logistic model was adjusting the probabilities well at the end of the games.  I had some examples where I had 0 time left in the game, and yet the logistic model was giving a 70% chance of victory for a team.  I speculated this was due to the fact that my original features were not fitting the end of game very well.  To fix this, I created a spreader custom feature that basically takes the score difference and amplifies it as the score nears the end of the game.  This way this feature is very predictive at the end of games and can help adjust the probablities to be more certain at the end of games.","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468437992090_-1149169620","id":"20160713-192632_635936501","dateCreated":"Jul 13, 2016 7:26:32 PM","dateStarted":"Aug 18, 2016 7:35:07 PM","dateFinished":"Aug 18, 2016 7:35:07 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:761","errorMessage":""},{"title":"Show effect of custom spreader feature ","text":"z.show(rtscoresDF.select(\"pct-complete\",\"pct-left\",\"scoreb-scorea\",\"key\",\"cf1\", \"cf2\").filter($\"pct-complete\" < 95))\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"colWidth":6,"graph":{"mode":"scatterChart","height":300,"optionOpen":true,"keys":[{"name":"pct-complete","index":0,"aggr":"sum"}],"values":[{"name":"pct-left","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"pct-complete","index":0,"aggr":"sum"},"yAxis":{"name":"scoreb-scorea","index":2,"aggr":"sum"},"group":{"name":"key","index":3,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468175355713_1580725910","id":"20160710-182915_1047155367","dateCreated":"Jul 10, 2016 6:29:15 PM","dateStarted":"Aug 18, 2016 7:35:17 PM","dateFinished":"Aug 18, 2016 7:35:20 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:762","errorMessage":""},{"title":"Inspect Custom features ...","text":"z.show(rtscoresDF.select(\"pct-complete\",\"pct-left\",\"scoreb-scorea\",\"key\",\"cf1\", \"cf2\").filter($\"pct-complete\" < 95))","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"colWidth":6,"graph":{"mode":"scatterChart","height":300,"optionOpen":true,"keys":[{"name":"pct-complete","index":0,"aggr":"sum"}],"values":[{"name":"pct-left","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"pct-complete","index":0,"aggr":"sum"},"group":{"name":"key","index":3,"aggr":"sum"},"yAxis":{"name":"cf2","index":5,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468172723654_220192274","id":"20160710-174523_1566282860","dateCreated":"Jul 10, 2016 5:45:23 PM","dateStarted":"Aug 18, 2016 7:35:19 PM","dateFinished":"Aug 18, 2016 7:35:22 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:763","errorMessage":""},{"title":"Lets Take a Look of what we Have for the two dataframes we just wrangled","text":"\n// Some Printouts .....\nprintln(\"final scores data frame\")\nfinalscoresDF.show(5)\nprintln(\"Total Games = \" + finalscoresDF.count())\nprintln(\"real time scores data frame\")\nrtscoresDF.show(5)\nprintln(\"Total Number of rt score records = \" + rtscoresDF.count())\n\n//rtscoresDF.filter($\"key\" === \"2016-04-10.orl.mia\").show(350)\n\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207537_420412904","id":"20160627-011647_2094990827","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:21 PM","dateFinished":"Aug 18, 2016 7:35:24 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:764","errorMessage":""},{"title":"Inspect Odds Data","text":"%sh\ncd /resources/data/\nrm -rf  /resources/data/basketball_nba.xml\necho \"Daily Odds Info\"\nwget http://www.referincome.com/odds/rss2/basketball_nba.xml \ngrep title /resources/data/basketball_nba.xml\n\necho \necho \"Historical Odds Info\"\nhead -n 5 nbaodds_042516.xml\n\n\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207537_420412904","id":"20160627-011647_766765595","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:09 PM","dateFinished":"Aug 18, 2016 7:35:10 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:765","errorMessage":""},{"title":"How to Read the Raw Odds data","text":"%md\n\nHow to interpret the odds data ...\n    \n    Example Golden State -12.5 O (207.0) -125.0 | Detroit 12.5 U (207.0) 145.0\n    The away team is listed first, and the home team is second\n    Here Golden State is a 12.5 pt favorite to win.  The over under is in parentheses (207) and is the 50/50 line between teams sum of scores\n    being above/below that line.  \n    Finally the -125 / +145 numbers are whats known at the moneyline odds. \n        A negative number means you need to bet 125$ to get a 100$ payout\n        A positive number means you need to bet 100$ to get a 145$ payout","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207536_420797653","id":"20160627-011647_858340435","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:07 PM","dateFinished":"Aug 18, 2016 7:35:07 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:766","errorMessage":""},{"title":"Load in Odds data","text":"///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// Here, the data is very raw, and needs to be pre-processed .  I will start by loading it as an RDD and perform a \n// lot of transformations.  Once I have it properly parsed, I will convert to a dataframe.\n// This is not beautiful, but gets the job done\n// Data format .....\n//       <title>New Orleans 2.5 O (207.0) 125.0 | Phila. -2.5 U (207.0) -145.0 (Apr 05, 2016 07:10 PM)</title>\n//       <title>Detroit 4.0 O (202.0) 160.0 | Miami -4.0 U (202.0) -190.0 (Apr 05, 2016 08:05 PM)</title>\n\n// Reading the data in as an RDD first.  There isn't a dataframe parser for this XML I have, so I will write a custom parser ....\nval oddsrdd = sc.textFile(\"/resources/data/nbaodds_042516.xml\")\n\n// just grabbing the text within the < ... > tags.  I can do this, because the format is super simple and not nested\nval gameStringRdd = oddsrdd.map(x => x.substring(x.indexOf('>')+1,x.lastIndexOf('<')))\n\n// String to Double converter helper\ndef parseDouble(s: String) = try { Some(s.toDouble) } catch { case _ : Throwable => None }\n\n// This is where I do the heavy lifting of parsing my XML .. and then finally convert my RDD to a dataframe .....\n// just lots of string parsing and data type conversions\nval oddsDF = gameStringRdd.map(x => {\n  // find the period, and then find the space prior ot the period\n  // Philadelphia special case.  since later on I index on a decimal point, I am assuming its part of a number and not a team abbrev\n  // also removing commas with the cryptic filterNot section ...  \n   val x1 = x.replace(\"Phila.\", \"Philadelphia\").filterNot(\",\" contains _)\n   val ss1 = x1.substring(0,x1.indexOf('.'))\n   val teamlonga = ss1.substring(0,ss1.lastIndexOf(' '))\n   val teama = teamMap(teamlonga)\n   val ss2 = x1.substring(ss1.lastIndexOf(' ')+1,x1.length)\n   val teamaspread = parseDouble(ss2.substring(0,ss2.indexOf(' ')) )\n   val ss3 = ss2.substring(ss2.indexOf(' ')+1,ss2.length)\n   val overunder = parseDouble(ss3.substring(ss3.indexOf('(')+1,ss3.indexOf(')')))\n   val ss4 = ss3.substring(ss3.indexOf(')')+2,ss3.length)\n   val teamaml = parseDouble(ss4.substring(0,ss4.indexOf(' ')))\n   val ss5x = ss4.substring(ss4.indexOf('|')+2,ss4.length)\n   val ss5 = ss5x.substring(0,ss5x.indexOf('.'))\n   \n   val teamlongb = ss5.substring(0,ss5.lastIndexOf(' '))\n   val teamb = teamMap(teamlongb)\n   val ss6 = ss5x.substring(ss5.lastIndexOf(' ')+1,ss5x.length)\n\n   val teambml = parseDouble(ss6.substring(ss6.indexOf(')')+2,ss6.lastIndexOf('(')-1))\n   val ss7 = ss6.substring(ss6.lastIndexOf('(')+1,ss6.length)\n   val dateInfo = ss7.split(' ')\n   val dateStr = dateInfo(2) + \"-\" + monthMap(dateInfo(0)) + \"-\" + dateInfo(1)\n   // This will become my join key for the other data sets\n   val key = dateStr +\".\" + teama + \".\" + teamb\n  (key,teamlonga,teama,teamaspread,overunder,teamaml,teamlongb,teamb,teambml,dateStr)\n   \n  }\n  // Had to add the groupby and average below because I was getting the game odds over multiple days, and it was \n  // adding noise to the analysis\n  // Rename some columns\n  // Create a few new columns for later analysis\n\n).toDF(\"key\",\"teamlonga\",\"teama\",\"teamaspread\",\"overunder\",\"teamaml\",\"teamlongb\",\"teamb\",\"teambml\",\"dateStr\")\n    .groupBy(\"key\",\"teamlonga\",\"teama\",\"teamlongb\",\"teamb\",\"dateStr\")\n    .agg(avg(\"teamaspread\"),avg(\"overunder\"),avg(\"teamaml\"),avg(\"teambml\"))\n    .withColumnRenamed(\"avg(teamaspread)\",\"teamaspread\")\n    .withColumnRenamed(\"avg(overunder)\",\"overunder\")\n    .withColumnRenamed(\"avg(teamaml)\",\"teamaml\")\n    .withColumnRenamed(\"avg(teambml)\",\"teambml\")\n    .withColumn(\"teambspread\", $\"teamaspread\" * -1)\n    .withColumn(\"teama_vegas_fscore\", $\"overunder\" / 2.0 - $\"teamaspread\"/2.0)\n    .withColumn(\"teamb_vegas_fscore\", $\"overunder\" / 2.0 + $\"teamaspread\"/2.0)\n\noddsDF.registerTempTable(\"oddsDF\")\n\n\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207538_421567150","id":"20160627-011647_946782234","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:22 PM","dateFinished":"Aug 18, 2016 7:35:25 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:767","errorMessage":""},{"title":"Inspect some of the Odds Data","text":"oddsDF.show(5)\nprintf(\"Total Home Teams      = %2d\\n\",oddsDF.select(\"teama\").distinct.sort(\"teama\").count())\nprintf(\"Total Away Teams      = %2d\\n\",oddsDF.select(\"teamb\").distinct.sort(\"teamb\").count())\nprintf(\"Total Games Collected = %d\\n \",oddsDF.count())\n\n//oddsDF.filter($\"key\" === \"2016-04-08.mil.bos\")\n//    .groupBy(\"key\",\"teamlonga\",\"teama\",\"teamlongb\",\"teamb\",\"dateStr\")\n//    .agg(avg(\"teamaspread\"),avg(\"overunder\"),avg(\"teamaml\"),avg(\"teambml\"))\n//    .withColumnRenamed(\"avg(teamaspread)\",\"teamaspread\")\n//    .withColumnRenamed(\"avg(overunder)\",\"overunder\")\n//    .withColumnRenamed(\"avg(teamaml)\",\"teamaml\")\n//    .withColumnRenamed(\"avg(teambml)\",\"teambml\")\n//    .show()\n    \n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"date","index":0,"aggr":"sum"}],"values":[{"name":"ts","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"date","index":0,"aggr":"sum"},"yAxis":{"name":"ts","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207538_421567150","id":"20160627-011647_232548107","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:24 PM","dateFinished":"Aug 18, 2016 7:35:40 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:768","errorMessage":""},{"title":"Avg Team Away Game Spread -  ( hint < 0 means favorite)","text":"z.show(oddsDF.groupBy(\"teama\",\"teamlongb\").agg(avg($\"teamaspread\"),avg($\"teambspread\")).orderBy(asc(\"teama\")))\n///////////////////////////////////////////////////////////////////////////////////////////////\n// Here we are averaging the away spread per team.  If the bar is above the zero line, then the \n// team is an underdog, and under the line the team is the favorite.  \n// 8 of the 32 teams were favorites on the road... and they are the likely suspect including \n// CLE/GST/OKC\n\n\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"colWidth":6,"graph":{"mode":"multiBarChart","height":300,"optionOpen":true,"keys":[{"name":"teama","index":0,"aggr":"sum"}],"values":[{"name":"avg(teamaspread)","index":2,"aggr":"avg"}],"groups":[],"scatter":{"xAxis":{"name":"teama","index":0,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468287247158_-524741527","id":"20160712-013407_592831066","dateCreated":"Jul 12, 2016 1:34:07 AM","dateStarted":"Aug 18, 2016 7:35:26 PM","dateFinished":"Aug 18, 2016 7:35:44 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:769","errorMessage":""},{"title":"Avg Home Team Game Spread - (Hint > 0  means underdog)","text":"//z.show(oddsDF.orderBy(asc(\"teama\")))\nz.show(oddsDF.groupBy(\"teama\",\"teamb\").agg(avg($\"teamaspread\"),avg($\"teambspread\")).orderBy(asc(\"teamb\")))\n///////////////////////////////////////////////////////////////////////////////////////////////\n// Here we are averaging the home spread per team.  If the bar is above the zero line, then the \n// team is an underdog, and under the line the team is the favorite.  \n// Note here that the home teams are favored much more, with the usual suspects having a very \n// large advantage\n// SAN/GST/OKC\n\n\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"colWidth":6,"graph":{"mode":"multiBarChart","height":300,"optionOpen":true,"keys":[{"name":"teamb","index":1,"aggr":"sum"}],"values":[{"name":"avg(teambspread)","index":3,"aggr":"avg"}],"groups":[],"scatter":{"xAxis":{"name":"teama","index":0,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468288123427_891964099","id":"20160712-014843_1888000087","dateCreated":"Jul 12, 2016 1:48:43 AM","dateStarted":"Aug 18, 2016 7:35:40 PM","dateFinished":"Aug 18, 2016 7:35:48 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:770","errorMessage":""},{"title":"Join The Odds and Final Score data sets","text":"// Here is where we join the Odds/Realtime scores/ Final Scores into one wholistic data set as input for Logistic Machine Learning\n\n// Create a smaller Final Score Dataframe.  Just keep the key, final score a and b, the win/loss indicator\nval finalslicedscoresDF = finalscoresDF.select($\"key\",$\"fscorea\",$\"fscoreb\",$\"fscorea-fscoreb\",$\"fscoreb-fscorea\",$\"away-win\",$\"home-win\")\n\n// First Join the 2 smallest data frames ... odd and final.\nval gameDF = oddsDF.join(finalslicedscoresDF, oddsDF(\"key\") === finalscoresDF(\"key\")).drop(oddsDF(\"key\"))\n.drop(\"teamlonga\")\n.drop(\"teamlongb\")\n.drop(\"teama\")\n.drop(\"teamb\")\n\n// Print Out the Game Dataframe ... notice we have the odds data merged with the win loss data ....\nprintln(\"gameDF\")\ngameDF.show(3)\nprintf(\"Total Joined Games Collected = %d\\n \",gameDF.count())\ngameDF.registerTempTable(\"gameDF\")\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207539_421182402","id":"20160627-011647_1057859481","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:45 PM","dateFinished":"Aug 18, 2016 7:35:57 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:771","errorMessage":""},{"title":"Lets see if there are some correlations ... Spread vs Final Score Difference","text":"z.show(gameDF)\n\n\n\n\n\n\n\n\n// here we show that the better a team is (negative spread, the more they are likely to win ...)\n\n// Here the spread at the start of the game is a decent predictor regarding the end result\n\n// Final Score Difference vs Spread  \n// Top Left indicates teams with a large pos spread will lose by a wider margin\n// the line should approx pass through 0,0\n// lower Right indicates teams with large neg spread will win by a wider margin \n\n// The logistic and linear models we build will quantify this for us later!","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"scatterChart","height":326,"optionOpen":true,"keys":[{"name":"dateStr","index":0,"aggr":"sum"}],"values":[{"name":"teamaspread","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"fscoreb-fscorea","index":12,"aggr":"sum"},"yAxis":{"name":"teamaspread","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207539_421182402","id":"20160627-011647_1006181838","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:49 PM","dateFinished":"Aug 18, 2016 7:36:01 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:772","errorMessage":""},{"title":"Home / Away sensitivity to Point Spread ","text":"z.show(gameDF)\n\n//////////////////////////////////////////////////\n// Here we can show another weak correlation of the vegas overunder/spread to the actual final\n// outcome.\n// vegas_fscore was calculated by taking overunder/2 +- the spread/2 to get a projection of\n// the home/away teams score\n//\n// Here if the prediction and data were perfectly correlated, we would pass through the\n// y=x line.  in general we follow that path\n\n// we will see how this term plays when we dig into the linear model\n\n// here only home team is shown, but same trend holds for away team\n\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"scatterChart","height":384,"optionOpen":true,"keys":[],"values":[{"name":"fscorea","index":9,"aggr":"avg"},{"name":"teama_vegas_fscore","index":6,"aggr":"avg"}],"groups":[{"name":"home-win","index":14,"aggr":"sum"}],"scatter":{"xAxis":{"name":"teamb_vegas_fscore","index":7,"aggr":"sum"},"yAxis":{"name":"fscoreb","index":10,"aggr":"sum"}},"lineWithFocus":false},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207540_419258657","id":"20160627-011647_1968796859","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:35:58 PM","dateFinished":"Aug 18, 2016 7:36:05 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:773","errorMessage":""},{"title":"Join The Game Dataframe with the real time Score dataframe","text":"// This is the bigger merge.  Merging the odds/final score data with the real time indicators ...\n\nvar lrDF = rtscoresDF.join(gameDF, rtscoresDF(\"key\") === gameDF(\"key\")).drop(gameDF(\"key\"))\n\nprintln(\"lrDF : Logistic Regression Data Frame\")\nlrDF.show(3)\nprintf(\"Total Data Points in DataSet = %d\\n \",lrDF.count())\nlrDF.registerTempTable(\"lrDF\")\n\n\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207540_419258657","id":"20160627-011647_466854180","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:36:02 PM","dateFinished":"Aug 18, 2016 7:36:15 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:774","errorMessage":""},{"title":"Add a few More Features","text":"///////////////////////////////////////////////\n// Add an overunder/spread adjusted projection \n// as points are scored during the game\n// I found this is a strong indicator\nlrDF = lrDF.withColumn(\"teama_adj_fscore\",(( $\"pct-complete\" * -1)/100 + 1) *  $\"teama_vegas_fscore\" + $\"scorea\")\n           .withColumn(\"teamb_adj_fscore\",(( $\"pct-complete\" * -1)/100 + 1) *  $\"teamb_vegas_fscore\" + $\"scoreb\")\n           .withColumn(\"pfscoreb-pfscorea\", $\"teamb_adj_fscore\" - $\"teama_adj_fscore\" )\n                   \n\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468174418431_-574766599","id":"20160710-181338_700817447","dateCreated":"Jul 10, 2016 6:13:38 PM","dateStarted":"Aug 18, 2016 7:36:06 PM","dateFinished":"Aug 18, 2016 7:36:16 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:775","errorMessage":""},{"title":"Filter Out some Data due to data quality","text":"// There is an issue with the data I had captured.  When a quarter transitions from 1st->2nd (etc,etc), sometime the timestring doesn't get updated properly.  Since I used the timestring to calculate the timeleft in the game, I would get some rogue data points.  \n// Example, after 1 min in a game, something the two teams would have scores in the 20's, because it was really at 11 mins in the second quarter.  \n// My solution was to use the final score sum, and then just scale that down to the time left in the game.  I would then compare to the sum of scores i had, and if it was significantly higher, I would remove them.  I did this by visual inspection ... \n// dfa = departure_from_avg\nlrDF = lrDF.withColumn(\"dfa\",(($\"fscorea\" + $\"fscoreb\")/48) * ($\"timeleft\" * -1 + 48) -( $\"scorea\" + $\"scoreb\")).orderBy(asc(\"dfa\")).filter($\"dfa\" > -30)\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467131417302_2054543614","id":"20160628-163017_1643121764","dateCreated":"Jun 28, 2016 4:30:17 PM","dateStarted":"Aug 18, 2016 7:36:16 PM","dateFinished":"Aug 18, 2016 7:36:16 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:776","errorMessage":""},{"title":"Lets Look at some stats from logistic Regression dataframe","text":"\n lrDF.describe().show()","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"multiBarChart","height":300,"optionOpen":true,"keys":[],"values":[],"groups":[],"scatter":{"xAxis":{"name":"dateOrig","index":0,"aggr":"sum"},"yAxis":{"name":"ts","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207540_419258657","id":"20160627-011647_1227947176","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:36:16 PM","dateFinished":"Aug 18, 2016 7:36:39 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:777","errorMessage":""},{"dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467130162438_1803129083","id":"20160628-160922_1718422220","dateCreated":"Jun 28, 2016 4:09:22 PM","dateStarted":"Aug 18, 2016 7:36:17 PM","dateFinished":"Aug 18, 2016 7:36:39 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:778","errorMessage":""},{"title":"Debug only - Reduce data","text":"// Shrink the data just for now ...\n//val splits = lrDF.randomSplit(Array(0.75,0.24,0.01), seed = 11L)\n//val lrDF = splits(1).cache()\n\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207541_418873908","id":"20160627-011647_978624301","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:36:39 PM","dateFinished":"Aug 18, 2016 7:36:39 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:779","errorMessage":""},{"title":"Visualize some of our Time Series Data. ...","text":"z.show(lrDF.filter($\"key\".endsWith(\"cle\") || $\"key\".endsWith(\"gst\")))\n\n////////////////////////////////////////////////////////////////////\n// here we can see the trajectory of some of the games .....    \n// upper left beginning ... upper right (win), lower right (loss)\n// cool visual .... gives an idea about how the games flow\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"scatterChart","height":300,"optionOpen":true,"keys":[{"name":"dateOrig","index":0,"aggr":"sum"}],"values":[{"name":"ts","index":1,"aggr":"sum"}],"groups":[],"scatter":{"group":{"name":"key","index":26,"aggr":"sum"},"xAxis":{"name":"pct-complete","index":14,"aggr":"sum"},"yAxis":{"name":"scoreb-scorea","index":13,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207541_418873908","id":"20160627-011647_68606064","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:36:39 PM","dateFinished":"Aug 18, 2016 7:37:12 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:780","errorMessage":""},{"title":"Samples per Game Visualization - Data Quality check [opt]","text":"///////////////////////////////////////////////////////////////////////\n// This shows I need to work on my data wrangling a bit more ....\n// a better method so I don't overweight for any particular game would be to \n// interpolate and grab a set # of samples per game ...\n\nz.show(lrDF.groupBy(\"key\").agg(count(\"key\")).orderBy(asc(\"count(key)\")) )\n// Use this code snippet to zoom in on single games\n// z.show(lrDF.filter($\"key\" === \"2016-04-08.mil.bos\") )\n\n","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"graph":{"mode":"multiBarChart","height":300,"optionOpen":true,"keys":[{"name":"key","index":0,"aggr":"sum"}],"values":[{"name":"count(key)","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"key","index":0,"aggr":"sum"},"yAxis":{"name":"count(key)","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466990207543_419643406","id":"20160627-011647_1288829450","dateCreated":"Jun 27, 2016 1:16:47 AM","dateStarted":"Aug 18, 2016 7:36:40 PM","dateFinished":"Aug 18, 2016 7:37:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:781","errorMessage":""},{"text":"%sh\nrm -rf /resources/data/nba-datawrangle-lrDF.csv","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467038635979_-485880573","id":"20160627-144355_1319009343","dateCreated":"Jun 27, 2016 2:43:55 PM","dateStarted":"Aug 18, 2016 7:35:10 PM","dateFinished":"Aug 18, 2016 7:35:10 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:782","errorMessage":""},{"title":"Save Out DataFrame at this Point.  From Here we have seperate sheets for a Logistic W/L analysis and a Linear Score projection Analysis","text":"// Wanted to save out the dataset at this point as I will branch into seperate work efforts for a Logistic/Linear model building\n// drop some columns as we move on to next step !!  \n\nlrDF.drop(\"dateOrig\")\n    .drop(\"ts\")\n    .drop(\"teamlonga\")\n    .drop(\"teamlongb\")\n    .drop(\"time-string\")\n    .drop(\"gameid\")\n    .drop(\"date\")\n    .drop(\"dateStr\")\n    .drop(\"teamaml\")\n    .drop(\"teambml\")\n    .drop(\"dfa\")\n    .write.format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"false\") // Automatically infer data types)\n    .option(\"nullValue\", \"empty\")\n    .option(\"dateFormat\", \"yyyy-MM-dd\")\n    .option(\"mode\",\"DROPMALFORMED\")\n    .save(\"/resources/data/nba-datawrangle-lrDF.csv\")","dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466993045301_1821324688","id":"20160627-020405_1138906077","dateCreated":"Jun 27, 2016 2:04:05 AM","dateStarted":"Aug 18, 2016 7:37:13 PM","dateFinished":"Aug 18, 2016 7:37:50 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:783","errorMessage":""},{"dateUpdated":"Aug 19, 2016 2:18:36 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467134935362_1627027566","id":"20160628-172855_2109133027","dateCreated":"Jun 28, 2016 5:28:55 PM","dateStarted":"Aug 18, 2016 7:37:33 PM","dateFinished":"Aug 18, 2016 7:37:50 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:784","errorMessage":""}],"name":"NBA-Predictions-1-DataWrangling-V2","id":"2BQK79UZN","angularObjects":{"2BTHJE2D1":[],"2BTY1N6UK":[],"2BTY7DYNQ":[],"2BW65XEYQ":[]},"config":{"looknfeel":"default"},"info":{}}